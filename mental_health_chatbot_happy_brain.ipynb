{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a79395fe",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Happy Brain Mental Health Chatbot Training Notebook\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Set device\n",
    "# Force selection of the NVIDIA GPU (assumed as device 0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "10e1b2be",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets from Hugging Face...\n",
      "MH FAQ dataset loaded from HF\n"
     ]
    }
   ],
   "source": [
    "print('Loading datasets from Hugging Face...')\n",
    "\n",
    "# Mental Health FAQ dataset from HF\n",
    "try:\n",
    "    mh_faq = load_dataset('csv', data_files='hf://datasets/tolu07/Mental_Health_FAQ/Mental_Health_FAQ.csv')\n",
    "    print('MH FAQ dataset loaded from HF')\n",
    "except Exception as e:\n",
    "    print('Error loading MH FAQ dataset from HF:', e)\n",
    "    mh_faq = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5c15c021",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mental Health Counseling Conversations loaded\n"
     ]
    }
   ],
   "source": [
    "# Mental Health Counseling Conversations\n",
    "try:\n",
    "    mh_counseling = load_dataset('Amod/mental_health_counseling_conversations')\n",
    "    print('Mental Health Counseling Conversations loaded')\n",
    "except Exception as e:\n",
    "    print('Error loading Mental Health Counseling Conversations:', e)\n",
    "    mh_counseling = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4f4dba94",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Medical Chatbot dialogues loaded\n"
     ]
    }
   ],
   "source": [
    "# AI Medical Chatbot dialogues\n",
    "try:\n",
    "    ai_medical = load_dataset('parquet', data_files='hf://datasets/ruslanmv/ai-medical-chatbot/dialogues.parquet')\n",
    "    print('AI Medical Chatbot dialogues loaded')\n",
    "except Exception as e:\n",
    "    print('Error loading AI Medical Chatbot dialogues:', e)\n",
    "    ai_medical = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5407b8bc",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatDoctor dataset loaded\n"
     ]
    }
   ],
   "source": [
    "# ChatDoctor-HealthCareMagic dataset\n",
    "try:\n",
    "    chatdoctor = load_dataset('parquet', data_files='hf://datasets/lavita/ChatDoctor-HealthCareMagic-100k/data/train-00000-of-00001-5e7cb295b9cff0bf.parquet')\n",
    "    print('ChatDoctor dataset loaded')\n",
    "except Exception as e:\n",
    "    print('Error loading ChatDoctor dataset:', e)\n",
    "    chatdoctor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "60c054bb",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mental Health Chatbot dataset loaded from HF\n"
     ]
    }
   ],
   "source": [
    "# Mental Health Chatbot dataset from HF\n",
    "try:\n",
    "    mh_chatbot = load_dataset('parquet', data_files='hf://datasets/heliosbrahma/mental_health_chatbot_dataset/data/train-00000-of-00001-01391a60ef5c00d9.parquet')\n",
    "    print('Mental Health Chatbot dataset loaded from HF')\n",
    "except Exception as e:\n",
    "    print('Error loading Mental Health Chatbot dataset from HF:', e)\n",
    "    mh_chatbot = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a6076952",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# List the datasets for further processing\n",
    "hf_datasets = [\n",
    "    (mh_faq, 'mh_faq'),\n",
    "    (mh_counseling, 'mh_counseling'),\n",
    "    (ai_medical, 'ai_medical'),\n",
    "    (chatdoctor, 'chatdoctor'),\n",
    "    (mh_chatbot, 'mh_chatbot')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "23f89f1a",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local CSV files...\n",
      "Local Mental Health FAQ CSV loaded, columns: ['Question_ID', 'Questions', 'Answers']\n",
      "Error loading transformed_mental_health_chatbot.csv: [Errno 2] No such file or directory: 'transformed_mental_health_chatbot.csv'\n"
     ]
    }
   ],
   "source": [
    "print('Loading local CSV files...')\n",
    "\n",
    "# Load Mental Health FAQ local CSV\n",
    "try:\n",
    "    local_mh_faq = pd.read_csv('./data/mental_health_faq.csv', encoding='utf-8')\n",
    "    print('Local Mental Health FAQ CSV loaded, columns:', list(local_mh_faq.columns))\n",
    "except Exception as e:\n",
    "    print('Error loading local Mental Health FAQ CSV:', e)\n",
    "    local_mh_faq = None\n",
    "\n",
    "# Load transformed mental health chatbot CSV\n",
    "transformed_mh_chatbot = None\n",
    "try:\n",
    "    transformed_mh_chatbot = pd.read_csv(\"transformed_mental_health_chatbot.csv\")\n",
    "    print(\"Loaded transformed_mental_health_chatbot.csv with\", len(transformed_mh_chatbot), \"rows\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading transformed_mental_health_chatbot.csv:\", e)\n",
    "    transformed_mh_chatbot = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c8285140",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses CSV loaded, columns: ['User Input', 'Friend Mode Response', 'Professional Mode Response']\n"
     ]
    }
   ],
   "source": [
    "# Load the Friend mode and Professional mode Responses CSV\n",
    "try:\n",
    "    responses = pd.read_csv('./data/Mental Health Chatbot Dataset - Friend mode and Professional mode Responses.csv', encoding='utf-8')\n",
    "    print('Responses CSV loaded, columns:', list(responses.columns))\n",
    "except Exception as e:\n",
    "    print('Error loading Responses CSV:', e)\n",
    "    responses = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bde4ebd8",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local MH FAQ cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Clean local_mh_faq: Separate into question and answer columns if possible\n",
    "if local_mh_faq is not None:\n",
    "    # Attempt to find columns that contain 'question' and 'answer'\n",
    "    possible_q = [col for col in local_mh_faq.columns if 'question' in col.lower()]\n",
    "    possible_a = [col for col in local_mh_faq.columns if 'answer' in col.lower()]\n",
    "\n",
    "    if possible_q and possible_a:\n",
    "        local_mh_faq['text'] = 'User: ' + local_mh_faq[possible_q[0]].astype(str) + '\\nAssistant: ' + local_mh_faq[possible_a[0]].astype(str)\n",
    "    else:\n",
    "        # If specific columns are not found, use all columns concatenated\n",
    "        if 'text' in local_mh_faq.columns:\n",
    "            local_mh_faq['text'] = local_mh_faq['text']\n",
    "        else:\n",
    "            local_mh_faq['text'] = local_mh_faq.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "    print('Local MH FAQ cleaned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ee77716d",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses CSV cleaned by concatenation.\n",
      "Local CSV files loading and cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean responses CSV: Create separate texts for friend mode and professional mode\n",
    "if responses is not None:\n",
    "    if 'User Input' in responses.columns and 'Friend Mode Response' in responses.columns and 'Professional Mode Response' in responses.columns:\n",
    "        responses['friend_text'] = 'User: ' + responses['User Input'].astype(str) + '\\nAssistant: ' + responses['Friend Mode Response'].astype(str)\n",
    "        responses['professional_text'] = 'User: ' + responses['User Input'].astype(str) + '\\nAssistant: ' + responses['Professional Mode Response'].astype(str)\n",
    "    else:\n",
    "        responses['text'] = responses.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "    print('Responses CSV cleaned by concatenation.')\n",
    "\n",
    "print('Local CSV files loading and cleaning complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "699b2f60",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Mental Health FAQ CSV loaded, columns: ['Question_ID', 'Questions', 'Answers']\n"
     ]
    }
   ],
   "source": [
    "# Load Mental Health FAQ local CSV\n",
    "try:\n",
    "    local_mh_faq = pd.read_csv('./data/mental_health_faq.csv', encoding='utf-8')\n",
    "    print('Local Mental Health FAQ CSV loaded, columns:', list(local_mh_faq.columns))\n",
    "except Exception as e:\n",
    "    print('Error loading local Mental Health FAQ CSV:', e)\n",
    "    local_mh_faq = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ea808074",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses CSV loaded, columns: ['User Input', 'Friend Mode Response', 'Professional Mode Response']\n"
     ]
    }
   ],
   "source": [
    "# Load the Friend mode and Professional mode Responses CSV\n",
    "try:\n",
    "    responses = pd.read_csv('./data/Mental Health Chatbot Dataset - Friend mode and Professional mode Responses.csv', encoding='utf-8')\n",
    "    print('Responses CSV loaded, columns:', list(responses.columns))\n",
    "except Exception as e:\n",
    "    print('Error loading Responses CSV:', e)\n",
    "    responses = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6033c86e",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local MH FAQ cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Clean local_mh_faq: Separate into question and answer columns if possible\n",
    "if local_mh_faq is not None:\n",
    "    # Attempt to find columns that contain 'question' and 'answer'\n",
    "    possible_q = [col for col in local_mh_faq.columns if 'question' in col.lower()]\n",
    "    possible_a = [col for col in local_mh_faq.columns if 'answer' in col.lower()]\n",
    "\n",
    "    if possible_q and possible_a:\n",
    "        local_mh_faq['text'] = 'User: ' + local_mh_faq[possible_q[0]].astype(str) + '\\nAssistant: ' + local_mh_faq[possible_a[0]].astype(str)\n",
    "    else:\n",
    "        # If specific columns are not found, use all columns concatenated\n",
    "        if 'text' in local_mh_faq.columns:\n",
    "            local_mh_faq['text'] = local_mh_faq['text']\n",
    "        else:\n",
    "            local_mh_faq['text'] = local_mh_faq.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "    print('Local MH FAQ cleaned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a09319f2",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses CSV cleaned by concatenation.\n",
      "Local CSV files loading and cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean responses CSV: Create separate texts for friend mode and professional mode\n",
    "if responses is not None:\n",
    "    if 'User Input' in responses.columns and 'Friend Mode Response' in responses.columns and 'Professional Mode Response' in responses.columns:\n",
    "        responses['friend_text'] = 'User: ' + responses['User Input'].astype(str) + '\\nAssistant: ' + responses['Friend Mode Response'].astype(str)\n",
    "        responses['professional_text'] = 'User: ' + responses['User Input'].astype(str) + '\\nAssistant: ' + responses['Professional Mode Response'].astype(str)\n",
    "    else:\n",
    "        responses['text'] = responses.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "    print('Responses CSV cleaned by concatenation.')\n",
    "\n",
    "print('Local CSV files loading and cleaning complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1001521d",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "def format_sample(example, source):\n",
    "    # For FAQ data, assume 'text' column exists\n",
    "    if source in ['mh_faq', 'local_mh_faq']:\n",
    "        if 'text' in example:\n",
    "            return example['text']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'mh_counseling':\n",
    "        if 'conversation' in example:\n",
    "            return example['conversation']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'ai_medical':\n",
    "        if 'dialogue' in example:\n",
    "            return example['dialogue']\n",
    "        elif 'text' in example:\n",
    "            return example['text']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'chatdoctor':\n",
    "        if 'text' in example:\n",
    "            return example['text']\n",
    "        elif 'instruction' in example and 'output' in example:\n",
    "            return 'User: ' + example['instruction'] + '\\nAssistant: ' + example['output']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'mh_chatbot':\n",
    "        if 'text' in example:\n",
    "            return example['text']\n",
    "        elif 'conversation' in example:\n",
    "            return example['conversation']\n",
    "        elif 'question' in example and 'answer' in example:\n",
    "            return 'User: ' + example['question'] + '\\nAssistant: ' + example['answer']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'responses_friend':\n",
    "        if 'friend_text' in example:\n",
    "            return example['friend_text']\n",
    "        else:\n",
    "            return str(example)\n",
    "    elif source == 'responses_professional':\n",
    "        if 'professional_text' in example:\n",
    "            return example['professional_text']\n",
    "        else:\n",
    "            return str(example)\n",
    "    else:\n",
    "        return str(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0143652c",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing function defined.\n"
     ]
    }
   ],
   "source": [
    "# Function to apply formatting to a dataset from Hugging Face\n",
    "def preprocess_dataset(dataset, source):\n",
    "    if dataset is None:\n",
    "        return None\n",
    "    split = 'train' if 'train' in dataset else list(dataset.keys())[0]\n",
    "    data = dataset[split]\n",
    "    formatted = data.map(lambda x: {'text': format_sample(x, source)})\n",
    "    return formatted\n",
    "\n",
    "print('Preprocessing function defined.')\n",
    "# Removed duplicate and improperly indented code\n",
    "\n",
    "# Function to preprocess CSV datasets\n",
    "def preprocess_csv_dataset(df, source):\n",
    "    # Create a dataset from the dataframe\n",
    "    dataset_dict = {\n",
    "        \"question\": df[\"question\"].tolist(),\n",
    "        \"answer\": df[\"answer\"].tolist()\n",
    "    }\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "    # Format the dataset\n",
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": f\"Question: {example['question']}\\nAnswer: {example['answer']}\",  # Format as question-answer pairs\n",
    "        \"source\": source\n",
    "    }\n",
    "    \n",
    "    formatted = dataset.map(format_example)\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "84d6ac14",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added mh_faq to formatted datasets\n",
      "Added mh_counseling to formatted datasets\n",
      "Added ai_medical to formatted datasets\n",
      "Added chatdoctor to formatted datasets\n",
      "Added mh_chatbot to formatted datasets\n"
     ]
    }
   ],
   "source": [
    "formatted_datasets = []\n",
    "\n",
    "# Process datasets from HF\n",
    "for ds, source in hf_datasets:\n",
    "    if ds is not None:\n",
    "        formatted = preprocess_dataset(ds, source)\n",
    "        if formatted is not None:\n",
    "            formatted_datasets.append(formatted)\n",
    "            print('Added', source, 'to formatted datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b7e78759",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added local_mh_faq to formatted datasets\n",
      "Added responses_friend to formatted datasets\n",
      "Added responses_professional to formatted datasets\n"
     ]
    }
   ],
   "source": [
    "# Process local datasets\n",
    "if local_mh_faq is not None:\n",
    "    # Convert pandas DataFrame to Dataset\n",
    "    local_mh_faq_ds = Dataset.from_pandas(local_mh_faq[['text']])\n",
    "    formatted_datasets.append(local_mh_faq_ds)\n",
    "    print('Added local_mh_faq to formatted datasets')\n",
    "\n",
    "if responses is not None:\n",
    "    # Create two datasets: one for friend mode and one for professional mode\n",
    "    if 'friend_text' in responses.columns:\n",
    "        friend_ds = Dataset.from_pandas(responses[['friend_text']].rename(columns={'friend_text': 'text'}))\n",
    "        formatted_datasets.append(friend_ds)\n",
    "        print('Added responses_friend to formatted datasets')\n",
    "    \n",
    "    if 'professional_text' in responses.columns:\n",
    "        prof_ds = Dataset.from_pandas(responses[['professional_text']].rename(columns={'professional_text': 'text'}))\n",
    "        formatted_datasets.append(prof_ds)\n",
    "        print('Added responses_professional to formatted datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2904f7da",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset created with 373371 samples\n"
     ]
    }
   ],
   "source": [
    "# Combine all datasets\n",
    "if formatted_datasets:\n",
    "    combined_dataset = concatenate_datasets(formatted_datasets)\n",
    "    print('Combined dataset created with', len(combined_dataset), 'samples')\n",
    "else:\n",
    "    print('No datasets were successfully processed')\n",
    "    combined_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ec2aa2a0",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "model_name = 'gpt2'  # Base model to fine-tune\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6228ed57",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized\n",
      "Dataset split into 336033 training samples and 37338 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "if combined_dataset is not None:\n",
    "    tokenized_dataset = combined_dataset.map(tokenize_function, batched=True)\n",
    "    print('Dataset tokenized')\n",
    "\n",
    "    # Split the dataset\n",
    "    train_size = int(0.9 * len(tokenized_dataset))\n",
    "    test_size = len(tokenized_dataset) - train_size\n",
    "    \n",
    "    train_dataset = tokenized_dataset.select(range(train_size))\n",
    "    test_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "    \n",
    "    print('Dataset split into', len(train_dataset), 'training samples and', len(test_dataset), 'validation samples')\n",
    "else:\n",
    "    print('No dataset to tokenize')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6b81a65d",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./happy_brain',  # Use the new model name\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy='steps',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "abf92f86",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9ffdc8a7",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training setup complete\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "print('Training setup complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2833a550",
   "metadata": {
    "trusted": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='252027' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    51/252027 08:39 < 741:55:20, 0.09 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./happy_brain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\trainer.py:2556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2549\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2550\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2554\u001b[0m )\n\u001b[0;32m   2555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2556\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2559\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2561\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2562\u001b[0m ):\n\u001b[0;32m   2563\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2564\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\trainer.py:3764\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3762\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3764\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\accelerate\\accelerator.py:2454\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2454\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./happy_brain')\n",
    "loss_type = 'mse'  # Example loss type, adjust as needed\n",
    "tokenizer.save_pretrained('./happy_brain')\n",
    "\n",
    "print('Model trained and saved as \"happy_brain\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a98f4",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Load the trained model for inference\n",
    "inference_model = AutoModelForCausalLM.from_pretrained('./happy_brain')\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained('./happy_brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9554f9",
   "metadata": {
    "trusted": false
   },
   "outputs": [],
   "source": [
    "# Function to generate responses\n",
    "def generate_response(prompt, max_length=100):\n",
    "    inputs = inference_tokenizer('User: ' + prompt + '\n",
    "Assistant:', return_tensors='pt')\n",
    "    outputs = inference_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=inference_tokenizer.eos_token_id\n",
    "    )\n",
    "    response = inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract only the assistant's response\n",
    "    if 'Assistant:' in response:\n",
    "        response = response.split('Assistant:')[1].strip()\n",
    "    return response\n",
    "\n",
    "# Test the model with a few examples\n",
    "test_prompts = [\n",
    "    \"I'm feeling really anxious about my upcoming exam.\",\n",
    "    \"I've been feeling sad lately and I don't know why.\",\n",
    "    \"How can I improve my mental health?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generate_response(prompt)\n",
    "    print('User:', prompt)\n",
    "    print('Happy Brain:', response)\n",
    "    print('---')\n",
    "\n",
    "print('Inference test complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
