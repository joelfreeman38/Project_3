{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d559dfc",
   "metadata": {},
   "source": [
    "# Mental‑Health Chatbot: Happy Brain\n",
    "\n",
    "## Project Objective\n",
    "\n",
    "This project aims to build an emotion-aware mental health chatbot that is both conversational and empathetic. The system leverages:\n",
    "- **Emotion Classification:** Fine-tuned using `SamLowe/roberta-base-go_emotions` to detect multi-label emotions in user inputs.\n",
    "- **Text Generation:** Two T5 models are fine-tuned for:\n",
    "  - **Response Generation:** To generate supportive, therapist-style responses.\n",
    "  - **Question-Answering (QA):** To provide factual answers when questions are asked.\n",
    "- **Integrated Pipeline:** A routing mechanism that chooses the proper response mode (QA vs. supportive response) based on detected emotions and input type.\n",
    "- **Deployment:** A Gradio interface enabling voice and text-based interactions.\n",
    "\n",
    "## Notebook Structure and Sections\n",
    "\n",
    "1. **Library Imports & Environment Setup**  \n",
    "   Sets up all the necessary imports from standard libraries, data handling, PyTorch, Hugging Face, and Gradio.\n",
    "\n",
    "2. **Data Loading & Preprocessing**  \n",
    "   - Loads multiple mental health-related CSV datasets.\n",
    "   - Performs cleaning and mapping to uniform \"question\"/\"answer\" columns.\n",
    "   - Splits the combined dataset into training and testing sets.\n",
    "\n",
    "3. **Multi-Label Emotion Annotation & Custom Trainer Setup**  \n",
    "   - Processes multi-label emotion annotations using `MultiLabelBinarizer`.\n",
    "   - Defines custom data collators and trainers to support binary cross-entropy loss for multi-label classification.\n",
    "   - Tokenizes and prepares data for the emotion classifier model.\n",
    "\n",
    "4. **Model Training: Emotion Classification**  \n",
    "   - Fine-tunes the `SamLowe/roberta-base-go_emotions` model on the processed data.\n",
    "   - Logs evaluation metrics such as micro F1, precision, recall, and subset accuracy.\n",
    "   - Saves the fine-tuned emotion classifier model.\n",
    "\n",
    "5. **Model Training: T5 for Response Generation**  \n",
    "   - Constructs input/target pairs for response generation (user input -> supportive response).\n",
    "   - Fine-tunes a T5 model (e.g., `t5-small`) with evaluation metrics like ROUGE and BERTScore.\n",
    "   - Saves the fine-tuned T5 response generator model.\n",
    "\n",
    "6. **Model Training: T5 for Question-Answering (QA)**  \n",
    "   - Constructs QA pairs with a specific prompt format.\n",
    "   - Fine-tunes a separate T5 model for QA tasks.\n",
    "   - Saves the fine-tuned T5 QA model.\n",
    "\n",
    "7. **Model Deployment Setup with Combined Metadata**  \n",
    "   - Saves a combined metadata file (`combined_model_metadata.pt`) that stores the paths to all three fine-tuned models.\n",
    "   - Updates the loading code to reference this metadata file, ensuring that future training runs build on previous improvements rather than overwriting models.\n",
    "\n",
    "8. **Unified Pipeline & Gradio Interface**  \n",
    "   - Integrates the emotion classifier, T5 response generator, and T5 QA models into a single pipeline.\n",
    "   - Uses helper functions (for prompt formatting, emotion detection, audio transcription, etc.) to generate responses.\n",
    "   - Provides a fully-functional Gradio interface supporting both text and voice inputs with options for language, chat history, and conversation logging.\n",
    "\n",
    "9. **Evaluation**  \n",
    "   - Evaluates the emotion classifier using metrics such as micro F1, precision, recall, and subset accuracy.\n",
    "   - Evaluates the generation models (both response generation and QA) using metrics including ROUGE, BERTScore, perplexity, and loss.\n",
    "   - Outputs formatted evaluation results for easy monitoring.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook offers a complete end-to-end pipeline—from data preprocessing and model fine-tuning to evaluation and interactive deployment via Gradio—for an emotion-aware mental health chatbot. The modular structure ensures that each component (emotion detection, response generation, QA) is individually optimized, and a combined metadata system supports incremental training and easy deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e74c4",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fa4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Environment Setup\n",
    "# ================================\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Force CPU use\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# ================================\n",
    "# Standard Library Imports\n",
    "# ================================\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import tempfile\n",
    "import datetime\n",
    "import pprint  # Pretty-printing\n",
    "\n",
    "# ================================\n",
    "# Scientific & Data Libraries\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# ================================\n",
    "# Audio Processing\n",
    "# ================================\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "# ================================\n",
    "# NLP & Transformers (Hugging Face)\n",
    "# ================================\n",
    "import evaluate\n",
    "import gradio as gr\n",
    "import streamlit as st\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "# Transformers - Tokenizers & Models\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    T5ForConditionalGeneration\n",
    ")\n",
    "\n",
    "# Transformers - Training Tools\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    default_data_collator\n",
    ")\n",
    "\n",
    "# Transformers - Logging\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# ================================\n",
    "# PEFT / LoRA\n",
    "# ================================\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c446199",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CPU = True  # Set to True to force CPU mode\n",
    "device = torch.device('cpu' if USE_CPU else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print('Device in use:', device)\n",
    "\n",
    "# Manual device override\n",
    "\n",
    "# FORCE CPU for the entire session\n",
    "USE_CPU = True\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"FORCED device:\", device)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "# Save directory setup\n",
    "\n",
    "SAVE_ROOT = Path(\"./saved_models\")\n",
    "for sub in [\"emotion_classifier\", \"flan_t5_response_generator\", \"t5_qa\", \"final_combined\"]:\n",
    "    (SAVE_ROOT / sub).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b54c6",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle individual CSVs and provide their column mapping\n",
    "# Format: name: (enabled, path, question_col, answer_col)\n",
    "\n",
    "DATASETS = {\n",
    "    \"ds1\": (True,  \"./data/ds1_transformed_mental_health_chatbot_dataset.csv\",  \"question\", \"answer\"),\n",
    "    \"ds2\": (False, \"./data/ds2_transformed_mental_health_chatbot.csv\",         \"question\", \"answer\"),\n",
    "    \"ds3\": (False, \"./data/ds3_mental_health_faq_cleaned.csv\",                 \"Question\", \"Answer\"),\n",
    "    \"ds4\": (False, \"./data/ds4_mental_health_chatbot_dataset_merged_modes.csv\",\"prompt\",   \"response\"),\n",
    "    \"ds5\": (False, \"./data/ds5_Mental_Health_FAQ.csv\",                         \"Question\", \"Answer\"),\n",
    "    \"ds6\": (False, \"./data/ds6_mental_health_counseling.csv\",                  \"query\",    \"completion\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b472611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaner that auto-maps columns to 'question' / 'answer'\n",
    "\n",
    "def load_and_clean(path, q_col, a_col):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Normalize headers\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    q_col = q_col.lower().strip()\n",
    "    a_col = a_col.lower().strip()\n",
    "\n",
    "    # Common renames\n",
    "    rename_map = {\n",
    "        \"prompt\": \"question\",\n",
    "        \"response\": \"answer\",\n",
    "        \"questions\": \"question\",\n",
    "        \"answers\": \"answer\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # If provided cols exist, rename them to standard names\n",
    "    \n",
    "    if q_col in df.columns:\n",
    "        df = df.rename(columns={q_col: \"question\"})\n",
    "    if a_col in df.columns:\n",
    "        df = df.rename(columns={a_col: \"answer\"})\n",
    "\n",
    "    # Try to map 'context' -> 'question' if needed\n",
    "    if \"question\" not in df.columns and \"context\" in df.columns:\n",
    "        df = df.rename(columns={\"context\": \"question\"})\n",
    "\n",
    "    # Verify if necessary columns exist\n",
    "    if not {\"question\", \"answer\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Could not find 'question'/'answer' in {path}. Available columns: {list(df.columns)}\")\n",
    "\n",
    "    # Retain only required columns, drop missing values and duplicates\n",
    "    df = df[[\"question\", \"answer\"]].dropna()\n",
    "    df[\"question\"] = df[\"question\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df[\"answer\"]   = df[\"answer\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    return Dataset.from_pandas(df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565109d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load enabled datasets and create a unified dataset\n",
    "datasets = []\n",
    "for key, (enabled, path, q_col, a_col) in DATASETS.items():\n",
    "    if enabled:\n",
    "        print(f\"Loading dataset '{key}' from {path} ...\")\n",
    "        ds = load_and_clean(path, q_col, a_col)\n",
    "        print(f\"Loaded {len(ds)} examples from '{key}'.\")\n",
    "        datasets.append(ds)\n",
    "    else:\n",
    "        print(f\"Skipping dataset '{key}' as its toggle is off.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasets:\n",
    "    combined_dataset = concatenate_datasets(datasets)\n",
    "    print(f\"\\nCombined dataset contains {len(combined_dataset)} examples.\")\n",
    "else:\n",
    "    raise ValueError(\"No datasets enabled. Please enable at least one dataset in DATASETS.\")\n",
    "\n",
    "# Shuffle and split into training and testing datasets (e.g., 90% train, 10% test)\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "split_dataset = combined_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "print(f\"Training set: {len(train_dataset)} examples, Testing set: {len(test_dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7228172",
   "metadata": {},
   "source": [
    "### Multi-Label Emotion Annotation & Custom Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8030b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multi-label Emotion Annotation & Trainer Setup\n",
    "\n",
    "\n",
    "# Set a seed if not already defined\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "# Data Collator: casts labels to float32\n",
    "\n",
    "def float_label_collator(features):\n",
    "    \"\"\"\n",
    "    Wrap the default HF collator but cast the `labels` tensor to float32\n",
    "    so BCEWithLogitsLoss gets the right dtype.\n",
    "    \"\"\"\n",
    "    batch = default_data_collator(features)\n",
    "    if \"labels\" in batch:\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float32)\n",
    "    # Uncomment next line to print label info during debugging\n",
    "    # print(\"collator labels dtype/shape:\", batch[\"labels\"].dtype, batch[\"labels\"].shape)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Trainer for Multi-Label Classification\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that computes loss using binary cross‑entropy with logits.\n",
    "    This ensures multi‑label targets (e.g., emotions) are correctly processed.\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs: bool = False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\").float()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # If labels' shape does not match logits, reshape to match\n",
    "        if labels.shape != logits.shape:\n",
    "            labels = labels.view_as(logits)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels, reduction=\"mean\")\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Callback: Print training progress (optional)\n",
    "\n",
    "class StepPrinter(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A Trainer callback that prints step-wise loss and evaluation metrics\n",
    "    while keeping the tqdm progress bar.\n",
    "    \"\"\"\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs or not state.is_local_process_zero:\n",
    "            return\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step:>6} • loss {logs['loss']:.4f}\")\n",
    "        if \"eval_loss\" in logs:\n",
    "            metric = logs.get(\"micro_f1\") or logs.get(\"bertscore_f1\") or logs.get(\"rougeL\")\n",
    "            metric_str = f\" • metric {metric:.4f}\" if metric is not None else \"\"\n",
    "            print(f\"Epoch {int(state.epoch)}/{int(args.num_train_epochs)}\"\n",
    "                  f\" • eval_loss {logs['eval_loss']:.4f}{metric_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Emotion Labels (28 total: 27 + neutral)\n",
    "\n",
    "GO_EMOTION_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "    'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "num_labels = len(GO_EMOTION_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Annotate each example with multi-label emotion annotations.\n",
    "# For this demo, we simulate emotion annotations: if an example has no emotion,\n",
    "# we default to \"neutral\". Replace this with your real emotion annotations if available.\n",
    "\n",
    "def annotate_emotions(example):\n",
    "    emos = example.get(\"emotions\", [])\n",
    "    \n",
    "    # If no explicit emotion annotations, default to [\"neutral\"]\n",
    "    if not emos:\n",
    "        emos = [\"neutral\"]\n",
    "    \n",
    "    # Save original emotions for visibility (optional)\n",
    "    example[\"emotions\"] = emos\n",
    "    \n",
    "    # Create a binary label vector for each of the 28 emotions\n",
    "    example[\"labels\"] = [1.0 if lbl in emos else 0.0 for lbl in GO_EMOTION_LABELS]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper to retrieve input text from an example (for debugging)\n",
    "\n",
    "def get_input_text(example):\n",
    "    return example.get(\"text\") or example.get(\"question\") or \"[NO TEXT FOUND]\"\n",
    "\n",
    "\n",
    "# (Re‑)load datasets for emotion annotation if not already loaded.\n",
    "\n",
    "if 'train_dataset' not in globals() or 'test_dataset' not in globals():\n",
    "    print(\"Reloading datasets for emotion annotation ...\")\n",
    "    datasets_list = []\n",
    "    for name, (enabled, path, q_col, a_col) in DATASETS.items():\n",
    "        if not enabled:\n",
    "            continue\n",
    "        ds = load_and_clean(path=path, q_col=q_col, a_col=a_col)\n",
    "        datasets_list.append(ds)\n",
    "    if not datasets_list:\n",
    "        print(\"No datasets were enabled, using a fallback test dataset.\")\n",
    "        fallback_data = {\n",
    "            \"text\": [\n",
    "                \"How are you?\",\n",
    "                \"I feel really down today.\",\n",
    "                \"I'm so happy with my progress!\",\n",
    "                \"Why does nobody understand me?\",\n",
    "                \"I'm feeling anxious about school.\",\n",
    "                \"Life is good lately.\",\n",
    "                \"Sometimes I just want to cry.\",\n",
    "                \"Everything is falling apart.\",\n",
    "                \"I’m grateful for my therapist.\",\n",
    "                \"Can someone please just listen?\"\n",
    "            ]\n",
    "        }\n",
    "        ds = Dataset.from_dict(fallback_data)\n",
    "        datasets_list.append(ds)\n",
    "    full_ds = concatenate_datasets(datasets_list) if len(datasets_list) > 1 else datasets_list[0]\n",
    "    full_ds = full_ds.shuffle(seed=SEED)\n",
    "    split = full_ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "    train_dataset, test_dataset = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset):,} examples • Test dataset: {len(test_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f137d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Map emotion annotations onto training and testing datasets\n",
    "\n",
    "emo_train = train_dataset.map(annotate_emotions)\n",
    "emo_test  = test_dataset.map(annotate_emotions)\n",
    "\n",
    "print(\"Sample annotation:\")\n",
    "print(get_input_text(emo_train[0]), \"->\", emo_train[0][\"emotions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fbdc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter out any examples without at least one positive label (shouldn't happen after defaulting to neutral)\n",
    "\n",
    "def has_nonzero_labels(example):\n",
    "    return sum(example[\"labels\"]) > 0\n",
    "\n",
    "emo_train = emo_train.filter(has_nonzero_labels)\n",
    "emo_test = emo_test.filter(has_nonzero_labels)\n",
    "\n",
    "# Check for any problematic labels\n",
    "bad_labels = [ex for ex in emo_train if \"labels\" not in ex or sum(ex[\"labels\"]) == 0]\n",
    "print(\"Number of examples with problematic labels:\", len(bad_labels))\n",
    "if bad_labels:\n",
    "    print(\"Example with problematic labels:\", bad_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the tokenizer for emotion classification.\n",
    "# We'll be using the 'SamLowe/roberta-base-go_emotions' tokenizer.\n",
    "\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Print columns before renaming for clarity\n",
    "print(\"Before column rename:\", emo_train.column_names)\n",
    "# Rename 'question' to 'text' if needed for tokenization\n",
    "if \"question\" in emo_train.column_names:\n",
    "    emo_train = emo_train.rename_column(\"question\", \"text\")\n",
    "if \"question\" in emo_test.column_names:\n",
    "    emo_test = emo_test.rename_column(\"question\", \"text\")\n",
    "print(\"After column rename:\", emo_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1eddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization: tokenize using the 'text' column.\n",
    "\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74efbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to cast labels to float32 using numpy\n",
    "\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and cast labels for both training and testing sets (batched processing)\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "# Set the dataset format for PyTorch\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(f\"Tokenized training examples: {len(emo_train_tok)} • Tokenized testing examples: {len(emo_test_tok)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = {\n",
    "    \"t5_qa\": \"./saved_models/t5_qa\",\n",
    "    \"emotion_classifier\": \"./saved_models/emotion_classifier\",\n",
    "    \"flan_t5_response_generator\": \"./saved_models/flan_t5_response_generator\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf9090",
   "metadata": {},
   "source": [
    "## 4. Train Emotion Classifier (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55885294",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model_path = SAVE_ROOT / \"emotion_classifier\"\n",
    "\n",
    "try:\n",
    "    if emotion_model_path.is_dir() and any(emotion_model_path.iterdir()):\n",
    "        print(\"Loading previously fine-tuned emotion model...\")\n",
    "        emo_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_path).to(device)\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    print(\"Loading base emotion model...\")\n",
    "    emo_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"SamLowe/roberta-base-go_emotions\",\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        num_labels=len(GO_EMOTION_LABELS)\n",
    "    ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aefce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Training: Emotion Classification with RoBERTa\n",
    "\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cpu\" if USE_CPU else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"Training device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization for Model Training: Use fixed max_length\n",
    "\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfa0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float conversion: ensures labels become float32 arrays.\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627e883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-tokenize the datasets using the new tokenization function\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(f\"After tokenization: {len(emo_train_tok)} training examples; {len(emo_test_tok)} testing examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aac608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained emotion classification model\n",
    "\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=num_labels,  # equals len(GO_EMOTION_LABELS)\n",
    ").to(device)\n",
    "print(\"Loaded model:\", emo_model.config._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Evaluation Metrics for Emotion Classification\n",
    "\n",
    "def compute_emo_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    # Compute probabilities using sigmoid on logits\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    # Apply threshold (0.3) to decide positive labels\n",
    "    preds = (probs > 0.3).int().numpy()\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Defensive check: only consider rows with at least one positive label\n",
    "    mask = labels.sum(axis=1) > 0\n",
    "    if mask.sum() == 0:\n",
    "        print(\"Warning: all evaluation labels are empty\")\n",
    "        return {\"micro_f1\": 0.0}\n",
    "\n",
    "    try:\n",
    "        f1 = f1_score(labels[mask], preds[mask], average=\"micro\", zero_division=0)\n",
    "    except ValueError as e:\n",
    "        print(\"Metric error:\", e)\n",
    "        f1 = 0.0\n",
    "\n",
    "    return {\"micro_f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set Up Training Arguments\n",
    "\n",
    "# Define a root folder for saving model checkpoints if not defined already\n",
    "from pathlib import Path\n",
    "SAVE_ROOT = Path(\"./saved_models\")\n",
    "emo_args = TrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"emotion_classifier\"),\n",
    "    \n",
    "    # Logging & Reporting\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Training hyper‑parameters\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Evaluation and checkpointing settings\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9069e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the Custom Trainer for Multi-Label Classification\n",
    "\n",
    "trainer_emo = MultiLabelTrainer(\n",
    "    model=emo_model,\n",
    "    args=emo_args,\n",
    "    train_dataset=emo_train_tok,\n",
    "    eval_dataset=emo_test_tok,\n",
    "    tokenizer=emo_tokenizer,\n",
    "    data_collator=float_label_collator,\n",
    "    compute_metrics=compute_emo_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f991cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Roberta Emotion Classifier\n",
    "num_labels = len(emo_train_tok[0]['labels'])\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"roberta-base\", num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./saved_models/emotion_classifier\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = (logits > 0).astype(int)\n",
    "    return {\n",
    "        \"f1\": f1_score(labels, preds, average=\"micro\"),\n",
    "        \"precision\": precision_score(labels, preds, average=\"micro\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"micro\"),\n",
    "        \"accuracy\": accuracy_score(labels, preds)\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=emotion_model,\n",
    "    args=training_args,\n",
    "    train_dataset=emo_train_tok,\n",
    "    eval_dataset=emo_test_tok,\n",
    "    tokenizer=emo_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "emotion_model.save_pretrained(\"./saved_models/emotion_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea3620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start Training (uncomment the next line to train)\n",
    "\n",
    "trainer_emo.train()\n",
    "\n",
    "\n",
    "# Save the Best Model and Tokenizer\n",
    "\n",
    "# Create the directory structure if it doesn't exist\n",
    "SAVE_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "(SAVE_ROOT / \"emotion_classifier\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "emo_model.save_pretrained(SAVE_ROOT / \"emotion_classifier\")\n",
    "emo_tokenizer.save_pretrained(SAVE_ROOT / \"emotion_classifier\")\n",
    "\n",
    "# Also save the trainer's final model state\n",
    "trainer_emo.save_model()\n",
    "print(\"Emotion classifier model and tokenizer saved to\", SAVE_ROOT / \"emotion_classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840db175",
   "metadata": {},
   "source": [
    "## 5. Train T5 for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10739d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/flan-t5-large\"\n",
    "SAVE_DIR = SAVE_ROOT / \"flan_t5_response_generator\"\n",
    "\n",
    "try:\n",
    "    if SAVE_DIR.is_dir() and any(SAVE_DIR.iterdir()):\n",
    "        print(\"Loading previously fine-tuned flan-t5-large response generator...\")\n",
    "        resp_model = T5ForConditionalGeneration.from_pretrained(SAVE_DIR).to(device)\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    print(\"Loading base FLAN-T5-XL model...\")\n",
    "    resp_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f801b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train T5 for Response Generation\n",
    "\n",
    "# Build input/target pairs: user text -> helpful response\n",
    "# For now we use 'question' as input and 'answer' as target.\n",
    "def build_t5_pairs(example):\n",
    "    # Retrieve the question from either \"question\" or \"text\" columns,\n",
    "    # and the corresponding answer from either \"answer\" or \"response\" columns.\n",
    "    question = example.get(\"question\") or example.get(\"text\") or \"\"\n",
    "    answer = example.get(\"answer\") or example.get(\"response\") or \"\"\n",
    "    example[\"input_text\"] = \"respond: \" + question\n",
    "    example[\"target_text\"] = answer\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the original training and testing datasets to build T5 pairs.\n",
    "# We're using the original QA datasets (train_dataset and test_dataset) from our data-loading cell.\n",
    "resp_train = train_dataset.map(build_t5_pairs)\n",
    "resp_test  = test_dataset.map(build_t5_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 tokenizer and model.\n",
    "t5_resp_model_name = \"google/flan-t5-large\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_resp_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input and target texts for T5.\n",
    "def t5_tokenize(batch):\n",
    "    # Tokenize the input text.\n",
    "    model_inputs = tokenizer_t5(batch[\"input_text\"], max_length=64, truncation=True)\n",
    "    # Prepare target text tokenization; this context manager sets the tokenizer into target mode.\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(batch[\"target_text\"], max_length=64, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ebc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the training and testing T5 pairs.\n",
    "resp_train_tok = resp_train.map(t5_tokenize, batched=True, remove_columns=resp_train.column_names)\n",
    "resp_test_tok  = resp_test.map(t5_tokenize, batched=True, remove_columns=resp_test.column_names)\n",
    "\n",
    "# Set datasets' format to output PyTorch tensors.\n",
    "resp_train_tok.set_format(\"torch\")\n",
    "resp_test_tok.set_format(\"torch\")\n",
    "\n",
    "print(f\"Tokenized training examples: {len(resp_train_tok)}; Tokenized testing examples: {len(resp_test_tok)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FLAN-T5 Response Generation Model & Tokenizer from metadata\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "flan_resp_path = Path(model_paths[\"flan_t5_response_generator\"])\n",
    "\n",
    "try:\n",
    "    if flan_resp_path.is_dir() and any(flan_resp_path.iterdir()):\n",
    "        print(\"Loading fine-tuned FLAN-T5 model...\")\n",
    "        resp_model = T5ForConditionalGeneration.from_pretrained(flan_resp_path).to(device)\n",
    "        resp_tokenizer = AutoTokenizer.from_pretrained(flan_resp_path)\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    print(\"Loading base flan-t5-large model...\")\n",
    "    resp_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "    resp_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2af057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    if isinstance(labels, tuple):\n",
    "        labels = labels[0]\n",
    "    labels = np.where(labels != -100, labels, resp_tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = resp_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = resp_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": result[\"rougeL\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8824d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Seq2Seq training arguments.\n",
    "\n",
    "resp_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./saved_models/flan_t5_response_generator\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\"\n",
    ")\n",
    "\n",
    "\n",
    "# Assert that tokenized response data exists\n",
    "\n",
    "assert 'resp_train_tok' in globals() and 'resp_test_tok' in globals(), \"Tokenized response data is not defined.\"\n",
    "\n",
    "# Create the Seq2SeqTrainer instance\n",
    "\n",
    "resp_trainer = Seq2SeqTrainer(\n",
    "    model=resp_model,  \n",
    "    args=resp_training_args,\n",
    "    train_dataset=resp_train_tok,  \n",
    "    eval_dataset=resp_test_tok,    \n",
    "    tokenizer=resp_tokenizer,      \n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=resp_tokenizer, model=resp_model),\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "# Begin training\n",
    "\n",
    "resp_trainer.train()\n",
    "resp_model.save_pretrained(\"./saved_models/flan_t5_response_generator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d398f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the T5 model for conditional generation and send it to the appropriate device.\n",
    "\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(t5_resp_model_name).to(device)\n",
    "print(\"Loaded T5 model:\", t5_resp_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f853d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained FLAN-T5 response generation model and tokenizer.\n",
    "\n",
    "(SAVE_ROOT / \"flan_t5_response_generator\").mkdir(exist_ok=True, parents=True)\n",
    "resp_model.save_pretrained(SAVE_ROOT / \"flan_t5_response_generator\")\n",
    "tokenizer_t5.save_pretrained(SAVE_ROOT / \"flan_t5_response_generator\")\n",
    "trainer_resp.save_model()  # Save trainer's model state.\n",
    "\n",
    "print(\"FLAN-T5 response generator model and tokenizer saved to\", SAVE_ROOT / \"flan_t5_response_generator\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6140c",
   "metadata": {},
   "source": [
    "## Train T5 for Question‑Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train or load T5 QA Model\n",
    "qa_model_path = Path(model_paths[\"t5_qa\"])\n",
    "\n",
    "try:\n",
    "    if qa_model_path.is_dir() and any(qa_model_path.iterdir()):\n",
    "        print(\"Loading previously fine-tuned T5 QA model...\")\n",
    "        t5_qa_model = T5ForConditionalGeneration.from_pretrained(qa_model_path).to(device)\n",
    "        t5_qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_path)\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except Exception:\n",
    "    print(\"Loading base T5 QA model (t5-base)...\")\n",
    "    t5_qa_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
    "    t5_qa_tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b533a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build QA pairs: \"question: <text>\" -> answer.\n",
    "def build_qa_pairs(example):\n",
    "    # Retrieve the question from 'question' (or 'text') and answer from 'answer' (or 'response').\n",
    "    question = example.get(\"question\") or example.get(\"text\") or \"\"\n",
    "    answer = example.get(\"answer\") or example.get(\"response\") or \"\"\n",
    "    example[\"input_text\"] = \"question: \" + question\n",
    "    example[\"target_text\"] = answer\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3871fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 QA Model & Tokenizer from metadata.\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(model_paths[\"t5_qa\"]).to(device)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"t5_qa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_tokenize(batch):\n",
    "    # Ensure 'context' exists — fallback to empty strings\n",
    "    context = batch[\"context\"] if \"context\" in batch else [\"\"] * len(batch[\"question\"])\n",
    "\n",
    "    # Tokenize inputs\n",
    "    inputs = qa_tokenizer(\n",
    "        batch[\"question\"],\n",
    "        context,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "    # Tokenize labels\n",
    "    with qa_tokenizer.as_target_tokenizer():\n",
    "        labels = qa_tokenizer(\n",
    "            batch[\"answer\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64\n",
    "        )\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_train = train_dataset.map(build_qa_pairs)\n",
    "qa_test  = test_dataset.map(build_qa_pairs)\n",
    "\n",
    "qa_train_tok = qa_train.map(qa_tokenize, batched=True, remove_columns=qa_train.column_names)\n",
    "qa_test_tok  = qa_test.map(qa_tokenize, batched=True, remove_columns=qa_test.column_names)\n",
    "\n",
    "qa_train_tok.set_format(\"torch\")\n",
    "qa_test_tok.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map the QA pair-building function onto the training and testing datasets.\n",
    "# qa_train = train_dataset.map(build_qa_pairs)\n",
    "# qa_test  = test_dataset.map(build_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce478b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_train_tok = qa_train.map(qa_tokenize, batched=True, remove_columns=qa_train.column_names)\n",
    "# qa_test_tok  = qa_test.map(qa_tokenize, batched=True, remove_columns=qa_test.column_names)\n",
    "\n",
    "# # Set the format for PyTorch tensors.\n",
    "# qa_train_tok.set_format(\"torch\")\n",
    "# qa_test_tok.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e30f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # Decode predictions and references\n",
    "    decoded_preds = t5_qa_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = t5_qa_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    # Compute ROUGE\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return {\n",
    "        \"rougeL\": result[\"rougeL\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "qa_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./saved_models/t5_qa\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\"\n",
    ")\n",
    "\n",
    "# Assert to ensure tokenized data is defined\n",
    "assert 'qa_train_tok' in globals() and 'qa_test_tok' in globals(), \"Tokenized QA data is not defined.\"\n",
    "\n",
    "# Create the Seq2SeqTrainer instance\n",
    "qa_trainer = Seq2SeqTrainer(\n",
    "    model=t5_qa_model,\n",
    "    args=qa_training_args,\n",
    "    train_dataset=qa_train_tok,\n",
    "    eval_dataset=qa_test_tok,\n",
    "    tokenizer=t5_qa_tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=t5_qa_tokenizer, model=t5_qa_model),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "qa_trainer.train()\n",
    "t5_qa_model.save_pretrained(\"./saved_models/t5_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before training, check if a fine-tuned model exists and load it.\n",
    "\n",
    "if os.path.isdir(SAVE_ROOT / \"t5_qa\"):\n",
    "    print(\"Loading previously fine-tuned T5 QA model...\")\n",
    "    qa_model = T5ForConditionalGeneration.from_pretrained(SAVE_ROOT / \"t5_qa\").to(device)\n",
    "else:\n",
    "    print(\"Loading base T5 QA model...\")\n",
    "    qa_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train T5 for Question‑Answer\n",
    "\n",
    "print(\"Training T5 for the Question‑Answer task.\")\n",
    "\n",
    "# Load (or initialize) the QA model and tokenizer.\n",
    "# Here we fine-tune the base T5 model (t5-small) for QA.\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(t5_resp_model_name).to(device)\n",
    "# Note: We're reusing tokenizer_t5 from the response generation section.\n",
    "\n",
    "# Create a DataCollator for Seq2Seq tasks.\n",
    "qa_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer_t5,\n",
    "    model=qa_model,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1603e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained QA model and tokenizer\n",
    "\n",
    "(SAVE_ROOT / \"t5_qa\").mkdir(exist_ok=True, parents=True)\n",
    "qa_model.save_pretrained(SAVE_ROOT / \"t5_qa\")\n",
    "tokenizer_t5.save_pretrained(SAVE_ROOT / \"t5_qa\")\n",
    "qa_trainer.save_model()\n",
    "tokenizer_t5.save_pretrained(qa_training_args.output_dir)\n",
    "\n",
    "print(\"T5 QA model and tokenizer saved to\", SAVE_ROOT / \"t5_qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a35b0",
   "metadata": {},
   "source": [
    "## Model Deployment Setup with Combined Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55db508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save Combined Model Metadata\n",
    "\n",
    "metadata = {\n",
    "    \"emotion_classifier\": str(SAVE_ROOT / \"emotion_classifier\"),\n",
    "    \"flan_t5_response_generator\": str(SAVE_ROOT / \"flan_t5_response_generator\"), \n",
    "    \"t5_qa\": str(SAVE_ROOT / \"t5_qa\")\n",
    "}\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "torch.save(metadata, metadata_path)\n",
    "print(\"Saved combined model metadata to:\", metadata_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Models Using Combined Metadata\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(map_location=device, metadata_path)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")\n",
    "\n",
    "# Load Emotion Classification Model & Tokenizer from metadata\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(model_paths[\"emotion_classifier\"]).to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"emotion_classifier\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"All models loaded from metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8533473",
   "metadata": {},
   "source": [
    "## Unified Pipeline & Gradio for the Mental Health Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1befa803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unified Pipeline & Gradio for the Mental Health Chatbot\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
    "\n",
    "# ---- Load Fine‑Tuned Models Using Combined Metadata ----\n",
    "# Define the path to the combined metadata file.\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(map_location=device, metadata_path)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Emotion Classification Model & Tokenizer from metadata.\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(model_paths[\"emotion_classifier\"]).to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"emotion_classifier\"])\n",
    "emo_model.eval()  # Set emotion classifier to evaluation mode\n",
    "\n",
    "# Load FLAN-T5 Response Generation Model & Tokenizer from metadata.\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(model_paths[\"flan_t5_response_generator\"]).to(device)\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"flan_t5_response_generator\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10565767",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---- Define Emotion Labels ----\n",
    "# These labels match those used in our emotion annotation step.\n",
    "DEFAULT_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'embarrassment', 'excitement', 'fear', 'gratitude',\n",
    "    'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n",
    "    'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "NUM_EMO_LABELS = emo_model.config.num_labels\n",
    "EMOTION_LABELS = DEFAULT_LABELS[:NUM_EMO_LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, define a subset for emotion-based routing.\n",
    "emotion_router_labels = {'confusion', 'caring', 'nervousness', 'grief', 'sadness', 'fear', 'remorse', 'love', 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4828899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper Functions ----\n",
    "\n",
    "def detect_emotions(text):\n",
    "    \"\"\"\n",
    "    Detects emotions in the provided text using the fine‑tuned emotion classifier.\n",
    "    Returns a list of emotion labels whose corresponding probabilities exceed 0.3.\n",
    "    \"\"\"\n",
    "    inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(emo_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = emo_model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    detected = [EMOTION_LABELS[i] for i, p in enumerate(probs) if p > 0.3]\n",
    "    return detected if detected else [\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34836f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_prompt(user_input, language=\"English\", history=None, emotions=None):\n",
    "    emotion_note = \"\"\n",
    "    if emotions and emotions != [\"neutral\"]:\n",
    "        emotion_note = f\"The user seems to feel {' and '.join(emotions)}. \"\n",
    "    if history:\n",
    "        combined = \"\\n\".join(history + [user_input])\n",
    "        return (f\"respond: {emotion_note}The conversation so far:\\n{combined}\")\n",
    "    return (f\"respond: {emotion_note}{user_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified Chatbot Pipeline Class\n",
    "\n",
    "class MentalHealthChatbotPipeline:\n",
    "    def __init__(self, labels, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.labels = labels\n",
    "        self.chat_history = []  \n",
    "        \n",
    "        # Ensure models are in evaluation mode.\n",
    "        self.emo_model = emo_model.eval()\n",
    "        self.qa_model = qa_model.eval()\n",
    "        self.resp_model = resp_model.eval()\n",
    "\n",
    "    def __call__(self, text, max_length=64):\n",
    "        \"\"\"\n",
    "        Processes user text, detects emotions, routes the input to the appropriate model,\n",
    "        and returns a dictionary with detected emotions, the reply, and the updated conversation history.\n",
    "        \"\"\"\n",
    "        self.chat_history.append((\"User\", text))\n",
    "        \n",
    "        # Emotion Detection\n",
    "        emotions = detect_emotions(text)\n",
    "        \n",
    "        # Model Selection Logic\n",
    "        # Simple decision: if the input contains a question mark, use the QA model; otherwise, use the response generation model.\n",
    "        if \"?\" in text:\n",
    "            model, tokenizer = self.qa_model, qa_tokenizer\n",
    "        else:\n",
    "            model, tokenizer = self.resp_model, resp_tokenizer\n",
    "\n",
    "        # Generate Response\n",
    "        # (Optionally, you could use format_input_prompt to incorporate history.)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_length=max_length)\n",
    "        reply = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        self.chat_history.append((\"Bot\", reply))\n",
    "        \n",
    "        return {\"Detected Emotions\": emotions, \"Response\": reply, \"History\": self.chat_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8306fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chatbot_response(user_text, audio_input, mode, language, use_history, history, route_by_emotion, persist):\n",
    "    history = history or []\n",
    "    user_input = user_text if mode == \"text\" else transcribe_audio(audio_input)\n",
    "    emotions = detect_emotions(user_input)\n",
    "\n",
    "    use_resp_model = any(e in emotion_router_labels for e in emotions) if route_by_emotion else False\n",
    "\n",
    "    if use_resp_model:\n",
    "        prompt = format_input_prompt(\n",
    "            user_input,\n",
    "            language=language,\n",
    "            history=history if use_history else None,\n",
    "            emotions=emotions\n",
    "        )\n",
    "        inputs = resp_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(resp_model.device)\n",
    "        model, tokenizer = resp_model, resp_tokenizer\n",
    "    else:\n",
    "        prompt = \"question: \" + user_input\n",
    "        inputs = qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(qa_model.device)\n",
    "        model, tokenizer = qa_model, qa_tokenizer\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    full_history = history + [f\"User: {user_input}\", f\"Bot: {response}\"]\n",
    "\n",
    "    if persist:\n",
    "        with open(\"chatlog.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"\\n[{datetime.datetime.now()}]\\n{full_history[-2]}\\n{full_history[-1]}\\nDetected emotions: {emotions}\\n\")\n",
    "\n",
    "    return response, \", \".join(emotions), full_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the chatbot pipeline.\n",
    "chatbot = MentalHealthChatbotPipeline(labels=EMOTION_LABELS, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95721f",
   "metadata": {},
   "source": [
    "## Gradio Interface for the Mental Health Chatbot (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0299535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Full-Functionality Gradio Interface for the Mental Health Chatbot\n",
    "\n",
    "\n",
    "# ---- Load Fine-Tuned Models Using Combined Metadata ----\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(map_location=device, metadata_path)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Response Generation Model & Tokenizer\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],  # This may vary depending on architecture\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "resp_model = get_peft_model(base_model, lora_config)\n",
    "resp_model.print_trainable_parameters()  # Optional: view trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17593d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Emotion Labels\n",
    "# Define the complete list of 28 emotion labels (as in the GoEmotions baseline)\n",
    "DEFAULT_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "    'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "    'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EMO_LABELS = emo_model.config.num_labels\n",
    "EMOTION_LABELS = DEFAULT_LABELS[:NUM_EMO_LABELS]\n",
    "\n",
    "# Define a subset of emotions for routing decisions (if needed)\n",
    "emotion_router_labels = set(EMOTION_LABELS) & {'confusion', 'caring', 'nervousness', 'grief', 'sadness', 'fear', 'remorse', 'love', 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_prompt(user_input, language=\"English\", history=None, emotions=None):\n",
    "    emotion_note = \"\"\n",
    "    if emotions and emotions != [\"neutral\"]:\n",
    "        emotion_note = f\"The user seems to feel {' and '.join(emotions)}. \"\n",
    "    if history:\n",
    "        combined = \"\\n\".join(history + [user_input])\n",
    "        return (f\"respond: {emotion_note}The conversation so far:\\n{combined}\")\n",
    "    return (f\"respond: {emotion_note}{user_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35881b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_emotions(text):\n",
    "    \"\"\"\n",
    "    Detects emotions in the provided text using the fine‑tuned emotion classifier.\n",
    "    Returns a list of emotion labels whose corresponding probabilities exceed 0.3.\n",
    "    \"\"\"\n",
    "    inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(emo_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = emo_model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    \n",
    "    # Safety check: trim probabilities if there are more than expected.\n",
    "    if len(probs) > len(EMOTION_LABELS):\n",
    "        print(f\"Warning: Received {len(probs)} probabilities; expected {len(EMOTION_LABELS)}. Trimming extra values.\")\n",
    "        probs = probs[:len(EMOTION_LABELS)]\n",
    "    \n",
    "    detected = [EMOTION_LABELS[i] for i, p in enumerate(probs) if p > 0.3]\n",
    "    return detected if detected else [\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"\n",
    "    Converts an input audio file to text using speech recognition.\n",
    "    Returns the transcribed text or an error message.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp:\n",
    "        audio.export(tmp.name, format=\"wav\")\n",
    "        with sr.AudioFile(tmp.name) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            try:\n",
    "                return recognizer.recognize_google(audio_data)\n",
    "            except sr.UnknownValueError:\n",
    "                return \"[Unrecognized speech]\"\n",
    "            except sr.RequestError:\n",
    "                return \"[Speech recognition failed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6981e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chatbot_response(user_text, audio_input, mode, language, use_history, history, route_by_emotion, persist):\n",
    "    history = history or []\n",
    "    user_input = user_text if mode == \"text\" else transcribe_audio(audio_input)\n",
    "    emotions = detect_emotions(user_input)\n",
    "\n",
    "    use_resp_model = any(e in emotion_router_labels for e in emotions) if route_by_emotion else False\n",
    "\n",
    "    if use_resp_model:\n",
    "        prompt = format_input_prompt(\n",
    "            user_input,\n",
    "            language=language,\n",
    "            history=history if use_history else None,\n",
    "            emotions=emotions\n",
    "        )\n",
    "        inputs = resp_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(resp_model.device)\n",
    "        model, tokenizer = resp_model, resp_tokenizer\n",
    "    else:\n",
    "        prompt = \"question: \" + user_input\n",
    "        inputs = qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(qa_model.device)\n",
    "        model, tokenizer = qa_model, qa_tokenizer\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=64,\n",
    "            num_beams=4,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    full_history = history + [f\"User: {user_input}\", f\"Bot: {response}\"]\n",
    "\n",
    "    if persist:\n",
    "        with open(\"chatlog.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"\\n[{datetime.datetime.now()}]\\n{full_history[-2]}\\n{full_history[-1]}\\nDetected emotions: {emotions}\\n\")\n",
    "\n",
    "    return response, \", \".join(emotions), full_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ecdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Gradio Interface\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_chatbot_response,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Type your message here (if using text mode)\"),\n",
    "        gr.Audio(type=\"filepath\", label=\"Or speak here (if using voice mode)\"),\n",
    "        gr.Radio([\"text\", \"voice\"], value=\"text\", label=\"Input Mode\"),\n",
    "        gr.Dropdown(choices=[\"English\", \"German\", \"Spanish\", \"French\"], value=\"English\", label=\"Response Language\"),\n",
    "        gr.Checkbox(label=\"Include chat history in response\", value=True),\n",
    "        gr.State(value=[]),\n",
    "        gr.Checkbox(label=\"Route by detected emotion\", value=True),\n",
    "        gr.Checkbox(label=\"Save conversation to chatlog.txt\", value=True)\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Therapist Response\"),\n",
    "        gr.Textbox(label=\"Detected Emotions\"),\n",
    "        gr.State()\n",
    "    ],\n",
    "    title=\"Voice + Text Enabled Emotion-Aware Mental Health Chatbot\",\n",
    "    description=\"You can type or speak your message. Emotion-aware routing decides between Q&A and therapist-style support.\"\n",
    ")\n",
    "\n",
    "# Launch the interface.\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Streamlit Interface\n",
    "# st.title(\"Voice + Text Enabled Emotion-Aware Mental Health Chatbot\")\n",
    "# st.write(\"You can type or speak your message. Emotion-aware routing decides between Q&A and therapist-style support.\")\n",
    "\n",
    "# # Initialize conversation history\n",
    "# history = []\n",
    "\n",
    "# # Input mode\n",
    "# mode = st.radio(\"Input Mode\", [\"text\", \"voice\"])\n",
    "\n",
    "# # Language selection\n",
    "# language = st.selectbox(\"Response Language\", [\"English\", \"German\", \"Spanish\", \"French\"])\n",
    "\n",
    "# # Include chat history in response\n",
    "# use_history = st.checkbox(\"Include chat history in response\")\n",
    "\n",
    "# # Route by detected emotion\n",
    "# route_by_emotion = st.checkbox(\"Route by detected emotion\")\n",
    "\n",
    "# # Save conversation to chatlog.txt\n",
    "# persist = st.checkbox(\"Save conversation to chatlog.txt\")\n",
    "\n",
    "# # Generate chatbot response\n",
    "# if mode == \"text\":\n",
    "#     user_input = st.text_input(\"Type your message here\")\n",
    "# else:\n",
    "#     audio_input = st.file_uploader(\"Or speak here\", type=[\"wav\", \"mp3\", \"ogg\"])\n",
    "\n",
    "# if st.button(\"Generate Response\"):\n",
    "#     if mode == \"text\":\n",
    "#         response, emotions, history = generate_chatbot_response(user_input, None, mode, language, use_history, history, route_by_emotion, persist)\n",
    "#     else:\n",
    "#         response, emotions, history = generate_chatbot_response(None, audio_input, mode, language, use_history, history, route_by_emotion, persist)\n",
    "\n",
    "#     st.write(f\"Therapist Response: {response}\")\n",
    "#     st.write(f\"Detected Emotions: {emotions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf78da",
   "metadata": {},
   "source": [
    "## Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb02ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Evaluation Cell\n",
    "\n",
    "\n",
    "### Evaluation for Emotion Classification ###\n",
    "\n",
    "def evaluate_emotion_classifier(model, tokenizer, dataset, batch_size=16):\n",
    "    \"\"\"\n",
    "    Evaluate the emotion classifier over the provided dataset.\n",
    "    Computes micro-averaged F1, Precision, Recall, and Subset Accuracy.\n",
    "    Assumes that dataset is formatted with columns \"input_ids\", \"attention_mask\", \"labels\"\n",
    "    and that labels is a multi-label binary vector.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Create a DataLoader for batch processing (if dataset is not huge, you can loop through it directly)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Move inputs and labels to device\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].numpy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        # Apply sigmoid for multi-label classification and threshold at 0.3\n",
    "        preds = (torch.sigmoid(logits) > 0.3).cpu().numpy().astype(int)\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    # Compute micro-averaged metrics\n",
    "    micro_f1 = f1_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    micro_precision = precision_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    micro_recall = recall_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    subset_acc = accuracy_score(all_labels, all_preds)  # subset accuracy is strict\n",
    "    \n",
    "    return {\n",
    "        \"Emotion Classifier Micro-F1\": micro_f1,\n",
    "        \"Emotion Classifier Micro-Precision\": micro_precision,\n",
    "        \"Emotion Classifier Micro-Recall\": micro_recall,\n",
    "        \"Emotion Classifier Subset Accuracy\": subset_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05735854",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Emotion Classifier...\")\n",
    "emo_metrics = evaluate_emotion_classifier(emo_model, emo_tokenizer, emo_test_tok)\n",
    "for metric, value in emo_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abddcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for Generation Models (Response Generation and QA)\n",
    "\n",
    "# We already defined a compute_resp_metrics function in the training cells.\n",
    "# Here, we define a helper to compute additional perplexity based on the evaluation loss.\n",
    "\n",
    "def evaluate_generation_model(trainer, test_dataset):\n",
    "    \"\"\"\n",
    "    Uses the Seq2SeqTrainer to compute evaluation metrics over the given test dataset.\n",
    "    Adds perplexity (exp(eval_loss)) to the standard metrics.\n",
    "    \"\"\"\n",
    "    # Predict returns a dictionary with metrics: eval_loss, and any metrics computed in compute_metrics.\n",
    "    result = trainer.predict(test_dataset)\n",
    "    eval_loss = result.metrics.get(\"eval_loss\")\n",
    "    \n",
    "    # Compute perplexity if loss is available. (If eval_loss is zero or not available, perplexity is undefined.)\n",
    "    if eval_loss is not None and eval_loss > 0:\n",
    "        perplexity = math.exp(eval_loss)\n",
    "    else:\n",
    "        perplexity = float(\"inf\")\n",
    "    \n",
    "    # Add perplexity to the metrics dictionary.\n",
    "    result.metrics[\"perplexity\"] = perplexity\n",
    "    return result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating T5 Response Generation Model...\")\n",
    "resp_metrics = evaluate_generation_model(trainer_resp, resp_test_tok)\n",
    "for metric, value in resp_metrics.items():\n",
    "    print(f\"T5 Response Generation {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating T5 QA Model...\")\n",
    "qa_metrics = evaluate_generation_model(trainer_qa, qa_test_tok)\n",
    "for metric, value in qa_metrics.items():\n",
    "    print(f\"T5 QA {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, you can evaluate other metrics such as ROUGE and BERTScore separately using the `evaluate` library,\n",
    "# if desired. For example:\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_generation_metrics(trainer, test_dataset, tokenizer):\n",
    "    result = trainer.predict(test_dataset)\n",
    "    predictions, labels = result.predictions, result.label_ids\n",
    "    # Replace -100 in labels by the tokenizer pad id.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    r = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    b = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    \n",
    "    # Here, we only extract the ROUGE-L and average BERTScore F1.\n",
    "    metrics = {\n",
    "        \"rougeL\": r[\"rougeL\"],\n",
    "        \"bertscore_f1\": np.mean(b[\"f1\"])\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAdditional Generation Metrics for T5 Response Generation:\")\n",
    "additional_resp_metrics = compute_generation_metrics(trainer_resp, resp_test_tok, tokenizer_t5)\n",
    "for metric, value in additional_resp_metrics.items():\n",
    "    print(f\"T5 Response Generation {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nAdditional Generation Metrics for T5 QA Model:\")\n",
    "additional_qa_metrics = compute_generation_metrics(trainer_qa, qa_test_tok, tokenizer_t5)\n",
    "for metric, value in additional_qa_metrics.items():\n",
    "    print(f\"T5 QA {metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
