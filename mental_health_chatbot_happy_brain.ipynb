{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9254d42",
   "metadata": {},
   "source": [
    "\n",
    "# Mental Health Chatbot Training Notebook\n",
    "\n",
    "This notebook provides a comprehensive pipeline to train a conversational mental health assistant. \n",
    "The system integrates emotion classification using `SamLowe/roberta-base-go_emotions` and text generation using `T5`.\n",
    "It processes multiple cleaned datasets, performs training, evaluation, and finally builds a chatbot interface using Gradio or Streamlit.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Load and preprocess multiple mental health-related datasets into a consistent question/answer format\n",
    "- Simulate and encode multi-label emotion annotations using `MultiLabelBinarizer`\n",
    "- Train a RoBERTa-based emotion classification model with live metric logging (accuracy, F1, precision, recall)\n",
    "- Fine-tune two separate T5 models:\n",
    "  - One for emotionally guided chatbot response generation\n",
    "  - One for direct factual Q&A answering\n",
    "- Implement emotion-aware routing logic that selects the appropriate model at inference time\n",
    "- Create a unified RoBERTa + T5 pipeline for real-time response generation\n",
    "- Build a Gradio chatbot interface using the full system\n",
    "- Evaluate all models with live logging and inference testing\n",
    "- Save all models and tokenizers in `./saved_models/`\n",
    "- Save a `.pt` metadata file pointing to model paths for easy deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81dff8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mward\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "\n",
    "import pandas as pd\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# PyTorch & Transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "from torchvision.ops import nms\n",
    "import shutil\n",
    "# Hugging Face Datasets & Evaluation\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "# External Evaluation Libraries\n",
    "import bert_score\n",
    "import rouge_score\n",
    "\n",
    "# Gradio for UI\n",
    "import gradio as gr\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Optional for advanced memory management (not required unless doing lazy loading)\n",
    "from accelerate import init_empty_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a8d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "Current Device Index: 0\n",
      "Device Name: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "print(\"GPU Available:\", torch.cuda.is_available())  # True if a GPU is accessible\n",
    "print(\"Current Device Index:\", torch.cuda.current_device())  # e.g., 0\n",
    "print(\"Device Name:\", torch.cuda.get_device_name(0))  # e.g., \"NVIDIA GeForce RTX 3070\"\n",
    "\n",
    "# Example: set default tensor type to GPU-based FloatTensor (optional)\n",
    "# This will make ALL newly created tensors go to GPU (Float32).\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "# torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7328221",
   "metadata": {},
   "source": [
    "## Load and Preprocess Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e48dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 154\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question', 'answer'],\n",
       "     num_rows: 18\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "# File paths\n",
    "dataset_paths = {\n",
    "    \"ds1\": \"./data/ds1_transformed_mental_health_chatbot_dataset.csv\",\n",
    "    \"ds2\": \"./data/ds2_transformed_mental_health_chatbot.csv\",\n",
    "    \"ds3\": \"./data/ds3_mental_health_faq_cleaned.csv\",\n",
    "    \"ds4\": \"./data/ds4_mental_health_chatbot_dataset_merged_modes.csv\",\n",
    "    \"ds5\": \"./data/ds5_Mental_Health_FAQ.csv\",\n",
    "    \"ds6\": \"./data/ds6_mental_health_counseling.csv\"\n",
    "}\n",
    "\n",
    "# Enable/disable datasets\n",
    "dataset_switches = {\n",
    "    \"ds1\": True,\n",
    "    \"ds2\": False,\n",
    "    \"ds3\": False,\n",
    "    \"ds4\": False,\n",
    "    \"ds5\": False,\n",
    "    \"ds6\": False\n",
    "}\n",
    "\n",
    "# Clean function\n",
    "def load_and_clean_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [col.lower().strip() for col in df.columns]\n",
    "    if \"prompt\" in df.columns and \"response\" in df.columns:\n",
    "        df = df.rename(columns={\"prompt\": \"question\", \"response\": \"answer\"})\n",
    "    if \"questions\" in df.columns:\n",
    "        df = df.rename(columns={\"questions\": \"question\"})\n",
    "    if \"answers\" in df.columns:\n",
    "        df = df.rename(columns={\"answers\": \"answer\"})\n",
    "    df = df[[col for col in df.columns if col in [\"question\", \"answer\"]]]\n",
    "    df.dropna(inplace=True)\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "# Load selected datasets\n",
    "datasets_list = []\n",
    "for name, path in dataset_paths.items():\n",
    "    if dataset_switches[name]:\n",
    "        datasets_list.append(load_and_clean_csv(path))\n",
    "\n",
    "# Merge and split\n",
    "combined_dataset = concatenate_datasets(datasets_list).shuffle(seed=42)\n",
    "split_dataset = combined_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_ds = split_dataset[\"train\"]\n",
    "test_ds = split_dataset[\"test\"]\n",
    "\n",
    "train_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643608e6",
   "metadata": {},
   "source": [
    "## Prepare 'labels' Column with MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4287b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = train_ds.to_pandas()\n",
    "df_test = test_ds.to_pandas()\n",
    "\n",
    "# Simulated emotion annotations (replace with real data if available)\n",
    "import random\n",
    "emotions_list = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "                 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "                 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization',\n",
    "                 'relief', 'remorse', 'sadness', 'surprise']\n",
    "df_train[\"emotions\"] = [random.sample(emotions_list, k=random.randint(1, 3)) for _ in range(len(df_train))]\n",
    "df_test[\"emotions\"] = [random.sample(emotions_list, k=random.randint(1, 3)) for _ in range(len(df_test))]\n",
    "\n",
    "# Encode with MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=emotions_list)\n",
    "df_train[\"labels\"] = list(mlb.fit_transform(df_train[\"emotions\"]))\n",
    "df_test[\"labels\"] = list(mlb.transform(df_test[\"emotions\"]))\n",
    "\n",
    "# Convert back to Hugging Face Dataset\n",
    "train_ds = Dataset.from_pandas(df_train)\n",
    "test_ds = Dataset.from_pandas(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d45158",
   "metadata": {},
   "source": [
    "## Configure RoBERTa for Multi-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65350e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import RobertaConfig, RobertaForSequenceClassification\n",
    "\n",
    "# Explicitly define configuration for multi-label classification\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# Load model with custom config\n",
    "model_emo = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    config=config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4b245b",
   "metadata": {},
   "source": [
    "## Train RoBERTa with Correct Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fcc73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously trained model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bd4a7191d74b53a68513dec03a734e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf02a7935c04b9c952d455cd476271b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0134d138d79143709ace6a88ef718adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea7507b721540d1bc385370c44146ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_4056\\1844043087.py:91: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_emo = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [53/60 00:46 < 00:06, 1.10 it/s, Epoch 2.60/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>0.273434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271400</td>\n",
       "      <td>0.272481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import (\n",
    "    RobertaTokenizer, RobertaForSequenceClassification,\n",
    "    Trainer, TrainingArguments, RobertaConfig,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Features, Value, Sequence\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Model Save Directory\n",
    "model_path = \"./saved_models/emotion_classifier\"\n",
    "\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_emo = RobertaTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Load existing model if found, otherwise initialize from base\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading previously trained model...\")\n",
    "    config = RobertaConfig.from_pretrained(model_path)\n",
    "    model_emo = RobertaForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "else:\n",
    "    print(\"No existing model found. Starting fresh.\")\n",
    "    config = RobertaConfig.from_pretrained(\"SamLowe/roberta-base-go_emotions\", problem_type=\"multi_label_classification\")\n",
    "    model_emo = RobertaForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\", config=config)\n",
    "\n",
    "# Define label format\n",
    "features = Features({\n",
    "    \"question\": Value(\"string\"),\n",
    "    \"answer\": Value(\"string\"),\n",
    "    \"emotions\": Sequence(Value(\"string\")),\n",
    "    \"labels\": Sequence(Value(\"float32\")) \n",
    "})\n",
    "\n",
    "# Apply format to datasets\n",
    "train_ds = train_ds.cast(features)\n",
    "test_ds = test_ds.cast(features)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_emotion(example):\n",
    "    enc = tokenizer_emo(example[\"question\"], padding=\"max_length\", truncation=True)\n",
    "    enc[\"labels\"] = example[\"labels\"]\n",
    "    return enc\n",
    "\n",
    "# Tokenize\n",
    "train_emo = train_ds.map(tokenize_emotion, batched=False)\n",
    "test_emo = test_ds.map(tokenize_emotion, batched=False)\n",
    "\n",
    "train_emo.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_emo.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer_emo, return_tensors=\"pt\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args_emo = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",        \n",
    "    logging_steps=10,                \n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=False, \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer_emo = Trainer(\n",
    "    model=model_emo,\n",
    "    args=training_args_emo,\n",
    "    train_dataset=train_emo,\n",
    "    eval_dataset=test_emo,\n",
    "    tokenizer=tokenizer_emo,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer_emo.train()\n",
    "trainer_emo.save_model(model_path)\n",
    "\n",
    "\n",
    "# Trainer setup\n",
    "trainer_emo = Trainer(\n",
    "    model=model_emo,\n",
    "    args=training_args_emo,\n",
    "    train_dataset=train_emo,\n",
    "    eval_dataset=test_emo,\n",
    "    tokenizer=tokenizer_emo,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "# Training\n",
    "trainer_emo.train()\n",
    "\n",
    "# Final save (no overwrite here, path already protected)\n",
    "trainer_emo.save_model(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a7782",
   "metadata": {},
   "source": [
    "## Train T5 for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85bcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found empty or invalid model folder. Deleting it...\n",
      "No valid saved model found. Loading 't5-small' fresh.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d476152484234d82b3fa5c63091e7b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed4af780ed147cc9467c7bdec98f6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_25344\\3013294863.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_chat = Trainer(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 105\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m     96\u001b[0m trainer_chat \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     97\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_t5_response,\n\u001b[0;32m     98\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args_chat,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics \n\u001b[0;32m    103\u001b[0m )\n\u001b[1;32m--> 105\u001b[0m \u001b[43mtrainer_chat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m trainer_chat\u001b[38;5;241m.\u001b[39msave_model(t5_response_path)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Trainer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2546\u001b[0m )\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3704\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1893\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1890\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1893\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1902\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1907\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1909\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1911\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\models\\t5\\modeling_t5.py:1002\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to initialize the model with valid token embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1002\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1004\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Define model save path\n",
    "t5_response_path = \"./saved_models/t5_response_generator\"\n",
    "\n",
    "# Delete invalid or empty model folder if necessary\n",
    "if os.path.exists(t5_response_path) and not any(\n",
    "    fname.endswith((\".bin\", \".safetensors\", \".h5\", \".index\", \".msgpack\")) for fname in os.listdir(t5_response_path)\n",
    "):\n",
    "    print(\"Found empty or invalid model folder. Deleting it...\")\n",
    "    shutil.rmtree(t5_response_path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load model safely\n",
    "if os.path.exists(t5_response_path) and os.path.exists(os.path.join(t5_response_path, \"pytorch_model.bin\")):\n",
    "    print(\"Loading previously trained T5 response model...\")\n",
    "    model_t5_response = T5ForConditionalGeneration.from_pretrained(t5_response_path)\n",
    "else:\n",
    "    print(\"No valid saved model found. Loading 't5-small' fresh.\")\n",
    "    model_t5_response = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "def tokenize_t5_chat(example):\n",
    "    input_text = \"chat: \" + example[\"question\"]\n",
    "    target_text = example[\"answer\"]\n",
    "\n",
    "    model_inputs = tokenizer_t5(\n",
    "        input_text,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(\n",
    "            target_text,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "    # Ensure data returned as plain Python int lists\n",
    "    return {\n",
    "        \"input_ids\": [int(i) for i in model_inputs[\"input_ids\"]],\n",
    "        \"attention_mask\": [int(i) for i in model_inputs[\"attention_mask\"]],\n",
    "        \"labels\": [int(i) for i in labels]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "train_chat = train_ds.map(tokenize_t5_chat)\n",
    "test_chat = test_ds.map(tokenize_t5_chat)\n",
    "\n",
    "train_chat.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_chat.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args_chat = TrainingArguments(\n",
    "    output_dir=t5_response_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",        \n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=False,  # <--- Prevent overwriting!\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer_chat = Trainer(\n",
    "    model=model_t5_response,\n",
    "    args=training_args_chat,\n",
    "    train_dataset=train_chat,\n",
    "    eval_dataset=test_chat,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    compute_metrics=compute_metrics \n",
    ")\n",
    "\n",
    "trainer_chat.train()\n",
    "trainer_chat.save_model(t5_response_path)\n",
    "\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer_chat = Trainer(\n",
    "    model=model_t5_response,\n",
    "    args=training_args_chat,\n",
    "    train_dataset=train_chat,\n",
    "    eval_dataset=test_chat,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and Save\n",
    "trainer_chat.train()\n",
    "trainer_chat.save_model(t5_response_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefcdc2d",
   "metadata": {},
   "source": [
    "## Train T5 for Q&A Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "# Define model save path\n",
    "t5_qa_path = \"./saved_models/t5_qa\"\n",
    "\n",
    "# Load existing model or initialize new\n",
    "if os.path.exists(t5_qa_path):\n",
    "    print(\"🔁 Loading previously trained T5 Q&A model...\")\n",
    "    model_t5_qa = T5ForConditionalGeneration.from_pretrained(t5_qa_path)\n",
    "else:\n",
    "    print(\"🆕 No existing Q&A model found. Starting from scratch.\")\n",
    "    model_t5_qa = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_t5_qa(example):\n",
    "    input_text = \"question: \" + example[\"question\"]\n",
    "    target_text = example[\"answer\"]\n",
    "    model_inputs = tokenizer_t5(input_text, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(target_text, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "train_qa = train_ds.map(tokenize_t5_qa, batched=True)\n",
    "test_qa = test_ds.map(tokenize_t5_qa, batched=True)\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro', zero_division=0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args_qa = TrainingArguments(\n",
    "    output_dir=t5_qa_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",        \n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    load_best_model_at_end=True,\n",
    "    overwrite_output_dir=False,  # <--- Protects against overwrite\n",
    "    report_to=\"none\",\n",
    "    compute_metrics=compute_metrics\n",
    "    \n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer_qa = Trainer(\n",
    "    model=model_t5_qa,\n",
    "    args=training_args_qa,\n",
    "    train_dataset=train_qa,\n",
    "    eval_dataset=test_qa,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train and save\n",
    "trainer_qa.train()\n",
    "trainer_qa.save_model(t5_qa_path)\n",
    "\n",
    "# Trainer setup\n",
    "trainer_qa = Trainer(\n",
    "    model=model_t5_qa,\n",
    "    args=training_args_qa,\n",
    "    train_dataset=train_qa,\n",
    "    eval_dataset=test_qa,\n",
    "    tokenizer=tokenizer_t5\n",
    ")\n",
    "\n",
    "# Train and save\n",
    "trainer_qa.train()\n",
    "trainer_qa.save_model(t5_qa_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0b4f8",
   "metadata": {},
   "source": [
    "## Unified Emotion-Aware Response System (RoBERTa + T5 Routing Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d4c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    RobertaForSequenceClassification, RobertaTokenizer,\n",
    "    T5ForConditionalGeneration, T5Tokenizer\n",
    ")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load models and tokenizers\n",
    "emotion_model = RobertaForSequenceClassification.from_pretrained(\"./saved_models/emotion_classifier\")\n",
    "emotion_tokenizer = RobertaTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "t5_chat_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/t5_response_generator\")\n",
    "t5_qa_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/t5_qa\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Emotion labels from GoEmotions dataset\n",
    "emotion_labels = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "    'gratitude', 'grief', 'joy', 'love', 'nervousness', 'neutral', 'optimism', 'pride', 'realization',\n",
    "    'relief', 'remorse', 'sadness', 'surprise'\n",
    "]\n",
    "\n",
    "# Emotion classifier\n",
    "def detect_emotions(text, threshold=0.5):\n",
    "    emotion_model.eval()\n",
    "    inputs = emotion_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = emotion_model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits).squeeze().tolist()\n",
    "    detected = [(emotion_labels[i], p) for i, p in enumerate(probs) if p > threshold]\n",
    "    return [label for label, _ in detected]\n",
    "\n",
    "# Response generator with dynamic routing\n",
    "def generate_combined_response(user_input):\n",
    "    emotions = detect_emotions(user_input)\n",
    "\n",
    "    emotional_keywords = {\n",
    "        'joy', 'sadness', 'anger', 'fear', 'love', 'grief', 'remorse', 'disappointment', 'gratitude', 'caring'\n",
    "    }\n",
    "    use_chat_model = any(e in emotional_keywords for e in emotions)\n",
    "\n",
    "    if use_chat_model:\n",
    "        prefix = \"chat: \"\n",
    "        model = t5_chat_model\n",
    "    else:\n",
    "        prefix = \"question: \"\n",
    "        model = t5_qa_model\n",
    "\n",
    "    context = f\"{' '.join(emotions)}: {user_input}\" if emotions else user_input\n",
    "    input_text = prefix + context\n",
    "\n",
    "    inputs = t5_tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(inputs[\"input_ids\"], max_length=128)\n",
    "    response = t5_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"Detected Emotions\": emotions,\n",
    "        \"Model Used\": \"Chat Model\" if use_chat_model else \"QA Model\",\n",
    "        \"Response\": response\n",
    "    }\n",
    "\n",
    "# 🔍 Test it\n",
    "generate_combined_response(\"I feel really hopeless and angry all the time.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b38b07",
   "metadata": {},
   "source": [
    "## Save All Final Models and Tokenizers (RoBERTa + T5s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save all models and tokenizers using Hugging Face's save_pretrained\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, T5Tokenizer\n",
    "\n",
    "# Load models\n",
    "chat_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/t5_response_generator\")\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(\"./saved_models/t5_qa\")\n",
    "emotion_model = RobertaForSequenceClassification.from_pretrained(\"./saved_models/emotion_classifier\")\n",
    "\n",
    "# Load tokenizers\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "emotion_tokenizer = RobertaTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Save full models and tokenizers\n",
    "chat_model.save_pretrained(\"./saved_models/final/chat_model\")\n",
    "qa_model.save_pretrained(\"./saved_models/final/qa_model\")\n",
    "emotion_model.save_pretrained(\"./saved_models/final/emotion_model\")\n",
    "\n",
    "t5_tokenizer.save_pretrained(\"./saved_models/final/t5_tokenizer\")\n",
    "emotion_tokenizer.save_pretrained(\"./saved_models/final/emotion_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff046f",
   "metadata": {},
   "source": [
    "## Save Final Model Metadata (.pt) for Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83888a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightweight metadata dictionary (pointer-style)\n",
    "final_model_metadata = {\n",
    "    \"chat_model_path\": \"./saved_models/final/chat_model\",\n",
    "    \"qa_model_path\": \"./saved_models/final/qa_model\",\n",
    "    \"emotion_model_path\": \"./saved_models/final/emotion_model\",\n",
    "    \"t5_tokenizer_path\": \"./saved_models/final/t5_tokenizer\",\n",
    "    \"emotion_tokenizer_path\": \"./saved_models/final/emotion_tokenizer\",\n",
    "    \"labels\": emotion_labels  # list of emotion label names only\n",
    "}\n",
    "\n",
    "# Save just this dictionary (no models inside)\n",
    "torch.save(final_model_metadata, \"./saved_models/final_model_metadata.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f42cce",
   "metadata": {},
   "source": [
    "## Gradio Chatbot Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def gradio_chat_interface(user_input):\n",
    "    global chat_history\n",
    "    response_data = generate_combined_response(user_input)\n",
    "    chatbot_response = response_data['Response']\n",
    "    chat_history.append((\"You: \" + user_input, \"Bot: \" + chatbot_response))\n",
    "    return chat_history\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=gradio_chat_interface,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"chatbot\",\n",
    "    title=\"Mental Health Chatbot\",\n",
    "    description=\"Emotion-aware chatbot using RoBERTa + T5\",\n",
    "    live=True\n",
    ")\n",
    "\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609a1ec",
   "metadata": {},
   "source": [
    "## Model Evaluation Metrics and Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9be7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "# Prepare sample data\n",
    "sample_size = min(100, len(test_ds))\n",
    "sample = test_ds.select(range(sample_size))\n",
    "\n",
    "def generate_predictions(model, tokenizer, dataset):\n",
    "    inputs = [f\"question: {x['question']}\" for x in dataset]\n",
    "    inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    output_ids = model.generate(inputs['input_ids'], max_length=128)\n",
    "    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    refs = [x[\"answer\"] for x in dataset]\n",
    "    return preds, refs\n",
    "\n",
    "predictions, references = generate_predictions(model_t5_qa, tokenizer_t5, sample)\n",
    "\n",
    "# Evaluate\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "bertscore_results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "\n",
    "# Binary approximation for classification scores\n",
    "true_labels = [1 if r.strip().lower() in p.strip().lower() else 0 for p, r in zip(predictions, references)]\n",
    "accuracy_result = accuracy.compute(references=true_labels, predictions=true_labels)\n",
    "precision_result = precision.compute(references=true_labels, predictions=true_labels)\n",
    "recall_result = recall.compute(references=true_labels, predictions=true_labels)\n",
    "f1_result = f1.compute(references=true_labels, predictions=true_labels)\n",
    "\n",
    "# Display\n",
    "print(\"ROUGE Scores:\", rouge_results)\n",
    "print(\"BERTScore:\", bertscore_results)\n",
    "print(\"Accuracy:\", accuracy_result)\n",
    "print(\"Precision:\", precision_result)\n",
    "print(\"Recall:\", recall_result)\n",
    "print(\"F1 Score:\", f1_result)\n",
    "\n",
    "# RoBERTa Emotion Classifier - Inference Test\n",
    "sample_input = \"I feel like I'm breaking down and can't handle anything.\"\n",
    "\n",
    "inputs = tokenizer_emo(sample_input, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "model_emo.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model_emo(**inputs).logits\n",
    "probs = torch.sigmoid(logits).squeeze().tolist()\n",
    "\n",
    "predicted_emotions = [emotion_labels[i] for i, p in enumerate(probs) if p > 0.5]\n",
    "print(\"🔍 Detected Emotions:\", predicted_emotions)\n",
    "\n",
    "# T5 QA Model - Inference Test\n",
    "qa_input = \"question: What are some ways to manage daily anxiety?\"\n",
    "inputs = tokenizer_t5(qa_input, return_tensors=\"pt\")\n",
    "\n",
    "qa_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = qa_model.generate(inputs[\"input_ids\"], max_length=128)\n",
    "\n",
    "response = tokenizer_t5.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"💡 QA Model Response:\", response)\n",
    "\n",
    "# T5 QA Model - Inference Test\n",
    "qa_input = \"question: What are some ways to manage daily anxiety?\"\n",
    "inputs = tokenizer_t5(qa_input, return_tensors=\"pt\")\n",
    "\n",
    "qa_model.eval()\n",
    "with torch.no_grad():\n",
    "    output_ids = qa_model.generate(inputs[\"input_ids\"], max_length=128)\n",
    "\n",
    "response = tokenizer_t5.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"💡 QA Model Response:\", response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075581b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emotion_labels[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
