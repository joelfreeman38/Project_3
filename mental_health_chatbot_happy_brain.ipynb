{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d559dfc",
   "metadata": {},
   "source": [
    "# Mental‑Health Chatbot: Happy Brain\n",
    "\n",
    "# Mental Health Chatbot: Training & Deployment Notebook\n",
    "\n",
    "## Project Objective\n",
    "\n",
    "This project aims to build an emotion-aware mental health chatbot that is both conversational and empathetic. The system leverages:\n",
    "- **Emotion Classification:** Fine-tuned using `SamLowe/roberta-base-go_emotions` to detect multi-label emotions in user inputs.\n",
    "- **Text Generation:** Two T5 models are fine-tuned for:\n",
    "  - **Response Generation:** To generate supportive, therapist-style responses.\n",
    "  - **Question-Answering (QA):** To provide factual answers when questions are asked.\n",
    "- **Integrated Pipeline:** A routing mechanism that chooses the proper response mode (QA vs. supportive response) based on detected emotions and input type.\n",
    "- **Deployment:** A Gradio interface enabling voice and text-based interactions.\n",
    "\n",
    "## Notebook Structure and Sections\n",
    "\n",
    "1. **Library Imports & Environment Setup**  \n",
    "   Sets up all the necessary imports from standard libraries, data handling, PyTorch, Hugging Face, and Gradio.\n",
    "\n",
    "2. **Data Loading & Preprocessing**  \n",
    "   - Loads multiple mental health-related CSV datasets.\n",
    "   - Performs cleaning and mapping to uniform \"question\"/\"answer\" columns.\n",
    "   - Splits the combined dataset into training and testing sets.\n",
    "\n",
    "3. **Multi-Label Emotion Annotation & Custom Trainer Setup**  \n",
    "   - Processes multi-label emotion annotations using `MultiLabelBinarizer`.\n",
    "   - Defines custom data collators and trainers to support binary cross-entropy loss for multi-label classification.\n",
    "   - Tokenizes and prepares data for the emotion classifier model.\n",
    "\n",
    "4. **Model Training: Emotion Classification**  \n",
    "   - Fine-tunes the `SamLowe/roberta-base-go_emotions` model on the processed data.\n",
    "   - Logs evaluation metrics such as micro F1, precision, recall, and subset accuracy.\n",
    "   - Saves the fine-tuned emotion classifier model.\n",
    "\n",
    "5. **Model Training: T5 for Response Generation**  \n",
    "   - Constructs input/target pairs for response generation (user input -> supportive response).\n",
    "   - Fine-tunes a T5 model (e.g., `t5-small`) with evaluation metrics like ROUGE and BERTScore.\n",
    "   - Saves the fine-tuned T5 response generator model.\n",
    "\n",
    "6. **Model Training: T5 for Question-Answering (QA)**  \n",
    "   - Constructs QA pairs with a specific prompt format.\n",
    "   - Fine-tunes a separate T5 model for QA tasks.\n",
    "   - Saves the fine-tuned T5 QA model.\n",
    "\n",
    "7. **Model Deployment Setup with Combined Metadata**  \n",
    "   - Saves a combined metadata file (`combined_model_metadata.pt`) that stores the paths to all three fine-tuned models.\n",
    "   - Updates the loading code to reference this metadata file, ensuring that future training runs build on previous improvements rather than overwriting models.\n",
    "\n",
    "8. **Unified Pipeline & Gradio Interface**  \n",
    "   - Integrates the emotion classifier, T5 response generator, and T5 QA models into a single pipeline.\n",
    "   - Uses helper functions (for prompt formatting, emotion detection, audio transcription, etc.) to generate responses.\n",
    "   - Provides a fully-functional Gradio interface supporting both text and voice inputs with options for language, chat history, and conversation logging.\n",
    "\n",
    "9. **Evaluation**  \n",
    "   - Evaluates the emotion classifier using metrics such as micro F1, precision, recall, and subset accuracy.\n",
    "   - Evaluates the generation models (both response generation and QA) using metrics including ROUGE, BERTScore, perplexity, and loss.\n",
    "   - Outputs formatted evaluation results for easy monitoring.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook offers a complete end-to-end pipeline—from data preprocessing and model fine-tuning to evaluation and interactive deployment via Gradio—for an emotion-aware mental health chatbot. The modular structure ensures that each component (emotion detection, response generation, QA) is individually optimized, and a combined metadata system supports incremental training and easy deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e74c4",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a6fa4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Standard Library Imports\n",
    "# ================================\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import tempfile\n",
    "import datetime\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import pprint  # for pretty-printing outputs\n",
    "\n",
    "# ================================\n",
    "# Scientific & Data Libraries\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# ================================\n",
    "# Audio Processing\n",
    "# ================================\n",
    "from pydub import AudioSegment\n",
    "import speech_recognition as sr\n",
    "\n",
    "# ================================\n",
    "# NLP & Transformers (Hugging Face)\n",
    "# ================================\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import evaluate\n",
    "import gradio as gr\n",
    "\n",
    "# Tokenizers and Models\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "\n",
    "# Training Tools\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "# Logging\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# Other\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3c446199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up logging\n",
    "\n",
    "hf_logging.set_verbosity_info()                          \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Ensure save directories exist\n",
    "SAVE_ROOT = Path(\"./saved_models\")\n",
    "for sub in [\"emotion_classifier\", \"t5_response_generator\", \"t5_qa\", \"final_combined\"]:\n",
    "    (SAVE_ROOT / sub).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b54c6",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "390c89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle individual CSVs and provide their column mapping\n",
    "# Format: name: (enabled, path, question_col, answer_col)\n",
    "DATASETS = {\n",
    "    \"ds1\": (True,  \"./data/ds1_transformed_mental_health_chatbot_dataset.csv\",  \"question\", \"answer\"),\n",
    "    \"ds2\": (False, \"./data/ds2_transformed_mental_health_chatbot.csv\",         \"question\", \"answer\"),\n",
    "    \"ds3\": (False, \"./data/ds3_mental_health_faq_cleaned.csv\",                 \"Question\", \"Answer\"),\n",
    "    \"ds4\": (False, \"./data/ds4_mental_health_chatbot_dataset_merged_modes.csv\",\"prompt\",   \"response\"),\n",
    "    \"ds5\": (False, \"./data/ds5_Mental_Health_FAQ.csv\",                         \"Question\", \"Answer\"),\n",
    "    \"ds6\": (False, \"./data/ds6_mental_health_counseling.csv\",                  \"query\",    \"completion\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1b472611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust cleaner that auto-maps columns to 'question' / 'answer'\n",
    "def load_and_clean(path, q_col, a_col):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Normalize headers\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    q_col = q_col.lower().strip()\n",
    "    a_col = a_col.lower().strip()\n",
    "\n",
    "    # Common renames\n",
    "    rename_map = {\n",
    "        \"prompt\": \"question\",\n",
    "        \"response\": \"answer\",\n",
    "        \"questions\": \"question\",\n",
    "        \"answers\": \"answer\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # If provided cols exist, rename them to standard names\n",
    "    if q_col in df.columns:\n",
    "        df = df.rename(columns={q_col: \"question\"})\n",
    "    if a_col in df.columns:\n",
    "        df = df.rename(columns={a_col: \"answer\"})\n",
    "\n",
    "    # Try to map 'context' -> 'question' if needed\n",
    "    if \"question\" not in df.columns and \"context\" in df.columns:\n",
    "        df = df.rename(columns={\"context\": \"question\"})\n",
    "\n",
    "    # Verify if necessary columns exist\n",
    "    if not {\"question\", \"answer\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Could not find 'question'/'answer' in {path}. Available columns: {list(df.columns)}\")\n",
    "\n",
    "    # Retain only required columns, drop missing values and duplicates\n",
    "    df = df[[\"question\", \"answer\"]].dropna()\n",
    "    df[\"question\"] = df[\"question\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df[\"answer\"]   = df[\"answer\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Convert to Hugging Face Dataset\n",
    "    return Dataset.from_pandas(df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "565109d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'ds1' from ./data/ds1_transformed_mental_health_chatbot_dataset.csv ...\n",
      "Loaded 172 examples from 'ds1'.\n",
      "Skipping dataset 'ds2' as its toggle is off.\n",
      "Skipping dataset 'ds3' as its toggle is off.\n",
      "Skipping dataset 'ds4' as its toggle is off.\n",
      "Skipping dataset 'ds5' as its toggle is off.\n",
      "Skipping dataset 'ds6' as its toggle is off.\n"
     ]
    }
   ],
   "source": [
    "# Load enabled datasets and create a unified dataset\n",
    "datasets = []\n",
    "for key, (enabled, path, q_col, a_col) in DATASETS.items():\n",
    "    if enabled:\n",
    "        print(f\"Loading dataset '{key}' from {path} ...\")\n",
    "        ds = load_and_clean(path, q_col, a_col)\n",
    "        print(f\"Loaded {len(ds)} examples from '{key}'.\")\n",
    "        datasets.append(ds)\n",
    "    else:\n",
    "        print(f\"Skipping dataset '{key}' as its toggle is off.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fd09483b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combined dataset contains 172 examples.\n",
      "Training set: 154 examples, Testing set: 18 examples.\n"
     ]
    }
   ],
   "source": [
    "if datasets:\n",
    "    combined_dataset = concatenate_datasets(datasets)\n",
    "    print(f\"\\nCombined dataset contains {len(combined_dataset)} examples.\")\n",
    "else:\n",
    "    raise ValueError(\"No datasets enabled. Please enable at least one dataset in DATASETS.\")\n",
    "\n",
    "# Shuffle and split into training and testing datasets (e.g., 90% train, 10% test)\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "split_dataset = combined_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "test_dataset = split_dataset['test']\n",
    "print(f\"Training set: {len(train_dataset)} examples, Testing set: {len(test_dataset)} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7228172",
   "metadata": {},
   "source": [
    "### Multi-Label Emotion Annotation & Custom Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b8030b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Multi-label Emotion Annotation & Trainer Setup\n",
    "# ================================\n",
    "\n",
    "# Set a seed if not already defined\n",
    "SEED = 42\n",
    "\n",
    "# -------------------------------\n",
    "# Data Collator: casts labels to float32\n",
    "# -------------------------------\n",
    "def float_label_collator(features):\n",
    "    \"\"\"\n",
    "    Wrap the default HF collator but cast the `labels` tensor to float32\n",
    "    so BCEWithLogitsLoss gets the right dtype.\n",
    "    \"\"\"\n",
    "    batch = default_data_collator(features)\n",
    "    if \"labels\" in batch:\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float32)\n",
    "    # Uncomment next line to print label info during debugging\n",
    "    # print(\"collator labels dtype/shape:\", batch[\"labels\"].dtype, batch[\"labels\"].shape)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d3af2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Custom Trainer for Multi-Label Classification\n",
    "# -------------------------------\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer that computes loss using binary cross‑entropy with logits.\n",
    "    This ensures multi‑label targets (e.g., emotions) are correctly processed.\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs: bool = False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\").float()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # If labels' shape does not match logits, reshape to match\n",
    "        if labels.shape != logits.shape:\n",
    "            labels = labels.view_as(logits)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels, reduction=\"mean\")\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b0db1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Callback: Print training progress (optional)\n",
    "# -------------------------------\n",
    "class StepPrinter(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A Trainer callback that prints step-wise loss and evaluation metrics\n",
    "    while keeping the tqdm progress bar.\n",
    "    \"\"\"\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs or not state.is_local_process_zero:\n",
    "            return\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step:>6} • loss {logs['loss']:.4f}\")\n",
    "        if \"eval_loss\" in logs:\n",
    "            metric = logs.get(\"micro_f1\") or logs.get(\"bertscore_f1\") or logs.get(\"rougeL\")\n",
    "            metric_str = f\" • metric {metric:.4f}\" if metric is not None else \"\"\n",
    "            print(f\"Epoch {int(state.epoch)}/{int(args.num_train_epochs)}\"\n",
    "                  f\" • eval_loss {logs['eval_loss']:.4f}{metric_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dfbe30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Define Emotion Labels (28 total: 27 + neutral)\n",
    "# -------------------------------\n",
    "GO_EMOTION_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n",
    "    'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "    'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n",
    "    'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "    'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "num_labels = len(GO_EMOTION_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d31ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Annotate each example with multi-label emotion annotations.\n",
    "# For this demo, we simulate emotion annotations: if an example has no emotion,\n",
    "# we default to \"neutral\". Replace this with your real emotion annotations if available.\n",
    "# -------------------------------\n",
    "def annotate_emotions(example):\n",
    "    emos = example.get(\"emotions\", [])\n",
    "    \n",
    "    # If no explicit emotion annotations, default to [\"neutral\"]\n",
    "    if not emos:\n",
    "        emos = [\"neutral\"]\n",
    "    \n",
    "    # Save original emotions for visibility (optional)\n",
    "    example[\"emotions\"] = emos\n",
    "    \n",
    "    # Create a binary label vector for each of the 28 emotions\n",
    "    example[\"labels\"] = [1.0 if lbl in emos else 0.0 for lbl in GO_EMOTION_LABELS]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ce5a3890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 154 examples • Test dataset: 18 examples\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Helper to retrieve input text from an example (for debugging)\n",
    "# -------------------------------\n",
    "def get_input_text(example):\n",
    "    return example.get(\"text\") or example.get(\"question\") or \"[NO TEXT FOUND]\"\n",
    "\n",
    "# -------------------------------\n",
    "# (Re‑)load datasets for emotion annotation if not already loaded.\n",
    "# -------------------------------\n",
    "if 'train_dataset' not in globals() or 'test_dataset' not in globals():\n",
    "    print(\"Reloading datasets for emotion annotation ...\")\n",
    "    datasets_list = []\n",
    "    for name, (enabled, path, q_col, a_col) in DATASETS.items():\n",
    "        if not enabled:\n",
    "            continue\n",
    "        ds = load_and_clean(path=path, q_col=q_col, a_col=a_col)\n",
    "        datasets_list.append(ds)\n",
    "    if not datasets_list:\n",
    "        print(\"No datasets were enabled, using a fallback test dataset.\")\n",
    "        fallback_data = {\n",
    "            \"text\": [\n",
    "                \"How are you?\",\n",
    "                \"I feel really down today.\",\n",
    "                \"I'm so happy with my progress!\",\n",
    "                \"Why does nobody understand me?\",\n",
    "                \"I'm feeling anxious about school.\",\n",
    "                \"Life is good lately.\",\n",
    "                \"Sometimes I just want to cry.\",\n",
    "                \"Everything is falling apart.\",\n",
    "                \"I’m grateful for my therapist.\",\n",
    "                \"Can someone please just listen?\"\n",
    "            ]\n",
    "        }\n",
    "        ds = Dataset.from_dict(fallback_data)\n",
    "        datasets_list.append(ds)\n",
    "    full_ds = concatenate_datasets(datasets_list) if len(datasets_list) > 1 else datasets_list[0]\n",
    "    full_ds = full_ds.shuffle(seed=SEED)\n",
    "    split = full_ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "    train_dataset, test_dataset = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset):,} examples • Test dataset: {len(test_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1f137d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ed8a9b1db044d59a57239d738e37c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d2df85c4854336905487f53e7c0999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample annotation:\n",
      "How does someone acquire a mental illness? -> ['neutral']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Map emotion annotations onto training and testing datasets\n",
    "# -------------------------------\n",
    "emo_train = train_dataset.map(annotate_emotions)\n",
    "emo_test  = test_dataset.map(annotate_emotions)\n",
    "\n",
    "print(\"Sample annotation:\")\n",
    "print(get_input_text(emo_train[0]), \"->\", emo_train[0][\"emotions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "59fbdc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48822d1db8dd4822ad9280055c513339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8e10598fdb4b3ab4dcc1b704660d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples with problematic labels: 0\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Filter out any examples without at least one positive label (shouldn't happen after defaulting to neutral)\n",
    "# -------------------------------\n",
    "def has_nonzero_labels(example):\n",
    "    return sum(example[\"labels\"]) > 0\n",
    "\n",
    "emo_train = emo_train.filter(has_nonzero_labels)\n",
    "emo_test = emo_test.filter(has_nonzero_labels)\n",
    "\n",
    "# Check for any problematic labels\n",
    "bad_labels = [ex for ex in emo_train if \"labels\" not in ex or sum(ex[\"labels\"]) == 0]\n",
    "print(\"Number of examples with problematic labels:\", len(bad_labels))\n",
    "if bad_labels:\n",
    "    print(\"Example with problematic labels:\", bad_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4b16533b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before column rename: ['question', 'answer', 'emotions', 'labels']\n",
      "After column rename: ['text', 'answer', 'emotions', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Load the tokenizer for emotion classification.\n",
    "# We'll be using the 'SamLowe/roberta-base-go_emotions' tokenizer.\n",
    "# -------------------------------\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")\n",
    "\n",
    "# Print columns before renaming for clarity\n",
    "print(\"Before column rename:\", emo_train.column_names)\n",
    "# Rename 'question' to 'text' if needed for tokenization\n",
    "if \"question\" in emo_train.column_names:\n",
    "    emo_train = emo_train.rename_column(\"question\", \"text\")\n",
    "if \"question\" in emo_test.column_names:\n",
    "    emo_test = emo_test.rename_column(\"question\", \"text\")\n",
    "print(\"After column rename:\", emo_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "da1eddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Tokenization: tokenize using the 'text' column.\n",
    "# -------------------------------\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "74efbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Function to cast labels to float32 using numpy\n",
    "# -------------------------------\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e415bbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757a6db107834dfca7baafdcc1310b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045cc8fb820f46a9b25bcb72404d6ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbaf240f5ff4e6883d1d19fbd4d048f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36403410857b4c96a1eef2c86e35f3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training examples: 154 • Tokenized testing examples: 18\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and cast labels for both training and testing sets (batched processing)\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "# Set the dataset format for PyTorch\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(f\"Tokenized training examples: {len(emo_train_tok)} • Tokenized testing examples: {len(emo_test_tok)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf9090",
   "metadata": {},
   "source": [
    "## 4. Train Emotion Classifier (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "55885294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_models\\emotion_classifier\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_models\\\\emotion_classifier\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\emotion_classifier\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at saved_models\\emotion_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously fine-tuned emotion model...\n"
     ]
    }
   ],
   "source": [
    "# Before training, check if a fine-tuned model exists and load it.\n",
    "\n",
    "if os.path.isdir(SAVE_ROOT / \"emotion_classifier\"):\n",
    "    print(\"Loading previously fine-tuned emotion model...\")\n",
    "    emo_model = AutoModelForSequenceClassification.from_pretrained(SAVE_ROOT / \"emotion_classifier\").to(device)\n",
    "else:\n",
    "    print(\"Loading base emotion model...\")\n",
    "    emo_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"SamLowe/roberta-base-go_emotions\",\n",
    "        problem_type=\"multi_label_classification\",\n",
    "        num_labels=len(GO_EMOTION_LABELS)\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2aefce4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Model Training: Emotion Classification with RoBERTa\n",
    "# ===============================================\n",
    "\n",
    "# Define device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Training device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eeb9f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Tokenization for Model Training: Use fixed max_length\n",
    "# -------------------------------\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "35dfa0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float conversion: ensures labels become float32 arrays.\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e627e883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19cbc235b034331a70d50bff7d4ff64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8450887e46ad485787e2c70537eb8b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3359675420184984bbac42282731fea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4180e476fb54055a57b82e32bb1cbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization: 154 training examples; 18 testing examples.\n"
     ]
    }
   ],
   "source": [
    "# Re-tokenize the datasets using the new tokenization function\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(f\"After tokenization: {len(emo_train_tok)} training examples; {len(emo_test_tok)} testing examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a6aac608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"SamLowe/roberta-base-go_emotions\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at SamLowe/roberta-base-go_emotions.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: SamLowe/roberta-base-go_emotions\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Load the pre-trained emotion classification model\n",
    "# -------------------------------\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"SamLowe/roberta-base-go_emotions\",\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=num_labels,  # equals len(GO_EMOTION_LABELS)\n",
    ").to(device)\n",
    "print(\"Loaded model:\", emo_model.config._name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d6e5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Define Evaluation Metrics for Emotion Classification\n",
    "# -------------------------------\n",
    "def compute_emo_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    # Compute probabilities using sigmoid on logits\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    # Apply threshold (0.3) to decide positive labels\n",
    "    preds = (probs > 0.3).int().numpy()\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Defensive check: only consider rows with at least one positive label\n",
    "    mask = labels.sum(axis=1) > 0\n",
    "    if mask.sum() == 0:\n",
    "        print(\"Warning: all evaluation labels are empty\")\n",
    "        return {\"micro_f1\": 0.0}\n",
    "\n",
    "    try:\n",
    "        f1 = f1_score(labels[mask], preds[mask], average=\"micro\", zero_division=0)\n",
    "    except ValueError as e:\n",
    "        print(\"Metric error:\", e)\n",
    "        f1 = 0.0\n",
    "\n",
    "    return {\"micro_f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5690b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Set Up Training Arguments\n",
    "# -------------------------------\n",
    "# Define a root folder for saving model checkpoints if not defined already\n",
    "from pathlib import Path\n",
    "SAVE_ROOT = Path(\"./saved_models\")\n",
    "emo_args = TrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"emotion_classifier\"),\n",
    "    \n",
    "    # Logging & Reporting\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Training hyper‑parameters\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Evaluation and checkpointing settings\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a9069e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Instantiate the Custom Trainer for Multi-Label Classification\n",
    "# -------------------------------\n",
    "trainer_emo = MultiLabelTrainer(\n",
    "    model=emo_model,\n",
    "    args=emo_args,\n",
    "    train_dataset=emo_train_tok,\n",
    "    eval_dataset=emo_test_tok,\n",
    "    tokenizer=emo_tokenizer,\n",
    "    data_collator=float_label_collator,\n",
    "    compute_metrics=compute_emo_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1ea3620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, emotions, answer. If text, emotions, answer are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 154\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30\n",
      "  Number of trainable parameters = 124,667,164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 02:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, emotions, answer. If text, emotions, answer are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     10 • loss 0.0296\n",
      "Epoch 1/3 • eval_loss 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\emotion_classifier\\checkpoint-10\n",
      "Configuration saved in saved_models\\emotion_classifier\\checkpoint-10\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\checkpoint-10\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\checkpoint-10\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\checkpoint-10\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, emotions, answer. If text, emotions, answer are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     20 • loss 0.0036\n",
      "Epoch 2/3 • eval_loss 0.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\emotion_classifier\\checkpoint-20\n",
      "Configuration saved in saved_models\\emotion_classifier\\checkpoint-20\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\checkpoint-20\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\checkpoint-20\\special_tokens_map.json\n",
      "Saving model checkpoint to saved_models\\emotion_classifier\\checkpoint-30\n",
      "Configuration saved in saved_models\\emotion_classifier\\checkpoint-30\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     30 • loss 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in saved_models\\emotion_classifier\\checkpoint-30\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\checkpoint-30\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text, emotions, answer. If text, emotions, answer are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 • eval_loss 0.0027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\emotion_classifier\\checkpoint-30\n",
      "Configuration saved in saved_models\\emotion_classifier\\checkpoint-30\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\checkpoint-30\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\checkpoint-30\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\checkpoint-30\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from saved_models\\emotion_classifier\\checkpoint-10 (score: 1.0).\n",
      "Configuration saved in saved_models\\emotion_classifier\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\special_tokens_map.json\n",
      "Saving model checkpoint to saved_models\\emotion_classifier\n",
      "Configuration saved in saved_models\\emotion_classifier\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion classifier model and tokenizer saved to saved_models\\emotion_classifier\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Start Training (uncomment the next line to train)\n",
    "# -------------------------------\n",
    "trainer_emo.train()\n",
    "\n",
    "# -------------------------------\n",
    "# Save the Best Model and Tokenizer\n",
    "# -------------------------------\n",
    "# Create the directory structure if it doesn't exist\n",
    "SAVE_ROOT.mkdir(exist_ok=True, parents=True)\n",
    "(SAVE_ROOT / \"emotion_classifier\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "emo_model.save_pretrained(SAVE_ROOT / \"emotion_classifier\")\n",
    "emo_tokenizer.save_pretrained(SAVE_ROOT / \"emotion_classifier\")\n",
    "\n",
    "# Also save the trainer's final model state\n",
    "trainer_emo.save_model()\n",
    "print(\"Emotion classifier model and tokenizer saved to\", SAVE_ROOT / \"emotion_classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840db175",
   "metadata": {},
   "source": [
    "## 5. Train T5 for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "10739d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_models\\t5_response_generator\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\t5_response_generator\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously fine-tuned T5 response generator model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models\\t5_response_generator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models\\t5_response_generator\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Before training, check if a fine-tuned model exists and load it.\n",
    "\n",
    "if os.path.isdir(SAVE_ROOT / \"t5_response_generator\"):\n",
    "    print(\"Loading previously fine-tuned T5 response generator model...\")\n",
    "    resp_model = T5ForConditionalGeneration.from_pretrained(SAVE_ROOT / \"t5_response_generator\").to(device)\n",
    "else:\n",
    "    print(\"Loading base T5 response generator model...\")\n",
    "    resp_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3647aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Train T5 for Response Generation\n",
    "# ===============================================\n",
    "# Build input/target pairs: user text -> helpful response\n",
    "# For now we use 'question' as input and 'answer' as target.\n",
    "def build_t5_pairs(example):\n",
    "    # Retrieve the question from either \"question\" or \"text\" columns,\n",
    "    # and the corresponding answer from either \"answer\" or \"response\" columns.\n",
    "    question = example.get(\"question\") or example.get(\"text\") or \"\"\n",
    "    answer = example.get(\"answer\") or example.get(\"response\") or \"\"\n",
    "    example[\"input_text\"] = \"respond: \" + question\n",
    "    example[\"target_text\"] = answer\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "25c2db80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9bff4f5e174a81958c61b14d4a529d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c99626aa738436fb1c963dee721f2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "# Map the original training and testing datasets to build T5 pairs.\n",
    "# We're using the original QA datasets (train_dataset and test_dataset) from our data-loading cell.\n",
    "resp_train = train_dataset.map(build_t5_pairs)\n",
    "resp_test  = test_dataset.map(build_t5_pairs)\n",
    "\n",
    "# Load T5 tokenizer and model.\n",
    "t5_resp_model_name = \"t5-small\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_resp_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "885e232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input and target texts for T5.\n",
    "def t5_tokenize(batch):\n",
    "    # Tokenize the input text.\n",
    "    model_inputs = tokenizer_t5(batch[\"input_text\"], max_length=128, truncation=True)\n",
    "    # Prepare target text tokenization; this context manager sets the tokenizer into target mode.\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(batch[\"target_text\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d398f38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e6dfb5687e4443ba81d324bf4e489e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80173cf6817642e383700c3dc7e90c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training examples: 154; Tokenized testing examples: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded T5 model: t5-small\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the training and testing T5 pairs.\n",
    "resp_train_tok = resp_train.map(t5_tokenize, batched=True, remove_columns=resp_train.column_names)\n",
    "resp_test_tok  = resp_test.map(t5_tokenize, batched=True, remove_columns=resp_test.column_names)\n",
    "\n",
    "# Set datasets' format to output PyTorch tensors.\n",
    "resp_train_tok.set_format(\"torch\")\n",
    "resp_test_tok.set_format(\"torch\")\n",
    "\n",
    "print(f\"Tokenized training examples: {len(resp_train_tok)}; Tokenized testing examples: {len(resp_test_tok)}\")\n",
    "\n",
    "# Instantiate the T5 model for conditional generation and send it to the appropriate device.\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(t5_resp_model_name).to(device)\n",
    "print(\"Loaded T5 model:\", t5_resp_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "853f2c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Define evaluation metrics: ROUGE and BERTScore\n",
    "# -----------------------------------------------\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_resp_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    # Replace -100 in the labels (which are ignored) with the pad_token_id so they can be decoded.\n",
    "    labels = np.where(labels != -100, labels, tokenizer_t5.pad_token_id)\n",
    "    \n",
    "    # Decode predictions and labels.\n",
    "    decoded_preds = tokenizer_t5.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute ROUGE.\n",
    "    r = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Compute BERTScore.\n",
    "    b = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    \n",
    "    return {\n",
    "        \"rougeL\": r[\"rougeL\"],\n",
    "        \"bertscore_f1\": np.mean(b[\"f1\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1bae41a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# Set up the Seq2Seq training arguments.\n",
    "# -----------------------------------------------\n",
    "resp_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"t5_response_generator\"),\n",
    "    \n",
    "    # Logging configuration.\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    \n",
    "    # Core hyper‑parameters.\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    # Evaluation and checkpointing.\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f594602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------\n",
    "# Create a data collator for T5 using Hugging Face's helper.\n",
    "# -----------------------------------------------\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer_t5, model=resp_model)\n",
    "\n",
    "# Instantiate the Seq2Seq trainer with our model, tokenized data, metrics, and callbacks.\n",
    "trainer_resp = Seq2SeqTrainer(\n",
    "    model=resp_model,\n",
    "    args=resp_args,\n",
    "    train_dataset=resp_train_tok,\n",
    "    eval_dataset=resp_test_tok,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_resp_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d17a344e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 154\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 60\n",
      "  Number of trainable parameters = 60,506,624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bertscore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.307000</td>\n",
       "      <td>2.634834</td>\n",
       "      <td>0.036774</td>\n",
       "      <td>0.190253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.033200</td>\n",
       "      <td>2.564415</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.245034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.003400</td>\n",
       "      <td>2.540509</td>\n",
       "      <td>0.039732</td>\n",
       "      <td>0.190298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     10 • loss 3.9551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     20 • loss 3.3070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\tokenizer_config.json\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\model.safetensors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 • eval_loss 2.6348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-20\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-20\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-20\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-20\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-20\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-20\\spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     30 • loss 3.0956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     40 • loss 3.0332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 • eval_loss 2.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-40\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-40\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-40\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-40\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-40\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-40\\spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     50 • loss 2.9013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-60\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-60\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-60\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     60 • loss 3.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-60\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-60\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-60\\spiece.model\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 • eval_loss 2.5405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-60\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-60\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-60\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-60\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-60\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-60\\spiece.model\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=3.215902900695801, metrics={'train_runtime': 21.3603, 'train_samples_per_second': 21.629, 'train_steps_per_second': 2.809, 'total_flos': 3241859088384.0, 'train_loss': 3.215902900695801, 'epoch': 3.0})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# Begin training.\n",
    "# -----------------------------------------------\n",
    "# Uncomment the line below to start training. Note that training may take a while.\n",
    "trainer_resp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7f853d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in saved_models\\t5_response_generator\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\spiece.model\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\n",
      "Configuration saved in saved_models\\t5_response_generator\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 response generator model and tokenizer saved to saved_models\\t5_response_generator\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------\n",
    "# Robustly save the trained T5 response generation model and tokenizer.\n",
    "# -----------------------------------------------\n",
    "(SAVE_ROOT / \"t5_response_generator\").mkdir(exist_ok=True, parents=True)\n",
    "resp_model.save_pretrained(SAVE_ROOT / \"t5_response_generator\")\n",
    "tokenizer_t5.save_pretrained(SAVE_ROOT / \"t5_response_generator\")\n",
    "trainer_resp.save_model()  # Save trainer's model state.\n",
    "\n",
    "print(\"T5 response generator model and tokenizer saved to\", SAVE_ROOT / \"t5_response_generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6140c",
   "metadata": {},
   "source": [
    "## Train T5 for Question‑Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1fd0f487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_models\\t5_qa\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\t5_qa\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously fine-tuned T5 QA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models\\t5_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models\\t5_qa\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Before training, check if a fine-tuned model exists and load it.\n",
    "\n",
    "if os.path.isdir(SAVE_ROOT / \"t5_qa\"):\n",
    "    print(\"Loading previously fine-tuned T5 QA model...\")\n",
    "    qa_model = T5ForConditionalGeneration.from_pretrained(SAVE_ROOT / \"t5_qa\").to(device)\n",
    "else:\n",
    "    print(\"Loading base T5 QA model...\")\n",
    "    qa_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1b46895d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training T5 for the Question‑Answer task.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Train T5 for Question‑Answer\n",
    "# ===============================================\n",
    "print(\"Training T5 for the Question‑Answer task.\")\n",
    "\n",
    "# Load (or initialize) the QA model and tokenizer.\n",
    "# Here we fine-tune the base T5 model (t5-small) for QA.\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(t5_resp_model_name).to(device)\n",
    "# Note: We're reusing tokenizer_t5 from the response generation section.\n",
    "\n",
    "# Create a DataCollator for Seq2Seq tasks.\n",
    "qa_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer_t5,\n",
    "    model=qa_model,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8fbec6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build QA pairs: \"question: <text>\" -> answer.\n",
    "def build_qa_pairs(example):\n",
    "    # Retrieve the question from 'question' (or 'text') and answer from 'answer' (or 'response').\n",
    "    question = example.get(\"question\") or example.get(\"text\") or \"\"\n",
    "    answer = example.get(\"answer\") or example.get(\"response\") or \"\"\n",
    "    example[\"input_text\"] = \"question: \" + question\n",
    "    example[\"target_text\"] = answer\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2dcd45a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5310e719b6406da29739f22b1abbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b427604ec774c4b8a2d6034a21fdf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the QA pair-building function onto the training and testing datasets.\n",
    "qa_train = train_dataset.map(build_qa_pairs)\n",
    "qa_test  = test_dataset.map(build_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "714c4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the QA pairs.\n",
    "def qa_tokenize(batch):\n",
    "    # Tokenize the input text.\n",
    "    model_inputs = tokenizer_t5(batch[\"input_text\"], max_length=128, truncation=True)\n",
    "    # Tokenize the target (answer) text.\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(batch[\"target_text\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f7a5b8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04676170f96c43c4942180e068ae1a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46fb04e6b9a441f86f562d3f07044b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa_train_tok = qa_train.map(qa_tokenize, batched=True, remove_columns=qa_train.column_names)\n",
    "qa_test_tok  = qa_test.map(qa_tokenize, batched=True, remove_columns=qa_test.column_names)\n",
    "\n",
    "# Set the format for PyTorch tensors.\n",
    "qa_train_tok.set_format(\"torch\")\n",
    "qa_test_tok.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d4d17444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized QA training examples: 154; Tokenized QA testing examples: 18\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokenized QA training examples: {len(qa_train_tok)}; Tokenized QA testing examples: {len(qa_test_tok)}\")\n",
    "\n",
    "# Define Seq2Seq training arguments for the QA task.\n",
    "qa_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"t5_qa\"),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5e9f0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Seq2Seq trainer for the QA task.\n",
    "trainer_qa = Seq2SeqTrainer(\n",
    "    model=qa_model,\n",
    "    args=qa_args,\n",
    "    train_dataset=qa_train_tok,\n",
    "    eval_dataset=qa_test_tok,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=qa_collator,\n",
    "    compute_metrics=compute_resp_metrics,  # Reusing the same metrics function as for response generation.\n",
    "    callbacks=[StepPrinter],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bb1603e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 154\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 60\n",
      "  Number of trainable parameters = 60,506,624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bertscore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.259000</td>\n",
       "      <td>2.643346</td>\n",
       "      <td>0.141225</td>\n",
       "      <td>0.588208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.017700</td>\n",
       "      <td>2.552439</td>\n",
       "      <td>0.150305</td>\n",
       "      <td>0.634685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.020500</td>\n",
       "      <td>2.539231</td>\n",
       "      <td>0.159643</td>\n",
       "      <td>0.683060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     10 • loss 3.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     20 • loss 3.2590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 • eval_loss 2.6433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-20\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-20\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-20\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-20\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-20\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-20\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-20\\spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     30 • loss 3.0657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     40 • loss 3.0177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 • eval_loss 2.5524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-40\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-40\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-40\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-40\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-40\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-40\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-40\\spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     50 • loss 2.8962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-60\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-60\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-60\\generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     60 • loss 3.0205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in saved_models\\t5_qa\\checkpoint-60\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-60\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-60\\spiece.model\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 • eval_loss 2.5392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-60\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-60\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-60\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-60\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-60\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-60\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-60\\spiece.model\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in saved_models\\t5_qa\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\spiece.model\n",
      "Saving model checkpoint to saved_models\\t5_qa\n",
      "Configuration saved in saved_models\\t5_qa\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\spiece.model\n",
      "tokenizer config file saved in saved_models\\t5_qa\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 QA model and tokenizer saved to saved_models\\t5_qa\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following line to start training (training may take some time):\n",
    "trainer_qa.train()\n",
    "\n",
    "# -------------------------------\n",
    "# Save the trained QA model and tokenizer\n",
    "# -------------------------------\n",
    "(SAVE_ROOT / \"t5_qa\").mkdir(exist_ok=True, parents=True)\n",
    "qa_model.save_pretrained(SAVE_ROOT / \"t5_qa\")\n",
    "tokenizer_t5.save_pretrained(SAVE_ROOT / \"t5_qa\")\n",
    "trainer_qa.save_model()\n",
    "tokenizer_t5.save_pretrained(qa_args.output_dir)\n",
    "\n",
    "print(\"T5 QA model and tokenizer saved to\", SAVE_ROOT / \"t5_qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a35b0",
   "metadata": {},
   "source": [
    "## Model Deployment Setup with Combined Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f55db508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined model metadata to: saved_models\\combined_model_metadata.pt\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Save Combined Model Metadata\n",
    "# --------------------------------------------\n",
    "metadata = {\n",
    "    \"emotion_classifier\": str(SAVE_ROOT / \"emotion_classifier\"),\n",
    "    \"t5_response_generator\": str(SAVE_ROOT / \"t5_response_generator\"),\n",
    "    \"t5_qa\": str(SAVE_ROOT / \"t5_qa\")\n",
    "}\n",
    "\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "torch.save(metadata, metadata_path)\n",
    "print(\"Saved combined model metadata to:\", metadata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "68f1a8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_models\\emotion_classifier\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_models\\\\emotion_classifier\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\emotion_classifier\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at saved_models\\emotion_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model metadata: {'emotion_classifier': 'saved_models\\\\emotion_classifier', 't5_response_generator': 'saved_models\\\\t5_response_generator', 't5_qa': 'saved_models\\\\t5_qa'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models\\t5_response_generator\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\t5_response_generator\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models\\t5_response_generator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models\\t5_response_generator\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models\\t5_qa\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\t5_qa\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models\\t5_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models\\t5_qa\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models loaded from metadata.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Load Models Using Combined Metadata\n",
    "# --------------------------------------------\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(metadata_path)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")\n",
    "\n",
    "# Load Emotion Classification Model & Tokenizer from metadata\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(model_paths[\"emotion_classifier\"]).to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"emotion_classifier\"])\n",
    "\n",
    "# Load T5 Response Generation Model & Tokenizer from metadata\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(model_paths[\"t5_response_generator\"]).to(device)\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"t5_response_generator\"])\n",
    "\n",
    "# Load T5 QA Model & Tokenizer from metadata\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(model_paths[\"t5_qa\"]).to(device)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"t5_qa\"])\n",
    "\n",
    "print(\"All models loaded from metadata.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8533473",
   "metadata": {},
   "source": [
    "## Unified Pipeline & Gradio for the Mental Health Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1befa803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model metadata: {'emotion_classifier': 'saved_models\\\\emotion_classifier', 't5_response_generator': 'saved_models\\\\t5_response_generator', 't5_qa': 'saved_models\\\\t5_qa'}\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Unified Pipeline & Gradio for the Mental Health Chatbot\n",
    "# ===============================================\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
    "\n",
    "# ---- Load Fine‑Tuned Models Using Combined Metadata ----\n",
    "# Define the path to the combined metadata file.\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(metadata_path)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7e2e041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_models\\emotion_classifier\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_models\\\\emotion_classifier\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\emotion_classifier\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at saved_models\\emotion_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models\\t5_response_generator\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\t5_response_generator\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models\\t5_response_generator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models\\t5_response_generator\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    }
   ],
   "source": [
    "# Load Emotion Classification Model & Tokenizer from metadata.\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(model_paths[\"emotion_classifier\"]).to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"emotion_classifier\"])\n",
    "emo_model.eval()  # Set emotion classifier to evaluation mode\n",
    "\n",
    "# Load T5 Response Generation Model & Tokenizer from metadata.\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(model_paths[\"t5_response_generator\"]).to(device)\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"t5_response_generator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "10565767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_models\\t5_qa\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\t5_qa\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models\\t5_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models\\t5_qa\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    }
   ],
   "source": [
    "# Load T5 QA Model & Tokenizer from metadata.\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(model_paths[\"t5_qa\"]).to(device)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"t5_qa\"])\n",
    "\n",
    "# ---- Define Emotion Labels ----\n",
    "# These labels match those used in our emotion annotation step.\n",
    "DEFAULT_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'embarrassment', 'excitement', 'fear', 'gratitude',\n",
    "    'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n",
    "    'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "NUM_EMO_LABELS = emo_model.config.num_labels\n",
    "EMOTION_LABELS = DEFAULT_LABELS[:NUM_EMO_LABELS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5cf0805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, define a subset for emotion-based routing.\n",
    "emotion_router_labels = {'confusion', 'caring', 'nervousness', 'grief', 'sadness', 'fear', 'remorse', 'love', 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f4828899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper Functions ----\n",
    "\n",
    "def detect_emotions(text):\n",
    "    \"\"\"\n",
    "    Detects emotions in the provided text using the fine‑tuned emotion classifier.\n",
    "    Returns a list of emotion labels whose corresponding probabilities exceed 0.3.\n",
    "    \"\"\"\n",
    "    inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(emo_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = emo_model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    detected = [EMOTION_LABELS[i] for i, p in enumerate(probs) if p > 0.3]\n",
    "    return detected if detected else [\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "34836f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_prompt(user_input, language=\"English\", history=None):\n",
    "    \"\"\"\n",
    "    Optionally formats a prompt incorporating conversational history.\n",
    "    \"\"\"\n",
    "    if history:\n",
    "        combined = \"\\n\".join(history + [user_input])\n",
    "        return f\"You are a supportive mental health assistant. Respond in {language}. The conversation so far:\\n{combined}\"\n",
    "    return f\"You are a supportive mental health assistant. Respond in {language}. The user says: {user_input}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6d15c7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Unified Chatbot Pipeline Class ----\n",
    "\n",
    "class MentalHealthChatbotPipeline:\n",
    "    def __init__(self, labels, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.labels = labels\n",
    "        self.chat_history = []  # Stores tuples like (speaker, text)\n",
    "        \n",
    "        # Ensure models are in evaluation mode.\n",
    "        self.emo_model = emo_model.eval()\n",
    "        self.qa_model = qa_model.eval()\n",
    "        self.resp_model = resp_model.eval()\n",
    "\n",
    "    def __call__(self, text, max_length=128):\n",
    "        \"\"\"\n",
    "        Processes user text, detects emotions, routes the input to the appropriate model,\n",
    "        and returns a dictionary with detected emotions, the reply, and the updated conversation history.\n",
    "        \"\"\"\n",
    "        self.chat_history.append((\"User\", text))\n",
    "        \n",
    "        # ---- Emotion Detection ----\n",
    "        emotions = detect_emotions(text)\n",
    "        \n",
    "        # ---- Model Selection Logic ----\n",
    "        # Simple decision: if the input contains a question mark, use the QA model; otherwise, use the response generation model.\n",
    "        if \"?\" in text:\n",
    "            model, tokenizer = self.qa_model, qa_tokenizer\n",
    "        else:\n",
    "            model, tokenizer = self.resp_model, resp_tokenizer\n",
    "\n",
    "        # ---- Generate Response ----\n",
    "        # (Optionally, you could use format_input_prompt to incorporate history.)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_length=max_length)\n",
    "        reply = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        self.chat_history.append((\"Bot\", reply))\n",
    "        \n",
    "        return {\"Detected Emotions\": emotions, \"Response\": reply, \"History\": self.chat_history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0233cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the chatbot pipeline.\n",
    "chatbot = MentalHealthChatbotPipeline(labels=EMOTION_LABELS, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95721f",
   "metadata": {},
   "source": [
    "## Gradio Interface for the Mental Health Chatbot (for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c0299535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model metadata: {'emotion_classifier': 'saved_models\\\\emotion_classifier', 't5_response_generator': 'saved_models\\\\t5_response_generator', 't5_qa': 'saved_models\\\\t5_qa'}\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# Full-Functionality Gradio Interface for the Mental Health Chatbot\n",
    "# ===============================================\n",
    "\n",
    "# ---- Load Fine-Tuned Models Using Combined Metadata ----\n",
    "metadata_path = SAVE_ROOT / \"combined_model_metadata.pt\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    model_paths = torch.load(metadata_path)\n",
    "    print(\"Loaded model metadata:\", model_paths)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Metadata file not found at {metadata_path}. Please ensure it exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "55e0e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models\\t5_response_generator\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\t5_response_generator\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models\\t5_response_generator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models\\t5_response_generator\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models\\t5_qa\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\t5_qa\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models\\t5_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models\\t5_qa\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models\\emotion_classifier\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_models\\\\emotion_classifier\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\emotion_classifier\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at saved_models\\emotion_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Response Generation Model & Tokenizer\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"t5_response_generator\"])\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(model_paths[\"t5_response_generator\"]).to(device)\n",
    "\n",
    "# Load QA Model & Tokenizer\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"t5_qa\"])\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(model_paths[\"t5_qa\"]).to(device)\n",
    "\n",
    "# Load Emotion Classification Model & Tokenizer\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(model_paths[\"emotion_classifier\"])\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(model_paths[\"emotion_classifier\"]).to(device)\n",
    "emo_model.eval()  # Set emotion classifier to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "17593d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Define Emotion Labels ----\n",
    "# Define the complete list of 28 emotion labels (as in the GoEmotions baseline)\n",
    "DEFAULT_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',\n",
    "    'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief',\n",
    "    'remorse', 'sadness', 'surprise', 'neutral'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7e1a5406",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EMO_LABELS = emo_model.config.num_labels\n",
    "EMOTION_LABELS = DEFAULT_LABELS[:NUM_EMO_LABELS]\n",
    "\n",
    "# Define a subset of emotions for routing decisions (if needed)\n",
    "emotion_router_labels = set(EMOTION_LABELS) & {'confusion', 'caring', 'nervousness', 'grief', 'sadness', 'fear', 'remorse', 'love', 'anger'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a789f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Helper Functions ----\n",
    "\n",
    "def format_input_prompt(user_input, language=\"English\", history=None):\n",
    "    \"\"\"Formats a prompt that incorporates conversation history.\"\"\"\n",
    "    if history:\n",
    "        combined = \"\\n\".join(history + [user_input])\n",
    "        return f\"You are a supportive mental health assistant. Respond in {language}. The conversation so far:\\n{combined}\"\n",
    "    return f\"You are a supportive mental health assistant. Respond in {language}. The user says: {user_input}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "35881b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_emotions(text):\n",
    "    \"\"\"\n",
    "    Detects emotions in the provided text using the fine‑tuned emotion classifier.\n",
    "    Returns a list of emotion labels whose corresponding probabilities exceed 0.3.\n",
    "    \"\"\"\n",
    "    inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(emo_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = emo_model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    \n",
    "    # Safety check: trim probabilities if there are more than expected.\n",
    "    if len(probs) > len(EMOTION_LABELS):\n",
    "        print(f\"Warning: Received {len(probs)} probabilities; expected {len(EMOTION_LABELS)}. Trimming extra values.\")\n",
    "        probs = probs[:len(EMOTION_LABELS)]\n",
    "    \n",
    "    detected = [EMOTION_LABELS[i] for i, p in enumerate(probs) if p > 0.3]\n",
    "    return detected if detected else [\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "759f7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"\n",
    "    Converts an input audio file to text using speech recognition.\n",
    "    Returns the transcribed text or an error message.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp:\n",
    "        audio.export(tmp.name, format=\"wav\")\n",
    "        with sr.AudioFile(tmp.name) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            try:\n",
    "                return recognizer.recognize_google(audio_data)\n",
    "            except sr.UnknownValueError:\n",
    "                return \"[Unrecognized speech]\"\n",
    "            except sr.RequestError:\n",
    "                return \"[Speech recognition failed]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a6981e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chatbot_response(user_text, audio_input, mode, language, use_history, history, route_by_emotion, persist):\n",
    "    \"\"\"\n",
    "    Main function to generate a chatbot response.\n",
    "    \n",
    "    Parameters:\n",
    "      - user_text: Text input (if mode == \"text\")\n",
    "      - audio_input: Audio file path (if mode == \"voice\")\n",
    "      - mode: \"text\" or \"voice\" input mode\n",
    "      - language: Desired language for the response\n",
    "      - use_history: If True, include conversation history in the prompt\n",
    "      - history: Current conversation history (as a list)\n",
    "      - route_by_emotion: If True, route input based on detected emotion\n",
    "      - persist: If True, save conversation to a log file\n",
    "    \n",
    "    Returns:\n",
    "      A tuple (response, detected emotions as a string, updated history)\n",
    "    \"\"\"\n",
    "    history = history or []\n",
    "    user_input = user_text if mode == \"text\" else transcribe_audio(audio_input)\n",
    "    emotions = detect_emotions(user_input)\n",
    "    \n",
    "    # Decide model based on detected emotions and routing flag.\n",
    "    use_resp_model = any(e in emotion_router_labels for e in emotions) if route_by_emotion else False\n",
    "\n",
    "    if use_resp_model:\n",
    "        prompt = format_input_prompt(user_input, language, history if use_history else None)\n",
    "        inputs = resp_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(resp_model.device)\n",
    "        model, tokenizer = resp_model, resp_tokenizer\n",
    "    else:\n",
    "        prompt = \"question: \" + user_input\n",
    "        inputs = qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(qa_model.device)\n",
    "        model, tokenizer = qa_model, qa_tokenizer\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=64,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    full_history = history + [f\"User: {user_input}\", f\"Bot: {response}\"]\n",
    "\n",
    "    if persist:\n",
    "        with open(\"chatlog.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"\\n[{datetime.datetime.now()}]\\n{full_history[-2]}\\n{full_history[-1]}\\nDetected emotions: {emotions}\\n\")\n",
    "\n",
    "    return response, \", \".join(emotions), full_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "17ecdfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7862/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7862/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# ---- Build the Gradio Interface ----\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_chatbot_response,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Type your message here (if using text mode)\"),\n",
    "        gr.Audio(type=\"filepath\", label=\"Or speak here (if using voice mode)\"),\n",
    "        gr.Radio([\"text\", \"voice\"], value=\"text\", label=\"Input Mode\"),\n",
    "        gr.Dropdown(choices=[\"English\", \"German\", \"Spanish\", \"French\"], value=\"English\", label=\"Response Language\"),\n",
    "        gr.Checkbox(label=\"Include chat history in response\", value=True),\n",
    "        gr.State(value=[]),\n",
    "        gr.Checkbox(label=\"Route by detected emotion\", value=True),\n",
    "        gr.Checkbox(label=\"Save conversation to chatlog.txt\", value=True)\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Therapist Response\"),\n",
    "        gr.Textbox(label=\"Detected Emotions\"),\n",
    "        gr.State()\n",
    "    ],\n",
    "    title=\"Voice + Text Enabled Emotion-Aware Mental Health Chatbot\",\n",
    "    description=\"You can type or speak your message. Emotion-aware routing decides between Q&A and therapist-style support.\"\n",
    ")\n",
    "\n",
    "# Launch the interface.\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf78da",
   "metadata": {},
   "source": [
    "## Metrics and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "cbb02ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Model Evaluation Cell\n",
    "# ===============================================\n",
    "\n",
    "### Evaluation for Emotion Classification ###\n",
    "\n",
    "def evaluate_emotion_classifier(model, tokenizer, dataset, batch_size=16):\n",
    "    \"\"\"\n",
    "    Evaluate the emotion classifier over the provided dataset.\n",
    "    Computes micro-averaged F1, Precision, Recall, and Subset Accuracy.\n",
    "    Assumes that dataset is formatted with columns \"input_ids\", \"attention_mask\", \"labels\"\n",
    "    and that labels is a multi-label binary vector.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Create a DataLoader for batch processing (if dataset is not huge, you can loop through it directly)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # Move inputs and labels to device\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        labels = batch[\"labels\"].numpy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        # Apply sigmoid for multi-label classification and threshold at 0.3\n",
    "        preds = (torch.sigmoid(logits) > 0.3).cpu().numpy().astype(int)\n",
    "        \n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    # Compute micro-averaged metrics\n",
    "    micro_f1 = f1_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    micro_precision = precision_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    micro_recall = recall_score(all_labels, all_preds, average=\"micro\", zero_division=0)\n",
    "    subset_acc = accuracy_score(all_labels, all_preds)  # subset accuracy is strict\n",
    "    \n",
    "    return {\n",
    "        \"Emotion Classifier Micro-F1\": micro_f1,\n",
    "        \"Emotion Classifier Micro-Precision\": micro_precision,\n",
    "        \"Emotion Classifier Micro-Recall\": micro_recall,\n",
    "        \"Emotion Classifier Subset Accuracy\": subset_acc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "05735854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Emotion Classifier...\n",
      "Emotion Classifier Micro-F1: 1.0000\n",
      "Emotion Classifier Micro-Precision: 1.0000\n",
      "Emotion Classifier Micro-Recall: 1.0000\n",
      "Emotion Classifier Subset Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Emotion Classifier...\")\n",
    "emo_metrics = evaluate_emotion_classifier(emo_model, emo_tokenizer, emo_test_tok)\n",
    "for metric, value in emo_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7abddcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation for Generation Models (Response Generation and QA) ###\n",
    "\n",
    "# We already defined a compute_resp_metrics function in the training cells.\n",
    "# Here, we define a helper to compute additional perplexity based on the evaluation loss.\n",
    "\n",
    "def evaluate_generation_model(trainer, test_dataset):\n",
    "    \"\"\"\n",
    "    Uses the Seq2SeqTrainer to compute evaluation metrics over the given test dataset.\n",
    "    Adds perplexity (exp(eval_loss)) to the standard metrics.\n",
    "    \"\"\"\n",
    "    # Predict returns a dictionary with metrics: eval_loss, and any metrics computed in compute_metrics.\n",
    "    result = trainer.predict(test_dataset)\n",
    "    eval_loss = result.metrics.get(\"eval_loss\")\n",
    "    \n",
    "    # Compute perplexity if loss is available. (If eval_loss is zero or not available, perplexity is undefined.)\n",
    "    if eval_loss is not None and eval_loss > 0:\n",
    "        perplexity = math.exp(eval_loss)\n",
    "    else:\n",
    "        perplexity = float(\"inf\")\n",
    "    \n",
    "    # Add perplexity to the metrics dictionary.\n",
    "    result.metrics[\"perplexity\"] = perplexity\n",
    "    return result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "98aa20ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating T5 Response Generation Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Response Generation test_loss: 2.5405\n",
      "T5 Response Generation test_rougeL: 0.0397\n",
      "T5 Response Generation test_bertscore_f1: 0.1903\n",
      "T5 Response Generation test_runtime: 1.8881\n",
      "T5 Response Generation test_samples_per_second: 9.5330\n",
      "T5 Response Generation test_steps_per_second: 1.5890\n",
      "T5 Response Generation perplexity: inf\n",
      "\n",
      "Evaluating T5 QA Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 QA test_loss: 2.5392\n",
      "T5 QA test_rougeL: 0.1596\n",
      "T5 QA test_bertscore_f1: 0.6831\n",
      "T5 QA test_runtime: 2.5008\n",
      "T5 QA test_samples_per_second: 7.1980\n",
      "T5 QA test_steps_per_second: 1.2000\n",
      "T5 QA perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating T5 Response Generation Model...\")\n",
    "resp_metrics = evaluate_generation_model(trainer_resp, resp_test_tok)\n",
    "for metric, value in resp_metrics.items():\n",
    "    print(f\"T5 Response Generation {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating T5 QA Model...\")\n",
    "qa_metrics = evaluate_generation_model(trainer_qa, qa_test_tok)\n",
    "for metric, value in qa_metrics.items():\n",
    "    print(f\"T5 QA {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d853ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionally, you can evaluate other metrics such as ROUGE and BERTScore separately using the `evaluate` library,\n",
    "# if desired. For example:\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_generation_metrics(trainer, test_dataset, tokenizer):\n",
    "    result = trainer.predict(test_dataset)\n",
    "    predictions, labels = result.predictions, result.label_ids\n",
    "    # Replace -100 in labels by the tokenizer pad id.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    r = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    b = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    \n",
    "    # Here, we only extract the ROUGE-L and average BERTScore F1.\n",
    "    metrics = {\n",
    "        \"rougeL\": r[\"rougeL\"],\n",
    "        \"bertscore_f1\": np.mean(b[\"f1\"])\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3d42bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional Generation Metrics for T5 Response Generation:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "INFO:absl:Using default tokenizer.\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\tokenizer_config.json\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\model.safetensors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 18\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Response Generation rougeL: 0.0397\n",
      "T5 Response Generation bertscore_f1: 0.1903\n",
      "\n",
      "Additional Generation Metrics for T5 QA Model:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 QA rougeL: 0.1596\n",
      "T5 QA bertscore_f1: 0.6831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAdditional Generation Metrics for T5 Response Generation:\")\n",
    "additional_resp_metrics = compute_generation_metrics(trainer_resp, resp_test_tok, tokenizer_t5)\n",
    "for metric, value in additional_resp_metrics.items():\n",
    "    print(f\"T5 Response Generation {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nAdditional Generation Metrics for T5 QA Model:\")\n",
    "additional_qa_metrics = compute_generation_metrics(trainer_qa, qa_test_tok, tokenizer_t5)\n",
    "for metric, value in additional_qa_metrics.items():\n",
    "    print(f\"T5 QA {metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
