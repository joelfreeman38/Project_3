{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528fc79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mward\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, T5ForConditionalGeneration, TrainingArguments,\n",
    "                          Trainer, Seq2SeqTrainer, DataCollatorWithPadding)\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Trainer\n",
    "from transformers import TrainerCallback\n",
    "from transformers import default_data_collator\n",
    "from transformers import logging as hf_logging\n",
    "import datetime\n",
    "import evaluate\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import numpy as np, torch\n",
    "import os\n",
    "import os, pprint, glob\n",
    "import os, random, json, time, itertools\n",
    "import pandas as pd\n",
    "import speech_recognition as sr\n",
    "import tempfile\n",
    "import torch\n",
    "import torch, torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "import warnings, logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d559dfc",
   "metadata": {},
   "source": [
    "# Mental‑Health Chatbot: Training & Deployment\n",
    "\n",
    "This notebook walks through **building, training, and serving** a multi‑task mental‑health chatbot that:\n",
    "\n",
    "1. Detects the user's emotions (multi‑label classification).\n",
    "2. Generates empathetic free‑text responses.\n",
    "3. Answers direct mental‑health questions accurately.\n",
    "\n",
    "We combine three fine‑tuned Hugging Face models:\n",
    "\n",
    "| Task | Base model | Output dir |\n",
    "|------|------------|------------|\n",
    "| Emotion classification | `SamLowe/roberta-base-go_emotions` | `./saved_models/emotion_classifier` |\n",
    "| Response generation | `google/t5-small` (or any T5) | `./saved_models/t5_response_generator` |\n",
    "| Question‑answering | `google/t5-small` (or any T5) | `./saved_models/t5_qa` |\n",
    "\n",
    "Finally, we wire them together in a small **pipeline** and expose it through a minimal [Gradio](https://gradio.app) UI that *remembers* the conversation.\n",
    "\n",
    "> **Tip** Training large models can take a while. Feel free to toggle individual datasets on/off or start with a tiny subset while you iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2238a52",
   "metadata": {},
   "source": [
    "## 0. Environment setup *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb4fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab/Kaggle add any missing libraries:\n",
    "# !pip install -q transformers datasets evaluate bert-score gradio sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e74c4",
   "metadata": {},
   "source": [
    "## 1. Imports & global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6fa4082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "hf_logging.set_verbosity_info()     # show INFO messages from Trainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                          \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Ensure save directories exist\n",
    "SAVE_ROOT = Path(\"./saved_models\")\n",
    "for sub in [\"emotion_classifier\", \"t5_response_generator\", \"t5_qa\", \"final_combined\"]:\n",
    "    (SAVE_ROOT / sub).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a848229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data collator: ensures labels are float32 tensors -----------------\n",
    "def float_label_collator(features):\n",
    "    batch = default_data_collator(features)\n",
    "    if \"labels\" in batch:\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float32)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02db9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Trainer for multi-label BCE loss ---------------------------\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    \"\"\"Casts labels to float32 and reshapes to logits shape if needed.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\").float()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if labels.shape != logits.shape:\n",
    "            labels = labels.view_as(logits)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels, reduction=\"mean\")\n",
    "        print(\"LOGITS\", logits.shape, \"LABELS\", labels.shape)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3130d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Data collator to ensure BCEWithLogitsLoss gets float labels ───\n",
    "\n",
    "def float_label_collator(features):\n",
    "    batch = default_data_collator(features)\n",
    "    if \"labels\" in batch:\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float32)\n",
    "    print(\"collator labels dtype/shape:\", batch[\"labels\"].dtype, batch[\"labels\"].shape)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cb0788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Callback: print step loss + epoch eval while keeping tqdm bar ───\n",
    "class StepPrinter(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs or not state.is_local_process_zero:\n",
    "            return\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step:>6} • loss {logs['loss']:.4f}\")\n",
    "        if \"eval_loss\" in logs:\n",
    "            metric = logs.get(\"micro_f1\") or logs.get(\"bertscore_f1\") or logs.get(\"rougeL\")\n",
    "            metric_str = f\" • metric {metric:.4f}\" if metric is not None else \"\"\n",
    "            print(f\"Epoch {int(state.epoch)}/{int(args.num_train_epochs)}\"\n",
    "                  f\" • eval_loss {logs['eval_loss']:.4f}{metric_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125cb3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# Callback: print \"Step xx • loss ...\" and epoch eval metrics\n",
    "class StepPrinter(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs or not state.is_local_process_zero:\n",
    "            return\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step:>6} • loss {logs['loss']:.4f}\")\n",
    "        if \"eval_loss\" in logs:\n",
    "            f1 = logs.get(\"micro_f1\") or logs.get(\"bertscore_f1\") or logs.get(\"rougeL\")\n",
    "            extra = f\" • metric {f1:.4f}\" if f1 is not None else \"\"\n",
    "            print(f\"Epoch {int(state.epoch)}/{int(args.num_train_epochs)}\"\n",
    "                  f\" • eval_loss {logs['eval_loss']:.4f}{extra}\")\n",
    "# ──────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44a6b5",
   "metadata": {},
   "source": [
    "## 2. Dataset switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a7dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle individual CSVs and provide their column mapping\n",
    "# Format: name: (enabled, path, question_col, answer_col)\n",
    "DATASETS = {\n",
    "    \"ds1\": (True,  \"./data/ds1_transformed_mental_health_chatbot_dataset.csv\",  \"question\", \"answer\"),\n",
    "    \"ds2\": (False,  \"./data/ds2_transformed_mental_health_chatbot.csv\",         \"question\", \"answer\"),\n",
    "    \"ds3\": (False,  \"./data/ds3_mental_health_faq_cleaned.csv\",                 \"Question\", \"Answer\"),\n",
    "    \"ds4\": (False,  \"./data/ds4_mental_health_chatbot_dataset_merged_modes.csv\",\"prompt\",   \"response\"),\n",
    "    \"ds5\": (False,  \"./data/ds5_Mental_Health_FAQ.csv\",                         \"Question\", \"Answer\"),\n",
    "    \"ds6\": (False, \"./data/ds6_mental_health_counseling.csv\",                  \"query\",    \"completion\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479db785",
   "metadata": {},
   "source": [
    "## 3. Load & preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd4a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust cleaner that auto‑maps columns to 'question' / 'answer'\n",
    "def load_and_clean(path, q_col, a_col):\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # normalise headers\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    q_col = q_col.lower().strip()\n",
    "    a_col = a_col.lower().strip()\n",
    "\n",
    "    # common renames\n",
    "    rename_map = {\n",
    "        \"prompt\": \"question\",\n",
    "        \"response\": \"answer\",\n",
    "        \"questions\": \"question\",\n",
    "        \"answers\": \"answer\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # if provided cols exist, rename them\n",
    "    if q_col in df.columns:\n",
    "        df = df.rename(columns={q_col: \"question\"})\n",
    "    if a_col in df.columns:\n",
    "        df = df.rename(columns={a_col: \"answer\"})\n",
    "\n",
    "        # Try to map context → question if needed\n",
    "    if \"question\" not in df.columns and \"context\" in df.columns:\n",
    "        df = df.rename(columns={\"context\": \"question\"})\n",
    "\n",
    "    if not {\"question\", \"answer\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Could not find 'question'/'answer' in {path}. Available columns: {list(df.columns)}\")\n",
    "\n",
    "\n",
    "\n",
    "    df = df[[\"question\", \"answer\"]].dropna()\n",
    "    df[\"question\"] = df[\"question\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df[\"answer\"]   = df[\"answer\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return Dataset.from_pandas(df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07112162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1.  Put this once, after your imports ──────────────────────────\n",
    "\n",
    "def float_label_collator(features):\n",
    "    \"\"\"\n",
    "    Wrap the default HF collator but cast the `labels` tensor to float32\n",
    "    so BCEWithLogitsLoss gets the right dtype.\n",
    "    \"\"\"\n",
    "    batch = default_data_collator(features)\n",
    "    if \"labels\" in batch:\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float32)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e7f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Final version of MultiLabelTrainer ─────────────────────────────\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    BCEWithLogitsLoss that survives any label dtype/shape:\n",
    "    * casts to float32\n",
    "    * reshapes to logits.shape when needed\n",
    "    \"\"\"\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        labels = inputs.pop(\"labels\").float()        # cast dtype\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Fix shape if the collator flattened the labels\n",
    "        if labels.shape != logits.shape:\n",
    "            labels = labels.view_as(logits)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels, reduction=\"mean\")\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d2a3e",
   "metadata": {},
   "source": [
    "### 3.1 Emotion label setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999ba5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels from the GoEmotions paper (27 emotions + neutral)\n",
    "GO_EMOTION_LABELS = [\n",
    "    'admiration','amusement','anger','annoyance','approval','caring','confusion',\n",
    "    'curiosity','desire','disappointment','disapproval','disgust','embarrassment',\n",
    "    'excitement','fear','gratitude','grief','joy','love','nervousness','optimism',\n",
    "    'pride','realization','relief','remorse','sadness','surprise','neutral'\n",
    "]\n",
    "num_labels = len(GO_EMOTION_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8fbab",
   "metadata": {},
   "source": [
    "### 3.2 Binarize emotion annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fde6ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 154 • test: 18\n"
     ]
    }
   ],
   "source": [
    "# ── Re‑create train_ds / test_ds ───────────────────────────────────\n",
    "datasets_list = []\n",
    "for name, (enabled, path, q_col, a_col) in DATASETS.items():\n",
    "    if not enabled:\n",
    "        continue\n",
    "    ds = load_and_clean(path=path, q_col=q_col, a_col=a_col)\n",
    "    datasets_list.append(ds)\n",
    "\n",
    "if not datasets_list:\n",
    "    print(\"No datasets were enabled, using a fallback test dataset.\")\n",
    "    fallback_data = {\n",
    "        \"text\": [\n",
    "            \"How are you?\",\n",
    "            \"I feel really down today.\",\n",
    "            \"I'm so happy with my progress!\",\n",
    "            \"Why does nobody understand me?\",\n",
    "            \"I'm feeling anxious about school.\",\n",
    "            \"Life is good lately.\",\n",
    "            \"Sometimes I just want to cry.\",\n",
    "            \"Everything is falling apart.\",\n",
    "            \"I’m grateful for my therapist.\",\n",
    "            \"Can someone please just listen?\"\n",
    "        ]\n",
    "    }\n",
    "    ds = Dataset.from_dict(fallback_data)\n",
    "    datasets_list.append(ds)\n",
    "\n",
    "\n",
    "full_ds = concatenate_datasets(datasets_list) if len(datasets_list) > 1 else datasets_list[0]\n",
    "full_ds = full_ds.shuffle(seed=SEED)\n",
    "\n",
    "split = full_ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "train_ds, test_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(f\"train: {len(train_ds):,} • test: {len(test_ds):,}\")\n",
    "# ───────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc7b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not datasets_list:\n",
    "    print(\"No datasets were enabled, using a fallback test dataset.\")\n",
    "    fallback_data = {\n",
    "        \"text\": [\n",
    "            \"How are you?\",\n",
    "            \"I feel really down today.\",\n",
    "            \"I'm so happy with my progress!\",\n",
    "            \"Why does nobody understand me?\",\n",
    "            \"I'm feeling anxious about school.\",\n",
    "            \"Life is good lately.\",\n",
    "            \"Sometimes I just want to cry.\",\n",
    "            \"Everything is falling apart.\",\n",
    "            \"I’m grateful for my therapist.\",\n",
    "            \"Can someone please just listen?\"\n",
    "        ]\n",
    "    }\n",
    "    ds = Dataset.from_dict(fallback_data)\n",
    "    datasets_list.append(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89294c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8490330f7124cd1b479733d611a8776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/154 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e0d2580524477089805b9546a1f28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m emo_train \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mmap(annotate_emotions)\n\u001b[0;32m     22\u001b[0m emo_test  \u001b[38;5;241m=\u001b[39m test_ds\u001b[38;5;241m.\u001b[39mmap(annotate_emotions)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43memo_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m->\u001b[39m\u001b[38;5;124m\"\u001b[39m, emo_train[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotions\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "# For this demo we'll fake some annotations by mapping keywords -> emotions.\n",
    "# Replace with your real emotion annotations if available.\n",
    "KEYWORD2EMO = {\n",
    "    \"sad\": \"sadness\", \"angry\": \"anger\", \"happy\": \"joy\",\n",
    "    \"thank\": \"gratitude\", \"sorry\": \"remorse\", \"love\": \"love\",\n",
    "    \"fear\": \"fear\", \"nervous\": \"nervousness\"\n",
    "}\n",
    "\n",
    "def annotate_emotions(example):\n",
    "    emos = example.get(\"emotions\", [])\n",
    "    \n",
    "    # If no emotions assigned, default to [\"neutral\"]\n",
    "    if not emos:\n",
    "        emos = [\"neutral\"]\n",
    "    \n",
    "    example[\"emotions\"] = emos  # store for visibility\n",
    "    example[\"labels\"] = [1.0 if lbl in emos else 0.0 for lbl in GO_EMOTION_LABELS]\n",
    "    return example\n",
    "\n",
    "\n",
    "emo_train = train_ds.map(annotate_emotions)\n",
    "emo_test  = test_ds.map(annotate_emotions)\n",
    "\n",
    "print(\"Sample:\", emo_train[0][\"text\"], \"->\", emo_train[0][\"emotions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1fac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4682bcb97c774664a188454d9adb3dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da24b4b1692462081f4a75be81c2346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def has_nonzero_labels(example):\n",
    "    return sum(example[\"labels\"]) > 0\n",
    "\n",
    "# Apply to your actual datasets\n",
    "emo_train = emo_train.filter(has_nonzero_labels)\n",
    "emo_test = emo_test.filter(has_nonzero_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad label examples: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for label issues\n",
    "bad_labels = [ex for ex in emo_train if \"labels\" not in ex or sum(ex[\"labels\"]) == 0]\n",
    "\n",
    "print(\"Number of bad label examples:\", len(bad_labels))\n",
    "\n",
    "# Print the first bad one if any\n",
    "if bad_labels:\n",
    "    print(\"Example with bad label:\", bad_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bba92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "emo_tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd79577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28b8a29233140c8981911934673a637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c868de21e5443783a82ecab548db74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c9b5a7817344bf99056703fd577868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d818ecafdd774f4f836c24b94ae62f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def emo_tokenize(batch):\n",
    "    # Dynamically detect which field contains the input text\n",
    "    if \"text\" in batch:\n",
    "        input_texts = batch[\"text\"]\n",
    "    elif \"question\" in batch:\n",
    "        input_texts = batch[\"question\"]\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'text' nor 'question' column found in dataset.\")\n",
    "\n",
    "    return emo_tokenizer(input_texts, padding=True, truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example\n",
    "\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf9090",
   "metadata": {},
   "source": [
    "## 4. Train emotion classifier (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21d877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'emotions', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(emo_train.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82e440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"SamLowe/roberta-base-go_emotions\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at SamLowe/roberta-base-go_emotions.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5dbfa5d212b424598ca35958a2aa13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 16\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m example\n\u001b[0;32m     13\u001b[0m emo_model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamLowe/roberta-base-go_emotions\u001b[39m\u001b[38;5;124m\"\u001b[39m, problem_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(GO_EMOTION_LABELS))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m emo_train_tok \u001b[38;5;241m=\u001b[39m \u001b[43memo_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43memo_tokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmap(cast_to_float)\n\u001b[0;32m     17\u001b[0m emo_test_tok  \u001b[38;5;241m=\u001b[39m emo_test\u001b[38;5;241m.\u001b[39mmap(emo_tokenize, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmap(cast_to_float)\n\u001b[0;32m     19\u001b[0m emo_train_tok\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\datasets\\arrow_dataset.py:3519\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3518\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 3519\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[0;32m   3520\u001b[0m         num_examples_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(i)\n\u001b[0;32m   3521\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\datasets\\arrow_dataset.py:3469\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[1;34m(shard_iterable)\u001b[0m\n\u001b[0;32m   3467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[1;32m-> 3469\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\datasets\\arrow_dataset.py:3392\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[1;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[0;32m   3390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[0;32m   3391\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[1;32m-> 3392\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m, in \u001b[0;36memo_tokenize\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21memo_tokenize\u001b[39m(batch):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m emo_tokenizer(\n\u001b[1;32m----> 3\u001b[0m         \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[0;32m      4\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m      7\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\datasets\\formatting\\formatting.py:280\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 280\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m    282\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'question'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(\n",
    "        batch[\"question\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example\n",
    "\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\", problem_type=\"multi_label_classification\", num_labels=len(GO_EMOTION_LABELS)).to(device)\n",
    "\n",
    "\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "# Metrics\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_emo_metrics(pred):\n",
    "\n",
    "    logits, labels = pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.3).int().numpy()\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Defensive check: filter out rows where labels.sum() == 0\n",
    "    mask = labels.sum(axis=1) > 0\n",
    "    if mask.sum() == 0:\n",
    "        print(\"Warning: all evaluation labels are empty\")\n",
    "        return {\"micro_f1\": 0.0}\n",
    "\n",
    "    try:\n",
    "        f1 = f1_score(labels[mask], preds[mask], average=\"micro\", zero_division=0)\n",
    "    except ValueError as e:\n",
    "        print(\"Metric error:\", e)\n",
    "        f1 = 0.0\n",
    "\n",
    "    return {\"micro_f1\": f1}\n",
    "\n",
    "\n",
    "\n",
    "emo_args = TrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"emotion_classifier\"),\n",
    "\n",
    "    # logging\n",
    "    logging_strategy=\"steps\", logging_steps=10, logging_dir=\"./logs\", report_to=\"none\",\n",
    "\n",
    "    # core hyper‑params\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # eval / ckpt\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer_emo = MultiLabelTrainer(\n",
    "    model=emo_model,\n",
    "    args=emo_args,\n",
    "    train_dataset=emo_train_tok,\n",
    "    eval_dataset=emo_test_tok,\n",
    "    tokenizer=emo_tokenizer,\n",
    "    data_collator=float_label_collator,\n",
    "    compute_metrics=compute_emo_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")\n",
    "\n",
    "\n",
    "# Uncomment to train (may take a while)\n",
    "trainer_emo.train()\n",
    "\n",
    "# Save robustly\n",
    "(SAVE_ROOT/\"emotion_classifier\").mkdir(exist_ok=True, parents=True)\n",
    "emo_model.save_pretrained(SAVE_ROOT/\"emotion_classifier\")\n",
    "emo_tokenizer.save_pretrained(SAVE_ROOT/\"emotion_classifier\")\n",
    "\n",
    "trainer_emo.save_model()            # writes to output_dir\n",
    "emo_tokenizer.save_pretrained(emo_args.output_dir)\n",
    "\n",
    "\n",
    "# Cast multi‑label targets to float32 tensors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367d7ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input_ids shape: torch.Size([128])\n",
      "Sample labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "Label dtype: <class 'torch.Tensor'> — torch.float32\n",
      "Model expects num_labels: 28\n"
     ]
    }
   ],
   "source": [
    "sample = emo_train_tok[0]\n",
    "print(\"Sample input_ids shape:\", sample[\"input_ids\"].shape)\n",
    "print(\"Sample labels:\", sample[\"labels\"])\n",
    "print(\"Label dtype:\", type(sample[\"labels\"]), \"—\", sample[\"labels\"].dtype if hasattr(sample[\"labels\"], 'dtype') else \"no dtype\")\n",
    "print(\"Model expects num_labels:\", emo_model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb45b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1583f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=resp_tokenizer,\n",
    "    model=resp_model,\n",
    "    padding=True,  # enables dynamic padding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840db175",
   "metadata": {},
   "source": [
    "## 5. Train T5 for response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3e970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f20ef5b52440f98bc5c4b4c550b53f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8f9433e8fb48a2b4ed326fb3465a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ebaccd34c6436ba270009a0ad56b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0049be75df7b464dbc38f0f20a9dfd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,910\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 717\n",
      "  Number of trainable parameters = 60,506,624\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='717' max='717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [717/717 04:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bertscore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.572100</td>\n",
       "      <td>3.410602</td>\n",
       "      <td>0.028149</td>\n",
       "      <td>0.236706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.556800</td>\n",
       "      <td>3.370675</td>\n",
       "      <td>0.026825</td>\n",
       "      <td>0.235849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.493100</td>\n",
       "      <td>3.361464</td>\n",
       "      <td>0.028112</td>\n",
       "      <td>0.236428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     10 • loss 4.4311\n",
      "Step     20 • loss 3.8409\n",
      "Step     30 • loss 3.7893\n",
      "Step     40 • loss 3.8028\n",
      "Step     50 • loss 3.7969\n",
      "Step     60 • loss 3.7434\n",
      "Step     70 • loss 3.6975\n",
      "Step     80 • loss 3.8072\n",
      "Step     90 • loss 3.7424\n",
      "Step    100 • loss 3.7283\n",
      "Step    110 • loss 3.7070\n",
      "Step    120 • loss 3.6852\n",
      "Step    130 • loss 3.6526\n",
      "Step    140 • loss 3.6594\n",
      "Step    150 • loss 3.7383\n",
      "Step    160 • loss 3.7420\n",
      "Step    170 • loss 3.6619\n",
      "Step    180 • loss 3.7016\n",
      "Step    190 • loss 3.6599\n",
      "Step    200 • loss 3.6751\n",
      "Step    210 • loss 3.6257\n",
      "Step    220 • loss 3.7237\n",
      "Step    230 • loss 3.5721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 213\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\tokenizer_config.json\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\model.safetensors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 • eval_loss 3.4106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-239\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-239\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-239\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-239\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-239\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-239\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-239\\spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    240 • loss 3.7082\n",
      "Step    250 • loss 3.5320\n",
      "Step    260 • loss 3.6090\n",
      "Step    270 • loss 3.5882\n",
      "Step    280 • loss 3.5946\n",
      "Step    290 • loss 3.5254\n",
      "Step    300 • loss 3.5369\n",
      "Step    310 • loss 3.5676\n",
      "Step    320 • loss 3.6388\n",
      "Step    330 • loss 3.5320\n",
      "Step    340 • loss 3.5776\n",
      "Step    350 • loss 3.6009\n",
      "Step    360 • loss 3.5462\n",
      "Step    370 • loss 3.5274\n",
      "Step    380 • loss 3.5207\n",
      "Step    390 • loss 3.4964\n",
      "Step    400 • loss 3.5077\n",
      "Step    410 • loss 3.5212\n",
      "Step    420 • loss 3.6007\n",
      "Step    430 • loss 3.5353\n",
      "Step    440 • loss 3.5469\n",
      "Step    450 • loss 3.5508\n",
      "Step    460 • loss 3.5572\n",
      "Step    470 • loss 3.5568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 213\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 • eval_loss 3.3707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-478\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-478\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-478\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-478\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-478\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-478\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-478\\spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    480 • loss 3.5134\n",
      "Step    490 • loss 3.5229\n",
      "Step    500 • loss 3.3536\n",
      "Step    510 • loss 3.4548\n",
      "Step    520 • loss 3.4801\n",
      "Step    530 • loss 3.4374\n",
      "Step    540 • loss 3.5200\n",
      "Step    550 • loss 3.4630\n",
      "Step    560 • loss 3.5662\n",
      "Step    570 • loss 3.4424\n",
      "Step    580 • loss 3.5012\n",
      "Step    590 • loss 3.5104\n",
      "Step    600 • loss 3.4781\n",
      "Step    610 • loss 3.5311\n",
      "Step    620 • loss 3.4986\n",
      "Step    630 • loss 3.4477\n",
      "Step    640 • loss 3.5346\n",
      "Step    650 • loss 3.4577\n",
      "Step    660 • loss 3.5015\n",
      "Step    670 • loss 3.4719\n",
      "Step    680 • loss 3.4505\n",
      "Step    690 • loss 3.4871\n",
      "Step    700 • loss 3.4630\n",
      "Step    710 • loss 3.4931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-717\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-717\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-717\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-717\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-717\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-717\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-717\\spiece.model\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 213\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 • eval_loss 3.3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-717\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-717\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-717\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-717\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-717\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-717\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-717\\spiece.model\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in saved_models\\t5_response_generator\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\spiece.model\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\n",
      "Configuration saved in saved_models\\t5_response_generator\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\spiece.model\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_models\\\\t5_response_generator\\\\tokenizer_config.json',\n",
       " 'saved_models\\\\t5_response_generator\\\\special_tokens_map.json',\n",
       " 'saved_models\\\\t5_response_generator\\\\spiece.model',\n",
       " 'saved_models\\\\t5_response_generator\\\\added_tokens.json',\n",
       " 'saved_models\\\\t5_response_generator\\\\tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build input/target pairs: user text -> helpful response\n",
    "# For now we use 'question' as input and 'answer' as target\n",
    "def build_t5_pairs(example):\n",
    "    example[\"input_text\"]  = \"respond: \" + example[\"question\"]\n",
    "    example[\"target_text\"] = example[\"answer\"]\n",
    "    return example\n",
    "\n",
    "resp_train = train_ds.map(build_t5_pairs)\n",
    "resp_test  = test_ds.map(build_t5_pairs)\n",
    "\n",
    "t5_resp_model_name = \"t5-small\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_resp_model_name)\n",
    "\n",
    "def t5_tokenize(batch):\n",
    "    model_inputs = tokenizer_t5(batch[\"input_text\"], max_length=128, truncation=True)\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(batch[\"target_text\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "resp_train_tok = resp_train.map(t5_tokenize, batched=True, remove_columns=resp_train.column_names)\n",
    "resp_test_tok  = resp_test.map(t5_tokenize, batched=True, remove_columns=resp_test.column_names)\n",
    "\n",
    "resp_train_tok.set_format(\"torch\")\n",
    "resp_test_tok.set_format(\"torch\")\n",
    "\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(t5_resp_model_name).to(device)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_resp_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer_t5.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer_t5.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    r = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    b = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    \n",
    "    return {\n",
    "        \"rougeL\": r[\"rougeL\"],\n",
    "        \"bertscore_f1\": np.mean(b[\"f1\"])\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "resp_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"t5_response_generator\"),\n",
    "\n",
    "    # logging\n",
    "    logging_strategy=\"steps\", logging_steps=10, logging_dir=\"./logs\", report_to=\"none\",\n",
    "\n",
    "    # core hyper‑params\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # eval / ckpt\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,   # now valid\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer_resp = Seq2SeqTrainer(\n",
    "    model=resp_model,\n",
    "    args=resp_args,\n",
    "    train_dataset=resp_train_tok,\n",
    "    eval_dataset=resp_test_tok,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=data_collator,  # ✅ keep this\n",
    "    compute_metrics=compute_resp_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")\n",
    "\n",
    "\n",
    "# Uncomment to train\n",
    "trainer_resp.train()\n",
    "\n",
    "# Robust save\n",
    "resp_model.save_pretrained(SAVE_ROOT/\"t5_response_generator\")\n",
    "tokenizer_t5.save_pretrained(SAVE_ROOT/\"t5_response_generator\"),\n",
    "callbacks=[StepPrinter]\n",
    "trainer_resp.save_model()\n",
    "tokenizer_t5.save_pretrained(resp_args.output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92020c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 58\n",
      "Label length: 128\n",
      "Label: tensor([ 6834,    53,    12,   143,  1112,    11,  3558,    21,  2927,    19,\n",
      "            8,   166,  1147,     5,  2035, 10622,  6261,    11,  7562,    19,\n",
      "           59,    46,   514, 20906,    12,   103,    30,    39,   293,     6,\n",
      "          902,     3,    99,    34,    31,     7,   118,   424,    25,    43,\n",
      "          118,     3, 25764,    21,   203,     5,   290,    33,   128,   315,\n",
      "         9729,    25,   164,   281,   323,    16,   455,    12,   456,     8,\n",
      "         5853,   433,    10,   634,  5846,    63,     3,    18,   253,     3,\n",
      "            9,     3,  9962,   113,    65,     3,     9,   418,    13,   351,\n",
      "           28,  6261,    11,  7562,     5,   148,    54,   103,    48,    57,\n",
      "          281,    32,   122,   697,    96,   152,   226, 27115,  7562,     3,\n",
      "         9962,    96,    16,    39,   690,     5, 15702,   254,   547,    92,\n",
      "           65,     3,     9,  8174,    13,  1237,     3,  9962,     7,     6,\n",
      "           38,   405, 18380,  1960, 15789,   257,     3,     1])\n",
      "Type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "sample = resp_train_tok[0]\n",
    "print(\"Input length:\", len(sample[\"input_ids\"]))\n",
    "print(\"Label length:\", len(sample[\"labels\"]))\n",
    "print(\"Label:\", sample[\"labels\"])\n",
    "print(\"Type:\", type(sample[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6140c",
   "metadata": {},
   "source": [
    "## 6. Train T5 for question‑answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a44457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_models/t5_qa\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models/t5_qa\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models/t5_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models/t5_qa\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(\"saved_models/t5_qa\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"saved_models/t5_qa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb440c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer_t5,\n",
    "    model=qa_model,\n",
    "    padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51577a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16fce275851840e9b44c8ba8b8ad36a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d820a68214c416781cc1be6c8a8e71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d14defbad6e4eddbee39e81c1e6cd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1910 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f6a275b44f46c282393b98f4807b08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/213 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 1,910\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 717\n",
      "  Number of trainable parameters = 60,506,624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='717' max='717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [717/717 05:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bertscore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.570200</td>\n",
       "      <td>3.411528</td>\n",
       "      <td>0.057469</td>\n",
       "      <td>0.470592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.554800</td>\n",
       "      <td>3.372182</td>\n",
       "      <td>0.064642</td>\n",
       "      <td>0.540780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.495400</td>\n",
       "      <td>3.361251</td>\n",
       "      <td>0.046253</td>\n",
       "      <td>0.413051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step     10 • loss 4.4628\n",
      "Step     20 • loss 3.8440\n",
      "Step     30 • loss 3.7900\n",
      "Step     40 • loss 3.8011\n",
      "Step     50 • loss 3.8050\n",
      "Step     60 • loss 3.7467\n",
      "Step     70 • loss 3.7024\n",
      "Step     80 • loss 3.8160\n",
      "Step     90 • loss 3.7450\n",
      "Step    100 • loss 3.7342\n",
      "Step    110 • loss 3.7049\n",
      "Step    120 • loss 3.6842\n",
      "Step    130 • loss 3.6596\n",
      "Step    140 • loss 3.6615\n",
      "Step    150 • loss 3.7443\n",
      "Step    160 • loss 3.7406\n",
      "Step    170 • loss 3.6558\n",
      "Step    180 • loss 3.6972\n",
      "Step    190 • loss 3.6568\n",
      "Step    200 • loss 3.6759\n",
      "Step    210 • loss 3.6259\n",
      "Step    220 • loss 3.7255\n",
      "Step    230 • loss 3.5702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 213\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 • eval_loss 3.4115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-239\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-239\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-239\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-239\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-239\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-239\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-239\\spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    240 • loss 3.7075\n",
      "Step    250 • loss 3.5316\n",
      "Step    260 • loss 3.6084\n",
      "Step    270 • loss 3.5949\n",
      "Step    280 • loss 3.5977\n",
      "Step    290 • loss 3.5222\n",
      "Step    300 • loss 3.5384\n",
      "Step    310 • loss 3.5699\n",
      "Step    320 • loss 3.6473\n",
      "Step    330 • loss 3.5316\n",
      "Step    340 • loss 3.5747\n",
      "Step    350 • loss 3.5966\n",
      "Step    360 • loss 3.5496\n",
      "Step    370 • loss 3.5213\n",
      "Step    380 • loss 3.5257\n",
      "Step    390 • loss 3.4918\n",
      "Step    400 • loss 3.5189\n",
      "Step    410 • loss 3.5261\n",
      "Step    420 • loss 3.6020\n",
      "Step    430 • loss 3.5392\n",
      "Step    440 • loss 3.5475\n",
      "Step    450 • loss 3.5585\n",
      "Step    460 • loss 3.5598\n",
      "Step    470 • loss 3.5548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 213\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 • eval_loss 3.3722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-478\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-478\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-478\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-478\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-478\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-478\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-478\\spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    480 • loss 3.5252\n",
      "Step    490 • loss 3.5245\n",
      "Step    500 • loss 3.3566\n",
      "Step    510 • loss 3.4640\n",
      "Step    520 • loss 3.4890\n",
      "Step    530 • loss 3.4389\n",
      "Step    540 • loss 3.5220\n",
      "Step    550 • loss 3.4664\n",
      "Step    560 • loss 3.5681\n",
      "Step    570 • loss 3.4476\n",
      "Step    580 • loss 3.5036\n",
      "Step    590 • loss 3.5184\n",
      "Step    600 • loss 3.4785\n",
      "Step    610 • loss 3.5316\n",
      "Step    620 • loss 3.5008\n",
      "Step    630 • loss 3.4488\n",
      "Step    640 • loss 3.5355\n",
      "Step    650 • loss 3.4592\n",
      "Step    660 • loss 3.5064\n",
      "Step    670 • loss 3.4792\n",
      "Step    680 • loss 3.4586\n",
      "Step    690 • loss 3.4929\n",
      "Step    700 • loss 3.4617\n",
      "Step    710 • loss 3.4954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-717\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-717\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-717\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-717\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-717\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-717\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-717\\spiece.model\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 213\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 • eval_loss 3.3613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-717\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-717\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-717\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-717\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-717\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-717\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-717\\spiece.model\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in saved_models\\t5_qa\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\special_tokens_map.json\n",
      "Saving model checkpoint to saved_models\\t5_qa\n",
      "Configuration saved in saved_models\\t5_qa\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\special_tokens_map.json\n",
      "tokenizer config file saved in saved_models\\t5_qa\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_models\\\\t5_qa\\\\tokenizer_config.json',\n",
       " 'saved_models\\\\t5_qa\\\\special_tokens_map.json',\n",
       " 'saved_models\\\\t5_qa\\\\spiece.model',\n",
       " 'saved_models\\\\t5_qa\\\\added_tokens.json',\n",
       " 'saved_models\\\\t5_qa\\\\tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build QA pairs: \"question: <text>\" -> answer\n",
    "def build_qa_pairs(example):\n",
    "    example[\"input_text\"]  = \"question: \" + example[\"question\"]\n",
    "    example[\"target_text\"] = example[\"answer\"]\n",
    "    return example\n",
    "\n",
    "qa_train = train_ds.map(build_qa_pairs)\n",
    "qa_test  = test_ds.map(build_qa_pairs)\n",
    "\n",
    "def qa_tokenize(batch):\n",
    "    model_inputs = tokenizer_t5(batch[\"input_text\"], max_length=128, truncation=True)\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(batch[\"target_text\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "qa_train_tok = qa_train.map(qa_tokenize, batched=True, remove_columns=qa_train.column_names)\n",
    "qa_test_tok  = qa_test.map(qa_tokenize, batched=True, remove_columns=qa_test.column_names)\n",
    "\n",
    "qa_train_tok.set_format(\"torch\")\n",
    "qa_test_tok.set_format(\"torch\")\n",
    "\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(t5_resp_model_name).to(device)\n",
    "\n",
    "qa_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"t5_qa\"),\n",
    "\n",
    "    logging_strategy=\"steps\", logging_steps=10, logging_dir=\"./logs\", report_to=\"none\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer_qa = Seq2SeqTrainer(\n",
    "    model=qa_model,\n",
    "    args=qa_args,\n",
    "    train_dataset=qa_train_tok,\n",
    "    eval_dataset=qa_test_tok,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=qa_collator,\n",
    "    compute_metrics=compute_resp_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")\n",
    "\n",
    "# Uncomment to train\n",
    "trainer_qa.train()\n",
    "\n",
    "qa_model.save_pretrained(SAVE_ROOT/\"t5_qa\")\n",
    "tokenizer_t5.save_pretrained(SAVE_ROOT/\"t5_qa\")\n",
    "callbacks=[StepPrinter]\n",
    "trainer_qa.save_model()\n",
    "tokenizer_t5.save_pretrained(qa_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b08e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top‑level content of saved_models:\n",
      "['emotion_classifier', 'final_combined', 't5_qa', 't5_response_generator']\n",
      "\n",
      "Any checkpoint folders?\n",
      "['saved_models\\\\t5_response_generator\\\\checkpoint-116',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-174',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-20',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-239',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-40',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-478',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-58',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-60',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-717']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root = Path(\"./saved_models\")\n",
    "\n",
    "print(\"Top‑level content of saved_models:\")\n",
    "pprint.pprint(os.listdir(root))\n",
    "\n",
    "print(\"\\nAny checkpoint folders?\")\n",
    "ckpts = sorted(glob.glob(str(root / \"t5_response_generator\" / \"checkpoint-*\")))\n",
    "pprint.pprint(ckpts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006aba6c",
   "metadata": {},
   "source": [
    "## 7. Combined pipeline & Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d25a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_models\\emotion_classifier\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_models\\\\emotion_classifier\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\emotion_classifier\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at saved_models\\emotion_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models\\t5_qa\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\t5_qa\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models\\t5_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models\\t5_qa\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    }
   ],
   "source": [
    "# Load (fine‑tuned) models back in – comment these if still in memory\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(SAVE_ROOT/\"emotion_classifier\").to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(SAVE_ROOT/\"emotion_classifier\")\n",
    "\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(SAVE_ROOT/\"t5_qa\").to(device)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(SAVE_ROOT/\"t5_qa\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "class MentalHealthChatbotPipeline:\n",
    "    def __init__(self, labels, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.labels = labels\n",
    "        self.chat_history = []  # (speaker, text)\n",
    "\n",
    "        self.emo_model = emo_model.eval()\n",
    "        self.qa_model  = qa_model.eval()\n",
    "        self.resp_model = resp_model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, text, max_length=128):\n",
    "        self.chat_history.append((\"user\", text))\n",
    "\n",
    "        # Emotion detection\n",
    "        emo_inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        probs = torch.sigmoid(self.emo_model(**emo_inputs).logits)[0]\n",
    "        emotions = [lbl for lbl, p in zip(self.labels, probs) if p > 0.3]\n",
    "\n",
    "        # Pick model\n",
    "        model, tok = (self.qa_model, qa_tokenizer) if \"?\" in text else (self.resp_model, resp_tokenizer)\n",
    "        ids = model.generate(**tok(text, return_tensors=\"pt\").to(self.device),\n",
    "                             max_length=max_length)\n",
    "        reply = tok.decode(ids[0], skip_special_tokens=True)\n",
    "        self.chat_history.append((\"bot\", reply))\n",
    "\n",
    "        return {\"Detected Emotions\": emotions, \"Response\": reply, \"History\": self.chat_history}\n",
    "\n",
    "chatbot = MentalHealthChatbotPipeline(labels=GO_EMOTION_LABELS, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76626236",
   "metadata": {},
   "source": [
    "## 8. Save final bundle metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3aef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to saved_models\\final_combined\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    \"emotion_model\": str(SAVE_ROOT/\"emotion_classifier\"),\n",
    "    \"response_model\": str(SAVE_ROOT/\"t5_response_generator\"),\n",
    "    \"qa_model\": str(SAVE_ROOT/\"t5_qa\"),\n",
    "    \"labels\": GO_EMOTION_LABELS\n",
    "}\n",
    "with open(SAVE_ROOT/\"final_combined\"/\"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"Metadata saved to\", SAVE_ROOT/\"final_combined\"/\"metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6257e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/gradio-messaging/en \"HTTP/1.1 200 OK\"\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models/t5_response_generator\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models/t5_response_generator\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models/t5_response_generator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models/t5_response_generator\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models/t5_qa\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models/t5_qa\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models/t5_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models/t5_qa\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models/emotion_classifier\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_models/emotion_classifier\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file saved_models/emotion_classifier\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at saved_models/emotion_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load models and tokenizers\n",
    "resp_model_path = \"saved_models/t5_response_generator\"\n",
    "qa_model_path = \"saved_models/t5_qa\"\n",
    "emo_model_path = \"saved_models/emotion_classifier\"\n",
    "\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(resp_model_path)\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(resp_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_path)\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(qa_model_path).to(resp_model.device)\n",
    "\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(emo_model_path)\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(emo_model_path).to(resp_model.device)\n",
    "emo_model.eval()\n",
    "\n",
    "# Emotion labels (based on go_emotions)\n",
    "NUM_EMO_LABELS = emo_model.config.num_labels\n",
    "\n",
    "DEFAULT_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'embarrassment', 'excitement', 'fear', 'gratitude',\n",
    "    'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n",
    "    'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "EMOTION_LABELS = DEFAULT_LABELS[:NUM_EMO_LABELS]\n",
    "emotion_router_labels = set(EMOTION_LABELS) & {\n",
    "    'confusion', 'caring', 'nervousness', 'grief', 'sadness', 'fear', 'remorse', 'love', 'anger'\n",
    "}\n",
    "\n",
    "\n",
    "# Format prompt\n",
    "def format_input_prompt(user_input, language=\"English\", history=None):\n",
    "    if history:\n",
    "        combined = \"\\n\".join(history + [user_input])\n",
    "        return f\"You are a supportive mental health assistant. Respond in {language}. The conversation so far:\\n{combined}\"\n",
    "    return f\"You are a supportive mental health assistant. Respond in {language}. The user says: {user_input}\"\n",
    "\n",
    "# Emotion classifier\n",
    "def detect_emotions(text):\n",
    "    inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(emo_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = emo_model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    thresholded = [EMOTION_LABELS[i] for i, p in enumerate(probs) if p > 0.3]\n",
    "    return thresholded if thresholded else [\"neutral\"]\n",
    "\n",
    "# Voice input to text\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp:\n",
    "        audio.export(tmp.name, format=\"wav\")\n",
    "        with sr.AudioFile(tmp.name) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            try:\n",
    "                return recognizer.recognize_google(audio_data)\n",
    "            except sr.UnknownValueError:\n",
    "                return \"[Unrecognized speech]\"\n",
    "            except sr.RequestError:\n",
    "                return \"[Speech recognition failed]\"\n",
    "\n",
    "# Generator with router\n",
    "def generate_chatbot_response(user_text, audio_input, mode, language, use_history, history, route_by_emotion, persist):\n",
    "    history = history or []\n",
    "    user_input = user_text if mode == \"text\" else transcribe_audio(audio_input)\n",
    "    emotions = detect_emotions(user_input)\n",
    "    use_resp_model = any(e in emotion_router_labels for e in emotions) if route_by_emotion else False\n",
    "\n",
    "    if use_resp_model:\n",
    "        prompt = format_input_prompt(user_input, language, history if use_history else None)\n",
    "        inputs = resp_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(resp_model.device)\n",
    "        model = resp_model\n",
    "        tokenizer = resp_tokenizer\n",
    "    else:\n",
    "        prompt = \"question: \" + user_input\n",
    "        inputs = qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(qa_model.device)\n",
    "        model = qa_model\n",
    "        tokenizer = qa_tokenizer\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=64,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    full_history = history + [f\"User: {user_input}\", f\"Bot: {response}\"]\n",
    "\n",
    "    if persist:\n",
    "        with open(\"chatlog.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"\\n[{datetime.datetime.now()}]\\n{full_history[-2]}\\n{full_history[-1]}\\nDetected emotions: {emotions}\\n\")\n",
    "\n",
    "    return response, emotions, full_history\n",
    "\n",
    "# Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=generate_chatbot_response,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Type your message here (if using text mode)\"),\n",
    "        gr.Audio(type=\"filepath\", label=\"Or speak here (if using voice mode)\"),\n",
    "        gr.Radio([\"text\", \"voice\"], value=\"text\", label=\"Input Mode\"),\n",
    "        gr.Dropdown(choices=[\"English\", \"German\", \"Spanish\", \"French\"], value=\"English\", label=\"Response Language\"),\n",
    "        gr.Checkbox(label=\"Include chat history in response\", value=True),\n",
    "        gr.State(value=[]),\n",
    "        gr.Checkbox(label=\"Route by detected emotion\", value=True),\n",
    "        gr.Checkbox(label=\"Save conversation to chatlog.txt\", value=True)\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Therapist Response\"),\n",
    "        gr.Textbox(label=\"Detected Emotions\"),\n",
    "        gr.State()\n",
    "    ],\n",
    "    title=\"Voice + Text Enabled Emotion-Aware Mental Health Chatbot\",\n",
    "    description=\"You can type or speak your message. Emotion-aware routing decides between Q&A and therapist-style support.\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
