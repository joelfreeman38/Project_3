{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "528fc79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, T5ForConditionalGeneration, TrainingArguments,\n",
    "                          Trainer, Seq2SeqTrainer, DataCollatorWithPadding)\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Trainer\n",
    "from transformers import TrainerCallback\n",
    "from transformers import default_data_collator\n",
    "from transformers import logging as hf_logging\n",
    "import datetime\n",
    "import evaluate\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import numpy as np, torch\n",
    "import os\n",
    "import os, pprint, glob\n",
    "import os, random, json, time, itertools\n",
    "import pandas as pd\n",
    "import speech_recognition as sr\n",
    "import tempfile\n",
    "import torch\n",
    "import torch, torch.nn.functional as F\n",
    "import torch.nn.functional as F\n",
    "import warnings, logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d559dfc",
   "metadata": {},
   "source": [
    "# Mental‑Health Chatbot: Training & Deployment\n",
    "\n",
    "This notebook walks through **building, training, and serving** a multi‑task mental‑health chatbot that:\n",
    "\n",
    "1. Detects the user's emotions (multi‑label classification).\n",
    "2. Generates empathetic free‑text responses.\n",
    "3. Answers direct mental‑health questions accurately.\n",
    "\n",
    "We combine three fine‑tuned Hugging Face models:\n",
    "\n",
    "| Task | Base model | Output dir |\n",
    "|------|------------|------------|\n",
    "| Emotion classification | `SamLowe/roberta-base-go_emotions` | `./saved_models/emotion_classifier` |\n",
    "| Response generation | `google/t5-small` (or any T5) | `./saved_models/t5_response_generator` |\n",
    "| Question‑answering | `google/t5-small` (or any T5) | `./saved_models/t5_qa` |\n",
    "\n",
    "Finally, we wire them together in a small **pipeline** and expose it through a minimal [Gradio](https://gradio.app) UI that *remembers* the conversation.\n",
    "\n",
    "> **Tip** Training large models can take a while. Feel free to toggle individual datasets on/off or start with a tiny subset while you iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2238a52",
   "metadata": {},
   "source": [
    "## 0. Environment setup *(optional)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1cb4fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab/Kaggle add any missing libraries:\n",
    "# !pip install -q transformers datasets evaluate bert-score gradio sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e74c4",
   "metadata": {},
   "source": [
    "## 1. Imports & global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a6fa4082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cuda\n"
     ]
    }
   ],
   "source": [
    "hf_logging.set_verbosity_info()     # show INFO messages from Trainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                          \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on: {device}\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Ensure save directories exist\n",
    "SAVE_ROOT = Path(\"./saved_models\")\n",
    "for sub in [\"emotion_classifier\", \"t5_response_generator\", \"t5_qa\", \"final_combined\"]:\n",
    "    (SAVE_ROOT / sub).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9a848229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data collator: ensures labels are float32 tensors -----------------\n",
    "def float_label_collator(features):\n",
    "    batch = default_data_collator(features)\n",
    "    if \"labels\" in batch:\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float32)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02db9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Trainer for multi-label BCE loss ---------------------------\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    \"\"\"Casts labels to float32 and reshapes to logits shape if needed.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\").float()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        if labels.shape != logits.shape:\n",
    "            labels = labels.view_as(logits)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels, reduction=\"mean\")\n",
    "        print(\"LOGITS\", logits.shape, \"LABELS\", labels.shape)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3130d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Data collator to ensure BCEWithLogitsLoss gets float labels ───\n",
    "\n",
    "def float_label_collator(features):\n",
    "    batch = default_data_collator(features)\n",
    "    if \"labels\" in batch:\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float32)\n",
    "    print(\"collator labels dtype/shape:\", batch[\"labels\"].dtype, batch[\"labels\"].shape)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8cb0788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Callback: print step loss + epoch eval while keeping tqdm bar ───\n",
    "class StepPrinter(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs or not state.is_local_process_zero:\n",
    "            return\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step:>6} • loss {logs['loss']:.4f}\")\n",
    "        if \"eval_loss\" in logs:\n",
    "            metric = logs.get(\"micro_f1\") or logs.get(\"bertscore_f1\") or logs.get(\"rougeL\")\n",
    "            metric_str = f\" • metric {metric:.4f}\" if metric is not None else \"\"\n",
    "            print(f\"Epoch {int(state.epoch)}/{int(args.num_train_epochs)}\"\n",
    "                  f\" • eval_loss {logs['eval_loss']:.4f}{metric_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "125cb3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────\n",
    "# Callback: print \"Step xx • loss ...\" and epoch eval metrics\n",
    "class StepPrinter(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if not logs or not state.is_local_process_zero:\n",
    "            return\n",
    "        if \"loss\" in logs:\n",
    "            print(f\"Step {state.global_step:>6} • loss {logs['loss']:.4f}\")\n",
    "        if \"eval_loss\" in logs:\n",
    "            f1 = logs.get(\"micro_f1\") or logs.get(\"bertscore_f1\") or logs.get(\"rougeL\")\n",
    "            extra = f\" • metric {f1:.4f}\" if f1 is not None else \"\"\n",
    "            print(f\"Epoch {int(state.epoch)}/{int(args.num_train_epochs)}\"\n",
    "                  f\" • eval_loss {logs['eval_loss']:.4f}{extra}\")\n",
    "# ──────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44a6b5",
   "metadata": {},
   "source": [
    "## 2. Dataset switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0a7dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggle individual CSVs and provide their column mapping\n",
    "# Format: name: (enabled, path, question_col, answer_col)\n",
    "DATASETS = {\n",
    "    \"ds1\": (False,  \"./data/ds1_transformed_mental_health_chatbot_dataset.csv\",  \"question\", \"answer\"),\n",
    "    \"ds2\": (False,  \"./data/ds2_transformed_mental_health_chatbot.csv\",         \"question\", \"answer\"),\n",
    "    \"ds3\": (False,  \"./data/ds3_mental_health_faq_cleaned.csv\",                 \"Question\", \"Answer\"),\n",
    "    \"ds4\": (False,  \"./data/ds4_mental_health_chatbot_dataset_merged_modes.csv\",\"prompt\",   \"response\"),\n",
    "    \"ds5\": (False,  \"./data/ds5_Mental_Health_FAQ.csv\",                         \"Question\", \"Answer\"),\n",
    "    \"ds6\": (False, \"./data/ds6_mental_health_counseling.csv\",                  \"query\",    \"completion\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479db785",
   "metadata": {},
   "source": [
    "## 3. Load & preprocess datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "abd4a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust cleaner that auto‑maps columns to 'question' / 'answer'\n",
    "def load_and_clean(path, q_col, a_col):\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # normalise headers\n",
    "    df.columns = [c.lower().strip() for c in df.columns]\n",
    "    q_col = q_col.lower().strip()\n",
    "    a_col = a_col.lower().strip()\n",
    "\n",
    "    # common renames\n",
    "    rename_map = {\n",
    "        \"prompt\": \"question\",\n",
    "        \"response\": \"answer\",\n",
    "        \"questions\": \"question\",\n",
    "        \"answers\": \"answer\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    # if provided cols exist, rename them\n",
    "    if q_col in df.columns:\n",
    "        df = df.rename(columns={q_col: \"question\"})\n",
    "    if a_col in df.columns:\n",
    "        df = df.rename(columns={a_col: \"answer\"})\n",
    "\n",
    "        # Try to map context → question if needed\n",
    "    if \"question\" not in df.columns and \"context\" in df.columns:\n",
    "        df = df.rename(columns={\"context\": \"question\"})\n",
    "\n",
    "    if not {\"question\", \"answer\"}.issubset(df.columns):\n",
    "        raise ValueError(f\"Could not find 'question'/'answer' in {path}. Available columns: {list(df.columns)}\")\n",
    "\n",
    "\n",
    "\n",
    "    df = df[[\"question\", \"answer\"]].dropna()\n",
    "    df[\"question\"] = df[\"question\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df[\"answer\"]   = df[\"answer\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return Dataset.from_pandas(df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07112162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 1.  Put this once, after your imports ──────────────────────────\n",
    "\n",
    "def float_label_collator(features):\n",
    "    \"\"\"\n",
    "    Wrap the default HF collator but cast the `labels` tensor to float32\n",
    "    so BCEWithLogitsLoss gets the right dtype.\n",
    "    \"\"\"\n",
    "    batch = default_data_collator(features)\n",
    "    if \"labels\" in batch:\n",
    "        batch[\"labels\"] = batch[\"labels\"].to(torch.float32)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "83e7f4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Final version of MultiLabelTrainer ─────────────────────────────\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    BCEWithLogitsLoss that survives any label dtype/shape:\n",
    "    * casts to float32\n",
    "    * reshapes to logits.shape when needed\n",
    "    \"\"\"\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        labels = inputs.pop(\"labels\").float()        # cast dtype\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Fix shape if the collator flattened the labels\n",
    "        if labels.shape != logits.shape:\n",
    "            labels = labels.view_as(logits)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels, reduction=\"mean\")\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d2a3e",
   "metadata": {},
   "source": [
    "### 3.1 Emotion label setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "999ba5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels from the GoEmotions paper (27 emotions + neutral)\n",
    "GO_EMOTION_LABELS = [\n",
    "    'admiration','amusement','anger','annoyance','approval','caring','confusion',\n",
    "    'curiosity','desire','disappointment','disapproval','disgust','embarrassment',\n",
    "    'excitement','fear','gratitude','grief','joy','love','nervousness','optimism',\n",
    "    'pride','realization','relief','remorse','sadness','surprise','neutral'\n",
    "]\n",
    "num_labels = len(GO_EMOTION_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d8fbab",
   "metadata": {},
   "source": [
    "### 3.2 Binarize emotion annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fde6ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No datasets were enabled, using a fallback test dataset.\n",
      "train: 9 • test: 1\n"
     ]
    }
   ],
   "source": [
    "# ── Re‑create train_ds / test_ds ───────────────────────────────────\n",
    "datasets_list = []\n",
    "for name, (enabled, path, q_col, a_col) in DATASETS.items():\n",
    "    if not enabled:\n",
    "        continue\n",
    "    ds = load_and_clean(path=path, q_col=q_col, a_col=a_col)\n",
    "    datasets_list.append(ds)\n",
    "\n",
    "if not datasets_list:\n",
    "    print(\"No datasets were enabled, using a fallback test dataset.\")\n",
    "    fallback_data = {\n",
    "        \"text\": [\n",
    "            \"How are you?\",\n",
    "            \"I feel really down today.\",\n",
    "            \"I'm so happy with my progress!\",\n",
    "            \"Why does nobody understand me?\",\n",
    "            \"I'm feeling anxious about school.\",\n",
    "            \"Life is good lately.\",\n",
    "            \"Sometimes I just want to cry.\",\n",
    "            \"Everything is falling apart.\",\n",
    "            \"I’m grateful for my therapist.\",\n",
    "            \"Can someone please just listen?\"\n",
    "        ]\n",
    "    }\n",
    "    ds = Dataset.from_dict(fallback_data)\n",
    "    datasets_list.append(ds)\n",
    "\n",
    "\n",
    "full_ds = concatenate_datasets(datasets_list) if len(datasets_list) > 1 else datasets_list[0]\n",
    "full_ds = full_ds.shuffle(seed=SEED)\n",
    "\n",
    "split = full_ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "train_ds, test_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "print(f\"train: {len(train_ds):,} • test: {len(test_ds):,}\")\n",
    "# ───────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4fc7b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not datasets_list:\n",
    "    print(\"No datasets were enabled, using a fallback test dataset.\")\n",
    "    fallback_data = {\n",
    "        \"text\": [\n",
    "            \"How are you?\",\n",
    "            \"I feel really down today.\",\n",
    "            \"I'm so happy with my progress!\",\n",
    "            \"Why does nobody understand me?\",\n",
    "            \"I'm feeling anxious about school.\",\n",
    "            \"Life is good lately.\",\n",
    "            \"Sometimes I just want to cry.\",\n",
    "            \"Everything is falling apart.\",\n",
    "            \"I’m grateful for my therapist.\",\n",
    "            \"Can someone please just listen?\"\n",
    "        ]\n",
    "    }\n",
    "    ds = Dataset.from_dict(fallback_data)\n",
    "    datasets_list.append(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "89294c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a830b9b8cea4d4586fc43fabc40516c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3855bb921504c62b4a2f0103ec8f8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: I'm feeling anxious about school. -> ['neutral']\n"
     ]
    }
   ],
   "source": [
    "# For this demo we'll fake some annotations by mapping keywords -> emotions.\n",
    "# Replace with your real emotion annotations if available.\n",
    "KEYWORD2EMO = {\n",
    "    \"sad\": \"sadness\", \"angry\": \"anger\", \"happy\": \"joy\",\n",
    "    \"thank\": \"gratitude\", \"sorry\": \"remorse\", \"love\": \"love\",\n",
    "    \"fear\": \"fear\", \"nervous\": \"nervousness\"\n",
    "}\n",
    "\n",
    "def annotate_emotions(example):\n",
    "    emos = example.get(\"emotions\", [])\n",
    "    \n",
    "    # If no emotions assigned, default to [\"neutral\"]\n",
    "    if not emos:\n",
    "        emos = [\"neutral\"]\n",
    "    \n",
    "    example[\"emotions\"] = emos  # store for visibility\n",
    "    example[\"labels\"] = [1.0 if lbl in emos else 0.0 for lbl in GO_EMOTION_LABELS]\n",
    "    return example\n",
    "\n",
    "def get_input_text(example):\n",
    "    return example.get(\"text\") or example.get(\"question\") or \"[NO TEXT FOUND]\"\n",
    "emo_train = train_ds.map(annotate_emotions)\n",
    "emo_test  = test_ds.map(annotate_emotions)\n",
    "\n",
    "print(\"Sample:\", get_input_text(emo_train[0]), \"->\", emo_train[0][\"emotions\"])\n",
    "\n",
    "\n",
    "def get_input_text(example):\n",
    "    return example.get(\"text\") or example.get(\"question\") or \"[NO TEXT FOUND]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "50c1fac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1906b93d5b4e3db377c966c5d2577f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62bc67e910d9428c9ed213c5c8a237a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def has_nonzero_labels(example):\n",
    "    return sum(example[\"labels\"]) > 0\n",
    "\n",
    "# Apply to your actual datasets\n",
    "emo_train = emo_train.filter(has_nonzero_labels)\n",
    "emo_test = emo_test.filter(has_nonzero_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e47f315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad label examples: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for label issues\n",
    "bad_labels = [ex for ex in emo_train if \"labels\" not in ex or sum(ex[\"labels\"]) == 0]\n",
    "\n",
    "print(\"Number of bad label examples:\", len(bad_labels))\n",
    "\n",
    "# Print the first bad one if any\n",
    "if bad_labels:\n",
    "    print(\"Example with bad label:\", bad_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "68bba92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "emo_tokenizer = AutoTokenizer.from_pretrained(\"SamLowe/roberta-base-go_emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0bd79577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE COLUMN RENAME: ['text', 'emotions', 'labels']\n",
      "AFTER COLUMN RENAME: ['text', 'emotions', 'labels']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e647b36971846c79bf638646c092379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47781d42ccab456699e30ec08fc4be6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690fa26800d0407db610e4b328b1c0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b36d6c56aa9416cbcb28b189154aa33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 🔄 Print + rename if necessary\n",
    "print(\"BEFORE COLUMN RENAME:\", emo_train.column_names)\n",
    "if \"question\" in emo_train.column_names:\n",
    "    emo_train = emo_train.rename_column(\"question\", \"text\")\n",
    "if \"question\" in emo_test.column_names:\n",
    "    emo_test = emo_test.rename_column(\"question\", \"text\")\n",
    "print(\"AFTER COLUMN RENAME:\", emo_train.column_names)\n",
    "\n",
    "# ✅ Tokenizer using just 'text'\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "# ✅ Float conversion\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example\n",
    "\n",
    "# ✅ Tokenize + cast\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "# ✅ Set format for PyTorch\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf9090",
   "metadata": {},
   "source": [
    "## 4. Train emotion classifier (RoBERTa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "af21d877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'emotions', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(emo_train.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ea82e440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"SamLowe/roberta-base-go_emotions\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions\\snapshots\\58b6c5b44a7a12093f782442969019c7e2982299\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at SamLowe/roberta-base-go_emotions.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2291d09b12d4d0ea72e6086a7daaf1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c51b7b495ba4254b334471f47c99fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f8ddc58d1141c08819e747012626bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe254fb088c4da4b1c022537b36e546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: emotions, text. If emotions, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 9\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "  Number of trainable parameters = 124,667,164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.201476</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.189389</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.183481</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: emotions, text. If emotions, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 • eval_loss 0.2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\emotion_classifier\\checkpoint-1\n",
      "Configuration saved in saved_models\\emotion_classifier\\checkpoint-1\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\checkpoint-1\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\checkpoint-1\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\checkpoint-1\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: emotions, text. If emotions, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 • eval_loss 0.1894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\emotion_classifier\\checkpoint-2\n",
      "Configuration saved in saved_models\\emotion_classifier\\checkpoint-2\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\checkpoint-2\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\checkpoint-2\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\checkpoint-2\\special_tokens_map.json\n",
      "Saving model checkpoint to saved_models\\emotion_classifier\\checkpoint-3\n",
      "Configuration saved in saved_models\\emotion_classifier\\checkpoint-3\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\checkpoint-3\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\checkpoint-3\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\checkpoint-3\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: emotions, text. If emotions, text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 • eval_loss 0.1835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to saved_models\\emotion_classifier\\checkpoint-3\n",
      "Configuration saved in saved_models\\emotion_classifier\\checkpoint-3\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\checkpoint-3\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\checkpoint-3\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\checkpoint-3\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from saved_models\\emotion_classifier\\checkpoint-1 (score: 0.0).\n",
      "Configuration saved in saved_models\\emotion_classifier\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\special_tokens_map.json\n",
      "Saving model checkpoint to saved_models\\emotion_classifier\n",
      "Configuration saved in saved_models\\emotion_classifier\\config.json\n",
      "Model weights saved in saved_models\\emotion_classifier\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\special_tokens_map.json\n",
      "tokenizer config file saved in saved_models\\emotion_classifier\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\emotion_classifier\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_models\\\\emotion_classifier\\\\tokenizer_config.json',\n",
       " 'saved_models\\\\emotion_classifier\\\\special_tokens_map.json',\n",
       " 'saved_models\\\\emotion_classifier\\\\vocab.json',\n",
       " 'saved_models\\\\emotion_classifier\\\\merges.txt',\n",
       " 'saved_models\\\\emotion_classifier\\\\added_tokens.json',\n",
       " 'saved_models\\\\emotion_classifier\\\\tokenizer.json')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def emo_tokenize(batch):\n",
    "    return emo_tokenizer(\n",
    "        batch[\"text\"],  # <- THIS is now the standard column name\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "\n",
    "def cast_to_float(example):\n",
    "    example[\"labels\"] = np.array(example[\"labels\"], dtype=np.float32)\n",
    "    return example\n",
    "\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(\"SamLowe/roberta-base-go_emotions\", problem_type=\"multi_label_classification\", num_labels=len(GO_EMOTION_LABELS)).to(device)\n",
    "\n",
    "\n",
    "emo_train_tok = emo_train.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "emo_test_tok  = emo_test.map(emo_tokenize, batched=True).map(cast_to_float)\n",
    "\n",
    "emo_train_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "emo_test_tok.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "# Metrics\n",
    "metric_f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_emo_metrics(pred):\n",
    "\n",
    "    logits, labels = pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.3).int().numpy()\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Defensive check: filter out rows where labels.sum() == 0\n",
    "    mask = labels.sum(axis=1) > 0\n",
    "    if mask.sum() == 0:\n",
    "        print(\"Warning: all evaluation labels are empty\")\n",
    "        return {\"micro_f1\": 0.0}\n",
    "\n",
    "    try:\n",
    "        f1 = f1_score(labels[mask], preds[mask], average=\"micro\", zero_division=0)\n",
    "    except ValueError as e:\n",
    "        print(\"Metric error:\", e)\n",
    "        f1 = 0.0\n",
    "\n",
    "    return {\"micro_f1\": f1}\n",
    "\n",
    "\n",
    "\n",
    "emo_args = TrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"emotion_classifier\"),\n",
    "\n",
    "    # logging\n",
    "    logging_strategy=\"steps\", logging_steps=10, logging_dir=\"./logs\", report_to=\"none\",\n",
    "\n",
    "    # core hyper‑params\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # eval / ckpt\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"micro_f1\",\n",
    "    greater_is_better=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer_emo = MultiLabelTrainer(\n",
    "    model=emo_model,\n",
    "    args=emo_args,\n",
    "    train_dataset=emo_train_tok,\n",
    "    eval_dataset=emo_test_tok,\n",
    "    tokenizer=emo_tokenizer,\n",
    "    data_collator=float_label_collator,\n",
    "    compute_metrics=compute_emo_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")\n",
    "\n",
    "\n",
    "# Uncomment to train (may take a while)\n",
    "trainer_emo.train()\n",
    "\n",
    "# Save robustly\n",
    "(SAVE_ROOT/\"emotion_classifier\").mkdir(exist_ok=True, parents=True)\n",
    "emo_model.save_pretrained(SAVE_ROOT/\"emotion_classifier\")\n",
    "emo_tokenizer.save_pretrained(SAVE_ROOT/\"emotion_classifier\")\n",
    "\n",
    "trainer_emo.save_model()            # writes to output_dir\n",
    "emo_tokenizer.save_pretrained(emo_args.output_dir)\n",
    "\n",
    "\n",
    "# Cast multi‑label targets to float32 tensors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "367d7ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input_ids shape: torch.Size([128])\n",
      "Sample labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "Label dtype: <class 'torch.Tensor'> — torch.float32\n",
      "Model expects num_labels: 28\n"
     ]
    }
   ],
   "source": [
    "sample = emo_train_tok[0]\n",
    "print(\"Sample input_ids shape:\", sample[\"input_ids\"].shape)\n",
    "print(\"Sample labels:\", sample[\"labels\"])\n",
    "print(\"Label dtype:\", type(sample[\"labels\"]), \"—\", sample[\"labels\"].dtype if hasattr(sample[\"labels\"], 'dtype') else \"no dtype\")\n",
    "print(\"Model expects num_labels:\", emo_model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "adb45b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1583f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=resp_tokenizer,\n",
    "    model=resp_model,\n",
    "    padding=True,  # enables dynamic padding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840db175",
   "metadata": {},
   "source": [
    "## 5. Train T5 for response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "47c3e970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55bb522abb14e2eb122de22152dd9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f81a8fa1864efcb6dbab90a6229a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\spiece.model\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5e3b4ca5594e349090669a9ea91154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d74f94b7534d0eb81e5890d6482bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 9\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n",
      "  Number of trainable parameters = 60,506,624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bertscore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.828214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.091553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.023079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\merges.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\tokenizer_config.json\n",
      "loading file tokenizer.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--roberta-large\\snapshots\\722cf37b1afa9454edce342e7895e588b6ff1d59\\model.safetensors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 • eval_loss 0.8282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty reference sentence detected; setting recall to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-2\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-2\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-2\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-2\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-2\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-2\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-2\\spiece.model\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 • eval_loss 0.0916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty reference sentence detected; setting recall to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-4\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-4\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-4\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-4\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-4\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-4\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-4\\spiece.model\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-6\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-6\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-6\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-6\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-6\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-6\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-6\\spiece.model\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 • eval_loss 0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty reference sentence detected; setting recall to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\\checkpoint-6\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-6\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\checkpoint-6\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\checkpoint-6\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\checkpoint-6\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\checkpoint-6\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\checkpoint-6\\spiece.model\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in saved_models\\t5_response_generator\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\spiece.model\n",
      "Saving model checkpoint to saved_models\\t5_response_generator\n",
      "Configuration saved in saved_models\\t5_response_generator\\config.json\n",
      "Configuration saved in saved_models\\t5_response_generator\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_response_generator\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\spiece.model\n",
      "tokenizer config file saved in saved_models\\t5_response_generator\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_response_generator\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_response_generator\\spiece.model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_models\\\\t5_response_generator\\\\tokenizer_config.json',\n",
       " 'saved_models\\\\t5_response_generator\\\\special_tokens_map.json',\n",
       " 'saved_models\\\\t5_response_generator\\\\spiece.model',\n",
       " 'saved_models\\\\t5_response_generator\\\\added_tokens.json',\n",
       " 'saved_models\\\\t5_response_generator\\\\tokenizer.json')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build input/target pairs: user text -> helpful response\n",
    "# For now we use 'question' as input and 'answer' as target\n",
    "def build_t5_pairs(example):\n",
    "    question = example.get(\"question\") or example.get(\"text\") or \"\"\n",
    "    answer = example.get(\"answer\") or example.get(\"response\") or \"\"\n",
    "    example[\"input_text\"] = \"respond: \" + question\n",
    "    example[\"target_text\"] = answer\n",
    "    return example\n",
    "\n",
    "\n",
    "resp_train = train_ds.map(build_t5_pairs)\n",
    "resp_test  = test_ds.map(build_t5_pairs)\n",
    "\n",
    "t5_resp_model_name = \"t5-small\"\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(t5_resp_model_name)\n",
    "\n",
    "def t5_tokenize(batch):\n",
    "    model_inputs = tokenizer_t5(batch[\"input_text\"], max_length=128, truncation=True)\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(batch[\"target_text\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "resp_train_tok = resp_train.map(t5_tokenize, batched=True, remove_columns=resp_train.column_names)\n",
    "resp_test_tok  = resp_test.map(t5_tokenize, batched=True, remove_columns=resp_test.column_names)\n",
    "\n",
    "resp_train_tok.set_format(\"torch\")\n",
    "resp_test_tok.set_format(\"torch\")\n",
    "\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(t5_resp_model_name).to(device)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "def compute_resp_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer_t5.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer_t5.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer_t5.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    r = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    b = bertscore.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    \n",
    "    return {\n",
    "        \"rougeL\": r[\"rougeL\"],\n",
    "        \"bertscore_f1\": np.mean(b[\"f1\"])\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "resp_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"t5_response_generator\"),\n",
    "\n",
    "    # logging\n",
    "    logging_strategy=\"steps\", logging_steps=10, logging_dir=\"./logs\", report_to=\"none\",\n",
    "\n",
    "    # core hyper‑params\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # eval / ckpt\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,   # now valid\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer_resp = Seq2SeqTrainer(\n",
    "    model=resp_model,\n",
    "    args=resp_args,\n",
    "    train_dataset=resp_train_tok,\n",
    "    eval_dataset=resp_test_tok,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=data_collator,  # ✅ keep this\n",
    "    compute_metrics=compute_resp_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")\n",
    "\n",
    "\n",
    "# Uncomment to train\n",
    "trainer_resp.train()\n",
    "\n",
    "# Robust save\n",
    "resp_model.save_pretrained(SAVE_ROOT/\"t5_response_generator\")\n",
    "tokenizer_t5.save_pretrained(SAVE_ROOT/\"t5_response_generator\"),\n",
    "callbacks=[StepPrinter]\n",
    "trainer_resp.save_model()\n",
    "tokenizer_t5.save_pretrained(resp_args.output_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92020c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 11\n",
      "Label length: 1\n",
      "Label: tensor([1])\n",
      "Type: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "sample = resp_train_tok[0]\n",
    "print(\"Input length:\", len(sample[\"input_ids\"]))\n",
    "print(\"Label length:\", len(sample[\"labels\"]))\n",
    "print(\"Label:\", sample[\"labels\"])\n",
    "print(\"Type:\", type(sample[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6140c",
   "metadata": {},
   "source": [
    "## 6. Train T5 for question‑answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f0a44457",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_models/t5_qa\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models/t5_qa\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models/t5_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models/t5_qa\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(\"saved_models/t5_qa\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"saved_models/t5_qa\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cfb440c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer_t5,\n",
    "    model=qa_model,\n",
    "    padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "51577a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ce23b52cf8437da543027beef73024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17588861a3b343aaa7bbe3c01060a6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c002c2ee9c4e69ad475d403a8528d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37376a1aa6354a74ba7fa2ad77a25a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\config.json\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\mward\\.cache\\huggingface\\hub\\models--t5-small\\snapshots\\df1b051c49625cf57a3d0d8d3863ed4d13564fe4\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "***** Running training *****\n",
      "  Num examples = 9\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n",
      "  Number of trainable parameters = 60,506,624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bertscore F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.024853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 • eval_loss 0.0249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty reference sentence detected; setting recall to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-2\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-2\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-2\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-2\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-2\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-2\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-2\\spiece.model\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 • eval_loss 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty reference sentence detected; setting recall to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-4\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-4\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-4\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-4\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-4\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-4\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-4\\spiece.model\n",
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-6\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-6\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-6\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-6\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-6\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-6\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-6\\spiece.model\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "INFO:absl:Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 • eval_loss 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting precision to be 0.\n",
      "Warning: Empty reference sentence detected; setting recall to be 0.\n",
      "Saving model checkpoint to saved_models\\t5_qa\\checkpoint-6\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-6\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\checkpoint-6\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\checkpoint-6\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\checkpoint-6\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\checkpoint-6\\special_tokens_map.json\n",
      "Copy vocab file to saved_models\\t5_qa\\checkpoint-6\\spiece.model\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Configuration saved in saved_models\\t5_qa\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\special_tokens_map.json\n",
      "Saving model checkpoint to saved_models\\t5_qa\n",
      "Configuration saved in saved_models\\t5_qa\\config.json\n",
      "Configuration saved in saved_models\\t5_qa\\generation_config.json\n",
      "Model weights saved in saved_models\\t5_qa\\model.safetensors\n",
      "tokenizer config file saved in saved_models\\t5_qa\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\special_tokens_map.json\n",
      "tokenizer config file saved in saved_models\\t5_qa\\tokenizer_config.json\n",
      "Special tokens file saved in saved_models\\t5_qa\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('saved_models\\\\t5_qa\\\\tokenizer_config.json',\n",
       " 'saved_models\\\\t5_qa\\\\special_tokens_map.json',\n",
       " 'saved_models\\\\t5_qa\\\\spiece.model',\n",
       " 'saved_models\\\\t5_qa\\\\added_tokens.json',\n",
       " 'saved_models\\\\t5_qa\\\\tokenizer.json')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build QA pairs: \"question: <text>\" -> answer\n",
    "def build_qa_pairs(example):\n",
    "    question = example.get(\"question\") or example.get(\"text\") or \"\"\n",
    "    answer = example.get(\"answer\") or example.get(\"response\") or \"\"\n",
    "    example[\"input_text\"] = \"question: \" + question\n",
    "    example[\"target_text\"] = answer\n",
    "    return example\n",
    "\n",
    "\n",
    "qa_train = train_ds.map(build_qa_pairs)\n",
    "qa_test  = test_ds.map(build_qa_pairs)\n",
    "\n",
    "def qa_tokenize(batch):\n",
    "    model_inputs = tokenizer_t5(batch[\"input_text\"], max_length=128, truncation=True)\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(batch[\"target_text\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "qa_train_tok = qa_train.map(qa_tokenize, batched=True, remove_columns=qa_train.column_names)\n",
    "qa_test_tok  = qa_test.map(qa_tokenize, batched=True, remove_columns=qa_test.column_names)\n",
    "\n",
    "qa_train_tok.set_format(\"torch\")\n",
    "qa_test_tok.set_format(\"torch\")\n",
    "\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(t5_resp_model_name).to(device)\n",
    "\n",
    "qa_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(SAVE_ROOT / \"t5_qa\"),\n",
    "\n",
    "    logging_strategy=\"steps\", logging_steps=10, logging_dir=\"./logs\", report_to=\"none\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "trainer_qa = Seq2SeqTrainer(\n",
    "    model=qa_model,\n",
    "    args=qa_args,\n",
    "    train_dataset=qa_train_tok,\n",
    "    eval_dataset=qa_test_tok,\n",
    "    tokenizer=tokenizer_t5,\n",
    "    data_collator=qa_collator,\n",
    "    compute_metrics=compute_resp_metrics,\n",
    "    callbacks=[StepPrinter],\n",
    ")\n",
    "\n",
    "# Uncomment to train\n",
    "trainer_qa.train()\n",
    "\n",
    "qa_model.save_pretrained(SAVE_ROOT/\"t5_qa\")\n",
    "tokenizer_t5.save_pretrained(SAVE_ROOT/\"t5_qa\")\n",
    "callbacks=[StepPrinter]\n",
    "trainer_qa.save_model()\n",
    "tokenizer_t5.save_pretrained(qa_args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "071b08e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top‑level content of saved_models:\n",
      "['emotion_classifier', 'final_combined', 't5_qa', 't5_response_generator']\n",
      "\n",
      "Any checkpoint folders?\n",
      "['saved_models\\\\t5_response_generator\\\\checkpoint-116',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-174',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-2',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-20',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-239',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-4',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-40',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-478',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-58',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-6',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-60',\n",
      " 'saved_models\\\\t5_response_generator\\\\checkpoint-717']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root = Path(\"./saved_models\")\n",
    "\n",
    "print(\"Top‑level content of saved_models:\")\n",
    "pprint.pprint(os.listdir(root))\n",
    "\n",
    "print(\"\\nAny checkpoint folders?\")\n",
    "ckpts = sorted(glob.glob(str(root / \"t5_response_generator\" / \"checkpoint-*\")))\n",
    "pprint.pprint(ckpts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006aba6c",
   "metadata": {},
   "source": [
    "## 7. Combined pipeline & Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "84d25a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_models\\emotion_classifier\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_models\\\\emotion_classifier\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\emotion_classifier\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at saved_models\\emotion_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models\\t5_qa\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models\\t5_qa\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models\\t5_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models\\t5_qa\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    }
   ],
   "source": [
    "# Load (fine‑tuned) models back in – comment these if still in memory\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(SAVE_ROOT/\"emotion_classifier\").to(device)\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(SAVE_ROOT/\"emotion_classifier\")\n",
    "\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(SAVE_ROOT/\"t5_qa\").to(device)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(SAVE_ROOT/\"t5_qa\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "class MentalHealthChatbotPipeline:\n",
    "    def __init__(self, labels, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        self.labels = labels\n",
    "        self.chat_history = []  # (speaker, text)\n",
    "\n",
    "        self.emo_model = emo_model.eval()\n",
    "        self.qa_model  = qa_model.eval()\n",
    "        self.resp_model = resp_model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, text, max_length=128):\n",
    "        self.chat_history.append((\"user\", text))\n",
    "\n",
    "        # Emotion detection\n",
    "        emo_inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        probs = torch.sigmoid(self.emo_model(**emo_inputs).logits)[0]\n",
    "        emotions = [lbl for lbl, p in zip(self.labels, probs) if p > 0.3]\n",
    "\n",
    "        # Pick model\n",
    "        model, tok = (self.qa_model, qa_tokenizer) if \"?\" in text else (self.resp_model, resp_tokenizer)\n",
    "        ids = model.generate(**tok(text, return_tensors=\"pt\").to(self.device),\n",
    "                             max_length=max_length)\n",
    "        reply = tok.decode(ids[0], skip_special_tokens=True)\n",
    "        self.chat_history.append((\"bot\", reply))\n",
    "\n",
    "        return {\"Detected Emotions\": emotions, \"Response\": reply, \"History\": self.chat_history}\n",
    "\n",
    "chatbot = MentalHealthChatbotPipeline(labels=GO_EMOTION_LABELS, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76626236",
   "metadata": {},
   "source": [
    "## 8. Save final bundle metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "05f3aef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to saved_models\\final_combined\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "metadata = {\n",
    "    \"emotion_model\": str(SAVE_ROOT/\"emotion_classifier\"),\n",
    "    \"response_model\": str(SAVE_ROOT/\"t5_response_generator\"),\n",
    "    \"qa_model\": str(SAVE_ROOT/\"t5_qa\"),\n",
    "    \"labels\": GO_EMOTION_LABELS\n",
    "}\n",
    "with open(SAVE_ROOT/\"final_combined\"/\"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"Metadata saved to\", SAVE_ROOT/\"final_combined\"/\"metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8c6257e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models/t5_response_generator\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models/t5_response_generator\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models/t5_response_generator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models/t5_response_generator\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file spiece.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models/t5_qa\\config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file saved_models/t5_qa\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at saved_models/t5_qa.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "loading configuration file saved_models/t5_qa\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file saved_models/emotion_classifier\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"saved_models/emotion_classifier\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"admiration\",\n",
      "    \"1\": \"amusement\",\n",
      "    \"2\": \"anger\",\n",
      "    \"3\": \"annoyance\",\n",
      "    \"4\": \"approval\",\n",
      "    \"5\": \"caring\",\n",
      "    \"6\": \"confusion\",\n",
      "    \"7\": \"curiosity\",\n",
      "    \"8\": \"desire\",\n",
      "    \"9\": \"disappointment\",\n",
      "    \"10\": \"disapproval\",\n",
      "    \"11\": \"disgust\",\n",
      "    \"12\": \"embarrassment\",\n",
      "    \"13\": \"excitement\",\n",
      "    \"14\": \"fear\",\n",
      "    \"15\": \"gratitude\",\n",
      "    \"16\": \"grief\",\n",
      "    \"17\": \"joy\",\n",
      "    \"18\": \"love\",\n",
      "    \"19\": \"nervousness\",\n",
      "    \"20\": \"optimism\",\n",
      "    \"21\": \"pride\",\n",
      "    \"22\": \"realization\",\n",
      "    \"23\": \"relief\",\n",
      "    \"24\": \"remorse\",\n",
      "    \"25\": \"sadness\",\n",
      "    \"26\": \"surprise\",\n",
      "    \"27\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"admiration\": 0,\n",
      "    \"amusement\": 1,\n",
      "    \"anger\": 2,\n",
      "    \"annoyance\": 3,\n",
      "    \"approval\": 4,\n",
      "    \"caring\": 5,\n",
      "    \"confusion\": 6,\n",
      "    \"curiosity\": 7,\n",
      "    \"desire\": 8,\n",
      "    \"disappointment\": 9,\n",
      "    \"disapproval\": 10,\n",
      "    \"disgust\": 11,\n",
      "    \"embarrassment\": 12,\n",
      "    \"excitement\": 13,\n",
      "    \"fear\": 14,\n",
      "    \"gratitude\": 15,\n",
      "    \"grief\": 16,\n",
      "    \"joy\": 17,\n",
      "    \"love\": 18,\n",
      "    \"nervousness\": 19,\n",
      "    \"neutral\": 27,\n",
      "    \"optimism\": 20,\n",
      "    \"pride\": 21,\n",
      "    \"realization\": 22,\n",
      "    \"relief\": 23,\n",
      "    \"remorse\": 24,\n",
      "    \"sadness\": 25,\n",
      "    \"surprise\": 26\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file saved_models/emotion_classifier\\model.safetensors\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at saved_models/emotion_classifier.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7862/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7862/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load models and tokenizers\n",
    "resp_model_path = \"saved_models/t5_response_generator\"\n",
    "qa_model_path = \"saved_models/t5_qa\"\n",
    "emo_model_path = \"saved_models/emotion_classifier\"\n",
    "\n",
    "resp_tokenizer = AutoTokenizer.from_pretrained(resp_model_path)\n",
    "resp_model = T5ForConditionalGeneration.from_pretrained(resp_model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_path)\n",
    "qa_model = T5ForConditionalGeneration.from_pretrained(qa_model_path).to(resp_model.device)\n",
    "\n",
    "emo_tokenizer = AutoTokenizer.from_pretrained(emo_model_path)\n",
    "emo_model = AutoModelForSequenceClassification.from_pretrained(emo_model_path).to(resp_model.device)\n",
    "emo_model.eval()\n",
    "\n",
    "# Emotion labels (based on go_emotions)\n",
    "NUM_EMO_LABELS = emo_model.config.num_labels\n",
    "\n",
    "DEFAULT_LABELS = [\n",
    "    'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n",
    "    'desire', 'disappointment', 'disapproval', 'embarrassment', 'excitement', 'fear', 'gratitude',\n",
    "    'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n",
    "    'sadness', 'surprise', 'neutral'\n",
    "]\n",
    "\n",
    "EMOTION_LABELS = DEFAULT_LABELS[:NUM_EMO_LABELS]\n",
    "emotion_router_labels = set(EMOTION_LABELS) & {\n",
    "    'confusion', 'caring', 'nervousness', 'grief', 'sadness', 'fear', 'remorse', 'love', 'anger'\n",
    "}\n",
    "\n",
    "\n",
    "# Format prompt\n",
    "def format_input_prompt(user_input, language=\"English\", history=None):\n",
    "    if history:\n",
    "        combined = \"\\n\".join(history + [user_input])\n",
    "        return f\"You are a supportive mental health assistant. Respond in {language}. The conversation so far:\\n{combined}\"\n",
    "    return f\"You are a supportive mental health assistant. Respond in {language}. The user says: {user_input}\"\n",
    "\n",
    "# Emotion classifier\n",
    "def detect_emotions(text):\n",
    "    inputs = emo_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(emo_model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = emo_model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    thresholded = [EMOTION_LABELS[i] for i, p in enumerate(probs) if p > 0.3]\n",
    "    return thresholded if thresholded else [\"neutral\"]\n",
    "\n",
    "# Voice input to text\n",
    "def transcribe_audio(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp:\n",
    "        audio.export(tmp.name, format=\"wav\")\n",
    "        with sr.AudioFile(tmp.name) as source:\n",
    "            audio_data = recognizer.record(source)\n",
    "            try:\n",
    "                return recognizer.recognize_google(audio_data)\n",
    "            except sr.UnknownValueError:\n",
    "                return \"[Unrecognized speech]\"\n",
    "            except sr.RequestError:\n",
    "                return \"[Speech recognition failed]\"\n",
    "\n",
    "# Generator with router\n",
    "def generate_chatbot_response(user_text, audio_input, mode, language, use_history, history, route_by_emotion, persist):\n",
    "    history = history or []\n",
    "    user_input = user_text if mode == \"text\" else transcribe_audio(audio_input)\n",
    "    emotions = detect_emotions(user_input)\n",
    "    use_resp_model = any(e in emotion_router_labels for e in emotions) if route_by_emotion else False\n",
    "\n",
    "    if use_resp_model:\n",
    "        prompt = format_input_prompt(user_input, language, history if use_history else None)\n",
    "        inputs = resp_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(resp_model.device)\n",
    "        model = resp_model\n",
    "        tokenizer = resp_tokenizer\n",
    "    else:\n",
    "        prompt = \"question: \" + user_input\n",
    "        inputs = qa_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(qa_model.device)\n",
    "        model = qa_model\n",
    "        tokenizer = qa_tokenizer\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=64,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    full_history = history + [f\"User: {user_input}\", f\"Bot: {response}\"]\n",
    "\n",
    "    if persist:\n",
    "        with open(\"chatlog.txt\", \"a\", encoding=\"utf-8\") as log:\n",
    "            log.write(f\"\\n[{datetime.datetime.now()}]\\n{full_history[-2]}\\n{full_history[-1]}\\nDetected emotions: {emotions}\\n\")\n",
    "\n",
    "    return response, emotions, full_history\n",
    "\n",
    "# Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=generate_chatbot_response,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Type your message here (if using text mode)\"),\n",
    "        gr.Audio(type=\"filepath\", label=\"Or speak here (if using voice mode)\"),\n",
    "        gr.Radio([\"text\", \"voice\"], value=\"text\", label=\"Input Mode\"),\n",
    "        gr.Dropdown(choices=[\"English\", \"German\", \"Spanish\", \"French\"], value=\"English\", label=\"Response Language\"),\n",
    "        gr.Checkbox(label=\"Include chat history in response\", value=True),\n",
    "        gr.State(value=[]),\n",
    "        gr.Checkbox(label=\"Route by detected emotion\", value=True),\n",
    "        gr.Checkbox(label=\"Save conversation to chatlog.txt\", value=True)\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Therapist Response\"),\n",
    "        gr.Textbox(label=\"Detected Emotions\"),\n",
    "        gr.State()\n",
    "    ],\n",
    "    title=\"Voice + Text Enabled Emotion-Aware Mental Health Chatbot\",\n",
    "    description=\"You can type or speak your message. Emotion-aware routing decides between Q&A and therapist-style support.\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
