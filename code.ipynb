{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mward\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from huggingface_hub.hf_api import HfApi\n",
    "from huggingface_hub.utils import logging\n",
    "import torch\n",
    "import os  \n",
    "import pandas as pd  \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling  \n",
    "from datasets import Dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable for cache directory to a local permitted directory \n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = './transformers_cache'  \n",
    "os.environ['HF_HOME'] = './hf_home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV file...\n"
     ]
    }
   ],
   "source": [
    "# Load CSV data  \n",
    "\n",
    "print(\"Loading CSV file...\")  \n",
    "df = pd.read_csv('./data/Combined Data.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after removing missing values: (52681, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing statements and filter them out  \n",
    "df = df.dropna(subset=['statement'])  \n",
    "print(\"Data after removing missing values: \" + str(df.shape))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hugging Face Dataset from the DataFrame (using just the 'statement' column)  \n",
    "dataset = Dataset.from_pandas(df[['statement']])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model...\n",
      "Tokenizer and model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model (using your working code)  \n",
    "print(\"Loading tokenizer and model...\")  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")  \n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")  \n",
    "print(\"Tokenizer and model loaded successfully.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tokenization function using the \"statement\" column  \n",
    "def tokenize_function(examples):  \n",
    "    return tokenizer(examples[\"statement\"], padding=\"max_length\", truncation=True, max_length=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52681/52681 [00:06<00:00, 8449.35 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset  \n",
    "print(\"Tokenizing dataset...\")  \n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)  \n",
    "print(\"Dataset tokenized successfully.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data collator (for causal LM, mlm is False)  \n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments (adjust hyperparameters as needed)  \n",
    "training_args = TrainingArguments(  \n",
    "    output_dir=\"./happy_brain\",  \n",
    "    overwrite_output_dir=True,  \n",
    "    num_train_epochs=1,  # Adjust the number of epochs as needed  \n",
    "    per_device_train_batch_size=4,  \n",
    "    save_steps=1000,  \n",
    "    save_total_limit=2,  \n",
    "    logging_steps=100,  \n",
    "    evaluation_strategy=\"no\"  \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized. Ready to start training.\n",
      "Note: Full training requires significant time and resources.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer  \n",
    "trainer = Trainer(  \n",
    "    model=model,  \n",
    "    args=training_args,  \n",
    "    train_dataset=tokenized_dataset,  \n",
    "    data_collator=data_collator,  \n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Ready to start training.\")  \n",
    "print(\"Note: Full training requires significant time and resources.\") \n",
    "\n",
    "# Uncomment the following line to start training:  \n",
    "# trainer.train()  \n",
    "  \n",
    "# Save the model and tokenizer after training (uncomment when trainer.train() is executed)  \n",
    "# model.save_pretrained(\"./happy_brain\")  \n",
    "# tokenizer.save_pretrained(\"./happy_brain\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example inference (using base model):\n",
      "Input: I've been feeling really anxious lately and can't sleep.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I've been feeling really anxious lately and can't sleep. I'm worried about my future and what's going to happen to me. I'm scared of the unknown and don't know how to co\n"
     ]
    }
   ],
   "source": [
    "# For demonstration, here's a sample inference function using the base model:  \n",
    "def generate_response(input_text, max_length=50):  \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")  \n",
    "    outputs = model.generate(  \n",
    "        inputs.input_ids,   \n",
    "        max_length=max_length,  \n",
    "        num_return_sequences=1,  \n",
    "        temperature=0.7  \n",
    "    )  \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)  \n",
    "\n",
    "print(\"\\nExample inference (using base model):\")  \n",
    "sample_input = \"I've been feeling really anxious lately and can't sleep.\"  \n",
    "print(\"Input: \" + sample_input)  \n",
    "print(\"Response: \" + generate_response(sample_input))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
