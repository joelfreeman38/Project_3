{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4153d61c",
   "metadata": {},
   "source": [
    "# Step 1: Mental Health Chatbot Training Pipeline\n",
    "\n",
    "This notebook performs a full end-to-end pipeline using all 8 specified datasets:\n",
    "- 5 from Hugging Face\n",
    "- 3 local CSV files\n",
    "\n",
    "We are training:\n",
    "1. RoBERTa for Emotion Detection (base model)\n",
    "2. T5 for Response Generation (small base model)\n",
    "3. T5 for Q&A Assistant\n",
    "\n",
    "We also built a full logic flow and evaluate the chatbot's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1544822d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mward\\AppData\\Roaming\\Python\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          T5Tokenizer, T5ForConditionalGeneration,\n",
    "                          Trainer, TrainingArguments,\n",
    "                          Seq2SeqTrainer, Seq2SeqTrainingArguments)\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu  \n",
    "from nltk.translate.meteor_score import meteor_score  \n",
    "from rouge_score import rouge_scorer\n",
    "import sentencepiece as spm  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f35c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79510ad78017453f8a88e6f60f742e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/98 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8875bf27e5472a970c7539a859c81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839e66d2ae734545ac48929f07e11125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load local files\n",
    "local1 = pd.read_csv(\"./data/mental_health_faq.csv\")\n",
    "local2 = pd.read_csv(\"./data/transformed_mental_health_chatbot.csv\")\n",
    "local3 = pd.read_csv(\"./data/Mental Health Chatbot Dataset - Friend mode and Professional mode Responses.csv\")\n",
    "\n",
    "# Unify function\n",
    "def unify(ds, q_col, a_col):\n",
    "    return ds.map(lambda x: {\"question\": str(x[q_col]).strip(), \"answer\": str(x[a_col]).strip()})\n",
    "\n",
    "# Convert local files\n",
    "local1_ds = unify(Dataset.from_pandas(local1), 'Question', 'Answer')\n",
    "local2_ds = unify(Dataset.from_pandas(local2), 'Question', 'Answer')\n",
    "local3_ds = unify(Dataset.from_pandas(local3), 'Prompt', 'Friend Response')  # or 'Professional Response'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f84b677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in the combined dataset: 373327\n",
      "Dataset loaded and unified successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load online datasets\n",
    "ds1 = load_dataset('tolu07/Mental_Health_FAQ', split='train')  # ['question', 'answer']\n",
    "ds2 = load_dataset('Amod/mental_health_counseling_conversations', split='train')  # ['Context', 'Response']\n",
    "ds3 = load_dataset('ruslanmv/ai-medical-chatbot', split='train')  # ['Patient', 'Doctor']\n",
    "ds4 = load_dataset('lavita/ChatDoctor-HealthCareMagic-100k', split='train')  # ['instruction', 'input', 'output']\n",
    "ds5 = load_dataset('heliosbrahma/mental_health_chatbot_dataset', split='train')  # ['text']\n",
    "\n",
    "# Normalize online datasets\n",
    "ds1 = unify(ds1, 'Questions', 'Answers')\n",
    "ds2 = unify(ds2, 'Context', 'Response')\n",
    "ds3 = unify(ds3, 'Patient', 'Doctor')  # Map 'Patient' to 'question' and 'Doctor' to 'answer'\n",
    "ds4 = ds4.map(lambda x: {\"question\": (x['instruction'] + ' ' + x['input']).strip(), \"answer\": x['output'].strip()})\n",
    "ds5 = ds5.map(lambda x: {\"question\": x['text'].split('\\n')[0].strip(), \"answer\": x['text'].split('\\n')[-1].strip()} if '\\n' in x['text'] else {\"question\": x['text'].strip(), \"answer\": x['text'].strip()})\n",
    "\n",
    "# Combine all\n",
    "all_data = concatenate_datasets([ds1, ds2, ds3, ds4, ds5, local1_ds, local2_ds, local3_ds])  # Combine all datasets\n",
    "all_data = all_data.filter(lambda x: bool(x['question']) and bool(x['answer']))\n",
    "print('Total records in the combined dataset:', len(all_data))\n",
    "\n",
    "print(\"Dataset loaded and unified successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a99020",
   "metadata": {},
   "source": [
    "## Step 2: Train RoBERTa for Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac987d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use subset of data for emotion labels (simulate labels for demo)\n",
    "sample_df = pd.DataFrame(all_data[:3000])  # Use 3000 examples for quick demo\n",
    "emotions = ['neutral', 'sadness', 'nervousness', 'anger', 'fear']\n",
    "sample_df['label'] = [random.choice(emotions) for _ in range(len(sample_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1bd68d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label2id = {label: i for i, label in enumerate(emotions)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "sample_df['label'] = sample_df['label'].map(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d0f8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Convert to Dataset\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model_roberta = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(label2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35de328a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5adcba7a4ed4ebd9c2100b1145c7177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset for RoBERTa\n",
    "def tokenize_roberta(examples):\n",
    "    return tokenizer_roberta(examples['question'], truncation=True, padding=\"max_length\")\n",
    "\n",
    "emo_dataset = Dataset.from_pandas(sample_df)\n",
    "emo_dataset = emo_dataset.map(tokenize_roberta, batched=True)\n",
    "emo_train_test = emo_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "training_args_roberta = TrainingArguments(\n",
    "    output_dir=\"./roberta-emotion\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b45bc21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer for RoBERTa\n",
    "# trainer_roberta = Trainer(\n",
    "#     model=model_roberta,\n",
    "#     args=training_args_roberta,\n",
    "#     train_dataset=emo_train_test[\"train\"],\n",
    "#     eval_dataset=emo_train_test[\"test\"],\n",
    "#     tokenizer=tokenizer_roberta,\n",
    "# )\n",
    "\n",
    "# trainer_roberta.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d258bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_models/roberta_emotion\\\\tokenizer_config.json',\n",
       " './saved_models/roberta_emotion\\\\special_tokens_map.json',\n",
       " './saved_models/roberta_emotion\\\\vocab.json',\n",
       " './saved_models/roberta_emotion\\\\merges.txt',\n",
       " './saved_models/roberta_emotion\\\\added_tokens.json',\n",
       " './saved_models/roberta_emotion\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_roberta.save_pretrained(\"./saved_models/roberta_emotion\")\n",
    "tokenizer_roberta.save_pretrained(\"./saved_models/roberta_emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4519f0",
   "metadata": {},
   "source": [
    "## Step 3: Train T5 for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73bf70f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples loaded: 165\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Define unified datasets\n",
    "datasets_list = []\n",
    "\n",
    "# ==== Toggle Each Dataset On/Off ====\n",
    "# Online\n",
    "USE_DS1 = False   # tolu07/Mental_Health_FAQ\n",
    "USE_DS2 = False  # Amod/mental_health_counseling_conversations\n",
    "USE_DS3 = False  # ruslanmv/ai-medical-chatbot\n",
    "USE_DS4 = False  # lavita/ChatDoctor-HealthCareMagic-100k\n",
    "USE_DS5 = True  # heliosbrahma/mental_health_chatbot_dataset\n",
    "\n",
    "# Local\n",
    "USE_LOCAL1 = False  # mental_health_faq.csv\n",
    "USE_LOCAL2 = False  # transformed_mental_health_chatbot.csv\n",
    "USE_LOCAL3 = False  # Mental Health Chatbot Dataset - Friend/Pro mode\n",
    "\n",
    "# === Load and append each if enabled ===\n",
    "if USE_DS1:\n",
    "    datasets_list.append(ds1)\n",
    "if USE_DS2:\n",
    "    datasets_list.append(ds2)\n",
    "if USE_DS3:\n",
    "    datasets_list.append(ds3)\n",
    "if USE_DS4:\n",
    "    datasets_list.append(ds4)\n",
    "if USE_DS5:\n",
    "    datasets_list.append(ds5)\n",
    "\n",
    "if USE_LOCAL1:\n",
    "    datasets_list.append(local1_ds)\n",
    "if USE_LOCAL2:\n",
    "    datasets_list.append(local2_ds)\n",
    "if USE_LOCAL3:\n",
    "    datasets_list.append(local3_ds)\n",
    "\n",
    "# Combine selected datasets\n",
    "all_data = concatenate_datasets(datasets_list)\n",
    "all_data = all_data.filter(lambda x: bool(x['question']) and bool(x['answer']))\n",
    "print(\"Total examples loaded:\", len(all_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd7d14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Initialize T5 for response generation\n",
    "\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"t5-small\")  # or t5-tiny if you have it\n",
    "model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0852a4eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't5_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Remove existing tokenized columns before applying preprocess_t5\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m t5_eval_dataset_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mt5_split\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mremove_columns([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mmap(preprocess_t5)\n",
      "\u001b[1;31mNameError\u001b[0m: name 't5_split' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_t5_batched(batch):\n",
    "    def clean_text(text):\n",
    "        return str(text).replace(\"HUMAN>:\", \"\").replace(\"HUMAN>\", \"\").replace(\"USER:\", \"\").replace(\"<ASSISTANT>:\", \"\").strip()\n",
    "\n",
    "    inputs = [\"question: \" + clean_text(q) for q in batch['question']]\n",
    "    targets = [clean_text(a) for a in batch['answer']]\n",
    "\n",
    "    model_inputs = tokenizer_t5(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer_t5.as_target_tokenizer():\n",
    "        labels = tokenizer_t5(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "\n",
    "# Remove existing tokenized columns before applying preprocess_t5\n",
    "t5_eval_dataset_cleaned = t5_split[\"test\"].remove_columns([\"input_ids\", \"attention_mask\", \"labels\"]).map(preprocess_t5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.remove_columns([col for col in all_data.column_names if col not in ['question', 'answer']])\n",
    "t5_dataset = all_data.map(preprocess_t5_batched, batched=True)\n",
    "t5_split = t5_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "training_args_t5 = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-response\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d51425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_15140\\3931529480.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_t5 = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.619164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=7.349626922607422, metrics={'train_runtime': 42.6945, 'train_samples_per_second': 3.466, 'train_steps_per_second': 0.234, 'total_flos': 5007646654464.0, 'train_loss': 7.349626922607422, 'epoch': 1.0})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Seq2SeqTrainer for T5\n",
    "trainer_t5 = Seq2SeqTrainer(\n",
    "    model=model_t5,\n",
    "    args=training_args_t5,\n",
    "    train_dataset=t5_split[\"train\"],\n",
    "    eval_dataset=t5_split[\"test\"],\n",
    "    tokenizer=tokenizer_t5,\n",
    ")\n",
    "\n",
    "trainer_t5.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1389b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b56ee59c53d4fca84b88823cdb1ebb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/98 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b777e0dc776a42ad90e376083062e480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02863f0e2c9f4f53aad08700dc955a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in the combined dataset: 373327\n"
     ]
    }
   ],
   "source": [
    "# Load local data  \n",
    "local1 = pd.read_csv('./data/mental_health_faq.csv')  # ['Questions', 'Answers']  \n",
    "local2 = pd.read_csv('./data/transformed_mental_health_chatbot.csv')  # ['question', 'answer']  \n",
    "local3 = pd.read_csv('./data/Mental Health Chatbot Dataset - Friend mode and Professional mode Responses.csv')  # ['Prompt', 'Friend Response']  \n",
    "  \n",
    "# Function to standardize datasets to have 'question' and 'answer' keys  \n",
    "def unify(ds, q_col, a_col):  \n",
    "    return ds.map(lambda x: {\"question\": str(x[q_col]).strip(), \"answer\": str(x[a_col]).strip()})  \n",
    "  \n",
    "# Convert local data to datasets  \n",
    "local1_ds = unify(Dataset.from_pandas(local1), 'Question', 'Answer')  \n",
    "local2_ds = unify(Dataset.from_pandas(local2), 'Question', 'Answer')  \n",
    "local3_ds = unify(Dataset.from_pandas(local3), 'Prompt', 'Friend Response')  \n",
    "  \n",
    "# Load online datasets and fix columns  \n",
    "ds1 = load_dataset('tolu07/Mental_Health_FAQ', split='train')  # ['question', 'answer']  \n",
    "ds2 = load_dataset('Amod/mental_health_counseling_conversations', split='train')  # ['Context', 'Response']  \n",
    "ds3 = load_dataset('ruslanmv/ai-medical-chatbot', split='train')  # ['Context', 'Response']  \n",
    "ds4 = load_dataset('lavita/ChatDoctor-HealthCareMagic-100k', split='train')  # ['instruction', 'input', 'output']  \n",
    "ds5 = load_dataset('heliosbrahma/mental_health_chatbot_dataset', split='train')  # ['text']  \n",
    "  \n",
    "# Normalize columns  \n",
    "ds1 = unify(ds1, 'Questions', 'Answers')    \n",
    "ds2 = unify(ds2, 'Context', 'Response')  \n",
    "ds3 = unify(ds3, 'Patient', 'Doctor')  # Map 'Patient' to 'question' and 'Doctor' to 'answer'  \n",
    "ds4 = ds4.map(lambda x: {\"question\": (x['instruction'] + ' ' + x['input']).strip(), \"answer\": x['output'].strip()})  \n",
    "ds5 = ds5.map(lambda x: {\"question\": x['text'].split('\\n')[0].strip(), \"answer\": x['text'].split('\\n')[-1].strip()} if '\\n' in x['text'] else {\"question\": x['text'].strip(), \"answer\": x['text'].strip()})  \n",
    "  \n",
    "# Combine everything  \n",
    "all_data = concatenate_datasets([ds1, ds2, ds3, ds4, ds5, local1_ds, local2_ds, local3_ds])  \n",
    "all_data = all_data.filter(lambda x: bool(x['question']) and bool(x['answer']))  \n",
    "  \n",
    "# Print out the total number of records  \n",
    "print('Total records in the combined dataset:', len(all_data))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd87080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_models/t5_response\\\\tokenizer_config.json',\n",
       " './saved_models/t5_response\\\\special_tokens_map.json',\n",
       " './saved_models/t5_response\\\\spiece.model',\n",
       " './saved_models/t5_response\\\\added_tokens.json')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_t5.save_pretrained(\"./saved_models/t5_response\")\n",
    "tokenizer_t5.save_pretrained(\"./saved_models/t5_response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0f546e",
   "metadata": {},
   "source": [
    "## Step 4: Train T5 for Q&A Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbece11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples loaded: 165\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Define unified datasets\n",
    "datasets_list = []\n",
    "\n",
    "# ==== Toggle Each Dataset On/Off ====\n",
    "# Online\n",
    "USE_DS1 = False   # tolu07/Mental_Health_FAQ\n",
    "USE_DS2 = False  # Amod/mental_health_counseling_conversations\n",
    "USE_DS3 = False  # ruslanmv/ai-medical-chatbot\n",
    "USE_DS4 = False  # lavita/ChatDoctor-HealthCareMagic-100k\n",
    "USE_DS5 = True  # heliosbrahma/mental_health_chatbot_dataset\n",
    "# Local\n",
    "USE_LOCAL1 = False  # mental_health_faq.csv\n",
    "USE_LOCAL2 = False  # transformed_mental_health_chatbot.csv\n",
    "USE_LOCAL3 = False  # Mental Health Chatbot Dataset - Friend/Pro mode\n",
    "\n",
    "# === Load and append each if enabled ===\n",
    "if USE_DS1:\n",
    "    datasets_list.append(ds1)\n",
    "if USE_DS2:\n",
    "    datasets_list.append(ds2)\n",
    "if USE_DS3:\n",
    "    datasets_list.append(ds3)\n",
    "if USE_DS4:\n",
    "    datasets_list.append(ds4)\n",
    "if USE_DS5:\n",
    "    datasets_list.append(ds5)\n",
    "\n",
    "if USE_LOCAL1:\n",
    "    datasets_list.append(local1_ds)\n",
    "if USE_LOCAL2:\n",
    "    datasets_list.append(local2_ds)\n",
    "if USE_LOCAL3:\n",
    "    datasets_list.append(local3_ds)\n",
    "\n",
    "# Combine selected datasets\n",
    "all_data = concatenate_datasets(datasets_list)\n",
    "all_data = all_data.filter(lambda x: bool(x['question']) and bool(x['answer']))\n",
    "print(\"Total examples loaded:\", len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09204f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qa = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "qa_dataset = all_data.map(preprocess_t5_batched, batched=True)\n",
    "\n",
    "qa_split = qa_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac8c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mward\\anaconda3\\envs\\dev2\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "all_data = all_data.remove_columns([col for col in all_data.column_names if col not in ['question', 'answer']])\n",
    "t5_dataset = all_data.map(preprocess_t5_batched, batched=True)\n",
    "t5_split = t5_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "training_args_t5 = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-response\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_qa = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-qa\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fa6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mward\\AppData\\Local\\Temp\\ipykernel_15140\\1708520510.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_qa = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>7.089900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=7.720272827148437, metrics={'train_runtime': 43.6613, 'train_samples_per_second': 3.39, 'train_steps_per_second': 0.229, 'total_flos': 5007646654464.0, 'train_loss': 7.720272827148437, 'epoch': 1.0})"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_qa = Seq2SeqTrainer(\n",
    "    model=model_qa,\n",
    "    args=training_args_qa,\n",
    "    train_dataset=qa_split[\"train\"],\n",
    "    eval_dataset=qa_split[\"test\"],\n",
    "    tokenizer=tokenizer_t5,\n",
    ")\n",
    "\n",
    "trainer_qa.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fce7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_models/t5_qa\\\\tokenizer_config.json',\n",
       " './saved_models/t5_qa\\\\special_tokens_map.json',\n",
       " './saved_models/t5_qa\\\\spiece.model',\n",
       " './saved_models/t5_qa\\\\added_tokens.json')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qa.save_pretrained(\"./saved_models/t5_qa\")\n",
    "tokenizer_t5.save_pretrained(\"./saved_models/t5_qa\")  # same tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be620f34",
   "metadata": {},
   "source": [
    "## Step 5: Combined Chatbot Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d265ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(user_input):\n",
    "    # Emotion Detection\n",
    "    emo_inputs = tokenizer_roberta(user_input, return_tensors=\"pt\")\n",
    "    emo_outputs = model_roberta(**emo_inputs)\n",
    "    emotion = id2label[int(emo_outputs.logits.argmax(dim=1))]\n",
    "\n",
    "    # Support message\n",
    "    support_msg = \"I'm here for you.\" if emotion != \"neutral\" else \"Letâ€™s talk more about how you're feeling.\"\n",
    "\n",
    "    # Response generation\n",
    "    response_input = tokenizer_t5(\"question: \" + user_input, return_tensors=\"pt\").input_ids\n",
    "    response_output = model_t5.generate(response_input, max_length=100)\n",
    "    response_text = tokenizer_t5.decode(response_output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Q&A answer\n",
    "    qa_output = model_qa.generate(response_input, max_length=100)\n",
    "    answer_text = tokenizer_t5.decode(qa_output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Detected Emotion: {emotion}\")\n",
    "    print(f\"Empathetic Support: {support_msg}\")\n",
    "    print(f\"Generated Response: {response_text}\")\n",
    "    print(f\"Factual Q&A: {answer_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74a206",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, sample_size=5, batch_size=4):\n",
    "    from evaluate import load as load_metric\n",
    "    from bert_score import score\n",
    "    import torch\n",
    "    from torch.nn import CrossEntropyLoss\n",
    "    from transformers import DataCollatorForSeq2Seq\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 1: Sample Predictions\n",
    "    print(\"\\nðŸ”¹ Generating sample predictions...\")\n",
    "    test_data = dataset.select(range(sample_size))\n",
    "    references = [ex['answer'] for ex in test_data]\n",
    "    predictions = []\n",
    "\n",
    "    for ex in tqdm(test_data, desc=\"Generating outputs\"):\n",
    "        input_text = \"question: \" + str(ex['question']).strip()\n",
    "        encoded = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "        input_ids = encoded['input_ids'].to(device)\n",
    "\n",
    "        output_ids = model.generate(input_ids=input_ids, max_length=100)\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        predictions.append(generated_text)\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 2: ROUGE Evaluation\n",
    "    print(\"\\nðŸ”¹ ROUGE Evaluation:\")\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    valid_predictions = [pred if pred.strip() else \"N/A\" for pred in predictions]\n",
    "    rouge_scores = rouge.compute(predictions=valid_predictions, references=references)\n",
    "    for key, value in rouge_scores.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 3: BERTScore Evaluation\n",
    "    print(\"\\nðŸ”¹ BERTScore Evaluation:\")\n",
    "    P, R, F1 = score(predictions, references, lang=\"en\", verbose=False)\n",
    "    print(f\"Precision: {P.mean().item():.4f}\")\n",
    "    print(f\"Recall:    {R.mean().item():.4f}\")\n",
    "    print(f\"F1 Score:  {F1.mean().item():.4f}\")\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    \n",
    "    # Ensure dataset has 'input_ids' and 'labels'\n",
    "    if not all(k in dataset.column_names for k in ['input_ids', 'labels']):\n",
    "        raise ValueError(\"Dataset must be tokenized with 'input_ids' and 'labels' before evaluating perplexity.\")\n",
    "\n",
    "\n",
    "    # Step 4: Perplexity on full dataset\n",
    "    print(\"\\nðŸ”¹ Perplexity Calculation:\")\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset.remove_columns([col for col in dataset.column_names if col not in ['input_ids', 'attention_mask', 'labels']]),\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating perplexity\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            loss_fct = CrossEntropyLoss(ignore_index=-100, reduction='sum')\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += (shift_labels != -100).sum().item()\n",
    "\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / total_tokens))\n",
    "    print(f\"Perplexity: {perplexity.item():.4f}\")\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 5: Print Sample Predictions\n",
    "    print(\"\\nðŸ”¹ Sample Predictions:\")\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        print(f\"REF:  {ref}\")\n",
    "        print(f\"PRED: {pred}\")\n",
    "        print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2056f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Generating sample predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating outputs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ ROUGE Evaluation:\n",
      "rouge1: 0.0460\n",
      "rouge2: 0.0063\n",
      "rougeL: 0.0460\n",
      "rougeLsum: 0.0458\n",
      "\n",
      "ðŸ”¹ BERTScore Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6570\n",
      "Recall:    0.6294\n",
      "F1 Score:  0.6424\n",
      "\n",
      "ðŸ”¹ Perplexity Calculation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating perplexity: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1177.8364\n",
      "\n",
      "ðŸ”¹ Sample Predictions:\n",
      "REF:  Remember, each person's journey to mental health is unique, so it's crucial to be patient with yourself and not compare your progress to others. Recovery is possible, and with the right support and dedication, you can lead a fulfilling life.\n",
      "PRED: HUMAN>:\n",
      "------------------------------------------------------------\n",
      "REF:  <ASSISTANT>: Substance abuse can simply be defined as a pattern of harmful use of any substance for mood-altering purposes. Medline's medical encyclopedia defines drug abuse as \"the use of illicit drugs or the abuse of prescription or over-the-counter drugs for purposes other than those for which they are indicated or in a manner or in quantities other than directed.\n",
      "PRED: substance abuse\n",
      "------------------------------------------------------------\n",
      "REF:  Remember, while it's essential to educate yourself, seeking professional help from a licensed mental health practitioner is crucial for personalized advice and treatment recommendations. Everyone's journey to mental well-being is unique, and a qualified professional can help tailor a treatment plan that suits your specific needs. Don't hesitate to reach out for support and guidance on your path to better mental health.\n",
      "PRED: \n",
      "------------------------------------------------------------\n",
      "REF:  4. Neuromodulation. In rare cases, when therapy and medication arenâ€™t making enough of a difference, your doctor might talk to you about devices that change the electrical activity in a certain area of your brain. One kind, transcranial magnetic stimulation, is FDA-approved for OCD treatment. It uses magnetic fields to stimulate nerve cells. A more complicated procedure, deep brain stimulation, uses electrodes that are implanted in your head.\n",
      "PRED: OCD\n",
      "------------------------------------------------------------\n",
      "REF:  5. Prevention and Early Intervention: Both mental and behavioral health practitioners emphasize the importance of prevention and early intervention. Addressing issues at an early stage can prevent them from escalating into more severe problems.\n",
      "PRED: mental health\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Clean the test set â€” only keep necessary columns\n",
    "test_clean = t5_split[\"test\"].remove_columns(\n",
    "    [col for col in t5_split[\"test\"].column_names if col not in ['question', 'answer']]\n",
    ")\n",
    "\n",
    "# STEP 2: Tokenize the test set using the CORRECT batched function\n",
    "t5_eval_dataset = test_clean.map(preprocess_t5_batched, batched=True)\n",
    "\n",
    "# STEP 3: Run evaluation\n",
    "evaluate_model(model_t5, tokenizer_t5, t5_eval_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
